[["index.html", "Practical Spatial Data Coastal and Marine Environment in R Welcome Preface What will you learn? Audience Acknowledgments 0.1 Dedications How this book came to being About the author License", " Practical Spatial Data Coastal and Marine Environment in R Masumbuko Semba 2021-01-17 Welcome This is the online home of Dealing with spatial data for Coastal and Marine Environment in R, a book on analysis and visualization of spatial data. Note: This book is now available online at https://lugoga.github.io/geomarine/ Inspired by bookdown and the Free and Open Source Software for Geospatial (FOSS4G) movement, this book is open source. This ensures its contents are reproducible and publicly accessible for people worldwide. The online version of the book is hosted at my Github Account, where is updated regularly and new version of the book is knitted once the upadates are incorpoarated and status of the build status are provided as follows: This version of the book was built on 2021-01-17. Preface The book has been prepared for use with the course GI 612GIS for coastal and Marine Environment at the Institute of Marine Sciences of the University of Dar es Salaam (IMS/UDSM). The course intends to equip student with skills for capturing, storing, querying, analyzing, displaying, reporting and disseminating coastal and marine spatial data. This course introduce briefly the concept of geographical information system (GIS) and its relevancy in helping decision making in the era of digital age. What will you learn? Practical of Spatial Data for Coastal and Marine Environment in R is a book with thirteen chapters aimed to introduce how to import, manipulate, analyze, and visualize geographic data with open source software. It is based on Ra statistical programming language that has powerful functions for data processing, manipulating, and visualizing spatial data. The book equips you with the knowledge and skills to tackle a wide range of issues in geographic context, including those with scientific, societal, and environmental implications. This book fits people from many backgrounds, who use Geographic Information Systems (GIS) in applying their domain-specific field. Our goal is that, after working through this material, students will not necessarily be expert in these methods and associated theory, but that they will develop an expanded toolkit and a greater appreciation for the wider world of data and statistical modeling. Hope you enjoy taking the course and discovering more about geospatial data! Audience This book is primarily aimed for aquatic ecology, oceanography, limnology, and other fields that works with spatial and temporal data. Moreover, since the methods discussed in the book are applicable not only to aquatic data but also to many other fields that deal with spatial data, the book is also good reference for researchers and practitioners of other areas wishing to learn how to code and visualize this type of data. The concept in the book is also appropriate for undergraduate and postgraduate students, who collect data and analyse them for their research. Acknowledgments R represents an excellent tool for the analysis of spatial data. I would like to thank the R community and the developers and contributors of open-source software that enable reproducible data analysis. In particular, I would like to thank the developers of spatial packages, and the authors of ggplot2 (Wickham 2016) and sf (Pebesma 2018) for the great function they created for spatial and non-spatial data analysis and visualization. I would also like to thank the developers of leaflet (Cheng, Karambelkar, and Xie 2019) and tmpa (Tennekes 2018) packages, which enable interactive mapping and visualization in R. Also and the creation of Shiny (Chang et al. 2020), a package that allows creation of web applications that make a difference on how information is distributed and insights are communicated. This book is written in R Markdown (Xie, Dervieux, and Riederer 2020) with bookdown (Xie 2016). I am grateful to developers of these packages, which made really easy to typeset this book. I would also like to express my sincere gratitude to the anonymous reviewers for their helpful comments that greatly improved the first version of this book. I also thank Tanzania Fisheries Research Institute for permission to use some of data for illustration in this book. I also thank the Institute of Marine Sciences of the University of Dar es Salaam for their suggestions and guidance throughout the preparation of this book. 0.1 Dedications To my wife, Nyamisi, my daughter, Grace and my sons, Daniel and Ethan, for enduring so much typing during dinner. And to Glad, for enduring so many plots over coffee. How this book came to being The book was prepared and using R (R Core Team 2020) and bookdown (Xie 2020), a package based on Markdown syntax (Allaire et al. 2019) in Rstudio. Several other packages were also used for various task during book development. Diagrams were plotted using ggplot2 (Wickham, Chang, et al. 2020) and metR (Campitelli 2020) packages. The kableExtra package was used for making nicer table (Zhu 2020). Basemaps data were obtained from spData (Bivand, Nowosad, and Lovelace 2020). Demographic and economic data like population and Gross domestic Product from World Bank database were obtained through wbstats (Piburn 2020). Fish landings catches were obtained from rfisheries (Ram, Boettiger, and Dyck 2016). Oceanographic data were obtained from various sources and color used for mapping their distribution were obtained from oce package (Kelley and Richards 2020). The webex package was used to create the interactive web exercises (Barr and DeBruine 2019). All of this packages and software are free and open source. About the author Masumbuko Semba works at the School of Material, Energy, Water and Environment of the Nelson Mandela African Institution of Science and Technology located in Arusha, Tanzania. Semba received his bachelors in fisheries and aquaculture and his master of marine sciences from the University of Dar es Salaam, Tanzania. Semba is current doing a PhD in the department of physical oceanography at the Institute of Marine Sciences of the University of Dar es Salaam. Semba research focuses on understanding the surface current dynamics using drifter observations and satellite altimetry data to understand how ocean current in the Indian Ocean vary both in space and time. Semba holds extensive analytic skills using different programming languages including MATLAB, Python and R. His keen interest in coding resulted from power of programming in analytic skills and ability to glean insights from data. Understanding a difficult facing scientist to analyse the data with programming, he developed a blog where he share his analytic concepts to the public. Semba has developed several data web-application that serve as data repository and exploration tool for Lake Tanganyika and Indian Ocean. These web application provide a platform to analyse marine and freshwater environmental data that vary both in space and time. Semba is active developing a package that is dedicated for marine and freshwater system in the WIO region. The packages is basically developed to help marine and freshwater scientist access a large and varied format of in-situ and satellite data in easy way. The package can be accessed and downloaded for installation in your machine from the wior package github link Citation For attribution, please cite this work as; Semba M. (2021). Dealing with spatial data for Coastal and Marine Environment in R. Retrieve from &quot;https://lugoga.github.io/geomarine/&quot; And the BibTex format is; @misc{semba2021, author = {Masumbuko Semba }, title = {Dealing with spatial data for Coastal and Marine Environment in R}, ur = {https://lugoga.github.io/geomarine/}, year = {2021} } License This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. References "],["intro.html", "Chapter 1 Introduction to GIS 1.1 Learning Objectives 1.2 What is a GIS 1.3 GIS software 1.4 Communication of results", " Chapter 1 Introduction to GIS 1.1 Learning Objectives After finishing this chapter, you should be able to: Understand what stands for GIS and explain what is GIS Understand the commercial and open source software used in GIS Distinguish between desktop application and R programming Understand what are packages and how useful they are in R 1.2 What is a GIS This is probably the most asked question posed to those in the Geographic Information Systems (GIS) field and is probably the hardest to answer in a succinct and clear manner. GIS is a technological field that incorporates geographical features with tabular data in order to map, analyze, and assess real-world problems. Bolstad (2016) describes a geographic information system as a computer framework of hardware and software for gathering, managing, analyzing and displaying geographically referenced data Rooted in the science of geography, GIS integrates many types of data. It analyzes spatial location and organizes layers of information into visualizations using maps. With this unique capability, GIS reveals deeper insights into data, such as patterns, relationships, and situationshelping users make smarter decisions. Its important to note that most datasets you will encounter in your lifetime can all be assigned a spatial location. So in essence, any dataset can be represented in a GIS: the question then becomes does it need to be analyzed in a GIS environment? The answer to this question depends on the purpose of the analysis. If, for example, we are interested in identifying the countries in the Africa with highest capture fisheries catches reported to FAO in 2018, a simple table listing those catches by country is all that is needed (Table 1.1). Table 1.1: Ten countries with high Capture Fisheries Reported in 2018 Country Fish Catches (MT) Morocco 1386548 Mauritania 967707 Nigeria 878155 South Africa 570545 Namibia 490149 Senegal 484750 Angola 442255 Uganda 439354 Ghana 376767 Tanzania 375755 However, if we are interested to know whether countries with a high catches are geographically clustered, does information in table 1.1 provide us with enough information to help answer this question? The answer is simple NO. We need geographical information data that link location and shape of each country with fisheries catch. A map similar to figure 1.1 that shows the variation of fisheries catch in space would be helpful. Figure 1.1: A static Choropleth representation of African spatial distribution of capture fisheries reported to FAO in 2018 Figure 1.2: An interactive Choropleth representation of African spatial distribution of capture fisheries reported to FAO in 2018 Maps are ubiquitousavailable online and in various print medium. But we seldom ask how the boundaries of the map features are encoded in a computing environment? After all, if we expect software to assist us in the analysis, the spatial elements of our data should be readily accessible in a digital form. Spending a few minutes thinking through this question will make you realize that simple tables or spreadsheets are not up to this task. A more complex data storage mechanism is required. This is the core of a GIS environment: a spatial database that facilitates the storage and retrieval of data that define the spatial boundaries, lines or points of the entities we are studying. This may seem trivial, but without a spatial database, most spatial data exploration and analysis would not be possible! 1.3 GIS software GIS provides a powerful environment for analysis of spatially-referenced data. The most widely used GIS applications in research are desktop software products with graphical user interfaces designed for interactive use. Two popular applications are ArcGIS and QGIS. 1.3.1 ArcGIS A popular commercial GIS software is ArcGIS developed by ESRI (ESRI),was once a small land-use consulting firm which did not start developing GIS software until the mid 1970s. The ArcGIS desktop environment encompasses a suite of applications which include ArcMap, ArcCatalog, ArcScene and ArcGlobe. ArcGIS comes in three different license levels (basic, standard and advanced) and can be purchased with additional add-on packages. As such, a single license can range from a few thousand dollars to well over ten thousand dollars. In addition to software licensing costs, ArcGIS is only available for Windows operating systems; so if your workplace is a Mac only environment, the purchase of a Windows PC would add to the expense. 1.3.2 QGIS A very capable open source (free) GIS software is QGIS. It encompasses most of the functionality included in ArcGIS. If you are looking for a GIS application for your Mac or Linux environment, QGIS is a wonderful choice given its multi-platform support. Built into the current versions of QGIS are functions from another open source software: GRASS. GRASS has been around since the 1980s and has many advanced GIS data manipulation functions however, its use is not as intuitive as that of QGIS or ArcGIS (hence the preferred QGIS alternative). These desktop applications (ArcMap, QGIS) are wonderful tools for exploratory data analysis and map production. However, in research they introduce the problem that much of the results of even fairly simple analytic work flows are not reproducible because the software is generally not designed to record all of the analytical processes the user performed. This is particularly salient during development of analytic methods, during which a substantial amount of trial-and-error occurs. 1.3.3 R The computer language we will be using in this book is R. The opensource computer language R was developed mainly to address statistical computing, nevertheless, pretty much anything can be carried out in R like creating website, publishing books, editing photos, marking plots and maps. R is one of the popular computer languages among scientist. In recent years, Rs spatial capabilities have increased rapidly with the development of dozens of packages dedicated for spatial analysis and hundreds of proven applications in academic research and industry. With the aid of editors like Rstudio, R is also now more accessible than ever before. Approaching GIS analysis within an R framework addresses many of the problem encountered when using desktop GIS software: Geoprocessing operations that are performed with programmatic code are reproducible. While ArcGIS and QGIS include Python for programmatic approaches, many researchers are already working with R, so only some new commands need to be learned, rather than an entire new language. R is a language that is relatively easy to read and write, lowering the bar for entry. R results are generally stored as data frames which can then be used by the many analytic functions in base R or in added packages. Geoprocessing operations produce results that are stored in memory. This will be a problem for very large data sets and/or limited RAM. On the other hand, it does not force the analyst to decide the file system location and name of a data set at run time; code can be altered to store file system outputs after work flows are determined to have created correct results. . Files created using programmatic code can be traced back to the code that generated them (i.e., find / -name \"*.R\" | xargs grep myawesomedataset.gpkg). Incorporating GIS analyses in R Markdown allows the analyst to create reports that include both the analytic code used to create results as well as to show results and even maps. In this book, we provide an argument for increased understanding and use of R as a command line application for applied GIS and demonstrate how it can be used to effortlessly undertake advanced spatial modelling and analysis and visualization within a cohesive environment. We can install R from the link https://cloud.r-project.org and download and install R based on your computer system. 1.3.4 Why R? Unlike most other GIS software designed specifically for mapping, R is free and open source, powerful, flexible, and relevant beyond the mapping coastal and marine features. Arguments against using and teaching R generally cluster around the following two points: teaching programming in addition to spatial statistic concepts is challenging and the command line is more intimidating to beginners than the graphical user interface (GUI) most point-and-click type software offer. One solution for these concerns is to avoid hands-on data analysis completely. If we do not ask our students to start with raw data and instead always provide them with small, tidy rectangles of data then there is never really a need for software beyond spreadsheet or graphing calculator. This is not what we want in a modern GIS analysis and is a disservice to students. Another solution is to use traditional point-and-click software for data analysis. The typical argument is that the GUI is easier for students to learn and so they can spend more time on concepts. However, this ignores the fact that these software tools also have nontrivial learning curves. In fact, teaching specific data analysis tasks using such software often requires lengthy step-by-step instructions, with annotated screenshots, for navigating menus and other interface elements. Also, it is not uncommon that instructions for one task do not easily extend to another. Replacing such instructions with just a few lines of R code actually makes the instructional materials more concise and less intimidating. Many in the spatial analysis community are in favor of teaching R (or some other programming language, like Python), however the value of using R in introductory statistics courses is not as widely accepted. We acknowledge that this addition can be burdensome, however we would argue that learning a tool that is applicable beyond the introductory spatial statistics course and that enhances students problem solving skills is a burden worth bearing. 1.3.5 Rstudio Rstudio is a platform that facilitates how we import, tidy, manipulate, model and visualize data in R. We can install Rstudio from its weblink https://www.rstuidio.com and download the installation file based on your computer. 1.3.6 Why RStudio? The RStudio IDE includes a viewable environment, a file browser, data viewer, and a plotting pane, which makes it less intimidating than the bare R shell. Additionally, since it is a full fledged IDE, it also features integrated help, syntax highlighting, and context-aware tab completion, which are all powerful tools that help flatten the learning curve. RStudio also has direct integration with other critically important tools for teaching computing best practices and reproducible research. Our recommendation is that students access the RStudio IDE and stop thinking about the console in base R. We recommend students to download and install R and Rstudio in their local machine. 1.3.7 Packages Although the base installation of R has many functions and prograaming approaches to manipulate the datacleaning and structuring data, we will use additional functions from packages in this book. Some of these packages include; tidyversea package with a set of packages that facilitate data importing, manipulation, modelling and displaying results. readxlpackage to import Excel spreasheet file into R metR lubridate patchwork core.packages = c(&quot;tidyverse&quot;,&quot;sf&quot;, &quot;oce&quot;,&quot;tmap&quot;, &quot;leaflet&quot;, &quot;mapview&quot;,&quot;ggrepel&quot;,&quot;metR&quot;) install.packages(core.packages) 1.4 Communication of results It is important to note that the goal of research is not merely to collect data for analysis, but to guide policy and decision makers with relevant information for proper and sustainable protection, utilization and management of aquatic resources. A key aspect of research is, therefore, the proper and timely dissemination of information to those responsible for management of aquatic resources. The R software provides excellent tools that greatly facilitate effective communication of results (R Core Team 2020). R offers visualization packages such as leaflet (Cheng, Karambelkar, and Xie 2019), tmap (Tennekes 2018) and mapview (Appelhans et al. 2020) for making interactive maps, dygraphs (Vanderkam et al. 2018) for plotting time series, and DT (Xie, Cheng, and Tan 2020) for displaying data tables. Moreover, findings can be easily included in reproducible reports generated with R Markdown (Xie, Dervieux, and Riederer 2020), interactive dashboards using flexdashboard (Iannone, Allaire, and Borges 2020), and interactive web applications built with Shiny (Chang et al. 2020). These tools provide important information on which to base action and a careful interpretation of them allows coastal and marine resources officers to allocate resources efficiently and target populations for education or management programs. References "],["glimpse.html", "Chapter 2 A glimpse of R, Rstudio and Packages 2.1 The Data Analysis Workflow 2.2 Installation", " Chapter 2 A glimpse of R, Rstudio and Packages This chapter is designed to help you to get started using R and RStudio, assuming no prior use of either. If you already have experience using R and RStudio, you may find some of the contents of this chapter to be a refresher - or as a chance to learn a few new things about setting up and using them. If you are looking to get started with the very basics of data loading and manipulation using the {tidyverse} (Wickham, Averick, et al. (2019)) right now, consider reading this chapter quickly and then starting with the importing data in chapter 4, reshaping data in Chapter 5 and manipulate data in chapter 6 What is R? Before we begin it is important to consider what R is. R is a programming language for statistical computing and graphics (R Core Team 2020). R is a programming language which is highly used in data science. R is becoming more and more popular due to two major reasons: R is open source. R has most of the latest statistical methods. R has a base language that allows a user to program almost anything they like. Of course to do this takes a lot of time and trial-and-error. This can be easily solved when you consider that there are also many user defined packages and functions. In fact there are over 15,000 packages as of January 2021 and this number is growing exponentially. Should you use R? You may want to ask yourself these questions? Do I need a tool to work with data? Am I looking for something cost effective? Do I want to learn to code in a language that gives me a great deal of freedom? Would I like to be able to easily define my own procedures and functions? If you answered Yes to any of those question than it may be worthwhile for you to start using R. We will begin to layout a bit more framework on why so many data scientists choose to work with R over every other language. 2.1 The Data Analysis Workflow We begin by looking at the Data Analysis workflow presented in figure 2.1, a concept by Hadley Wickham. The diagram shows the natural flow of how we work with data and perform research. We will begin to explore what this means as we continue. knitr::include_graphics(&quot;images/data_science_model.png&quot;) Figure 2.1: A conceptual diagram of data science model 2.1.1 Data Wrangling The first steps we take in any Data Analysis is Data Wrangling. Before we can do any kind of analysis we need to be able to collect our data. Sometimes this comes in from one source but many times this comes from multiple data sources. Once we have this data we find that very rarely is it ever in a useful form. In fact Wickham and Grolemund (2016) suggest that this data preparation of cleaning may take up to 80% of the time. 2.1.2 Importing Data When it comes to importing your data R is very powerful. R can grab data from many courses including .csv, .txt. .xls, . SPSS, SAS, Stata Web Scraping Databases 2.1.3 Tidying Data Tidying Data is the process in making data useful. In this concept we have ecah column of data represent a variable and each row of data represents a single observation. This format is quite useful for data analysis. In this course we will rely heavily on the tidyr package (Wickham 2020). 2.1.4 Transforming Data Once we have data into R and begin to tidy the data we usually need to transform multiple aspects of the data. R has many tools that allow a user to manipulate and transform data. R is one of the most capable languages to explore and analyze data. With over 15,000 packages it can be hard to find models or plots that do not already have multiple functions in R. 2.1.5 Visualizing Data There are multiple ways to vizualize data in R. The base graphics are easy to use and outperform Stata, SAS and SPSS. In this course we will focus on using the ggplot2 package (Wickham 2016). This package is actually a language for graphics and once a user becomes familiar and proficient you can create professional and publication quality graphs. 2.1.6 Models Once you have made your questions sufficiently precise, you can use a model to answer them. Models are a fundamentally mathematical or computational tool, so they generally scale well. Even when they dont, its usually cheaper to buy more computers than it is to buy more brains! But every model makes assumptions, and by its very nature a model cannot question its own assumptions. That means a model cannot fundamentally surprise you. 2.1.7 Data Collaboration and Publishing In every field it is key to be able to communicate what we learn and publish this work so that it can be beneficial to others. This is an absolutely critical part of any data analysis project. It doesnt matter how well your models and visualisation have led you to understand the data unless you can also communicate your results to others. Surrounding all these tools is programming, this is where R language has great power. 2.2 Installation 2.2.1 Installing R To download R, go to CRAN, the comprehensive R archive network. CRAN is composed of a set of mirror servers distributed around the world and is used to distribute R and R packages. Dont try and pick a mirror thats close to you: instead use the cloud mirror, https://cloud.r-project.org, which automatically figures it out for you. A new major version of R comes out once a year, and there are 2-3 minor releases each year. Its a good idea to update regularly. Upgrading can be a bit of a hassle, especially for major versions, which require you to reinstall all your packages, but putting it off only makes it worse. Go to https://cran.r-project.org/ Click Download R for Mac/Windows. Click the link appropriate for your system (Linux, Mac, Windows) Dont worry; you will not mess anything up if you download (or even install!) the wrong file. Once youve installed R, you can get started. 2.2.2 Installing RStudio RStudio is an integrated development environment, or IDE, for R programming (RStudio Team 2015). RStudio is a set of integrated tools that allows for a more user-friendly experience for using R. Although you will likely use RStudio as your main console and editor, you must first install R, as RStudio uses R behind the scenes. Both R and RStudio are freely-available, cross-platform, and open-source. Download and install it from http://www.rstudio.com/download. RStudio is updated a couple of times a year. When a new version is available, RStudio will let you know. Its a good idea to upgrade regularly so you can take advantage of the latest and greatest features. Go to https://www.rstudio.com/products/rstudio/download/ Click Download under RStudio Desktop. Click the link appropriate for your system (Linux, Mac, Windows) Follow the instructions of the Installer. 2.2.3 Rstudio Layout Whenever we want to work with R, well open RStudio. RStudio interfaces directly with R, and is an Integrated Development Environment (IDE). This means that RStudio comes with built-in features that make using R a little easier. When you start RStudio, youll see four key panels in the interface shown in figure ??. Well refer to these four panes as the editor, the Console, the Environment, and the Files panes. The large square on the left is the Console pane, the square in the top right is the Environment pane, and the square in the bottom right is the Files pane. As you work with R more, youll find yourself using the tabs within each of the panes. knitr::include_graphics(&quot;images/rstudio3.png&quot;) Figure 2.2: The interface of Rstudio IDE with four key panels When we create a new file, such as an R script, an R Markdown file, or a Shiny app, RStudio will open a fourth pane, known as the source or editor pane. The source pane should show up as a square in the top left. We can open up an .R script in the source pane by going to File, selecting New File, and then selecting R Script: 2.2.4 Installing Packages This section will briefly go over installing packages thats used throughout this book. An R package is a collection of functions, data, and documentation that extends the capabilities of base R. Using packages is key to the successful use of R. The majority of the packages that you will learn in this course are part of the so-called tidyverse. The packages in the tidyverse share a common philosophy of data and R programming, and are designed to work together naturally. The Tidyverse (Wickham, Averick, et al. 2019) packages form a core set of functions that will allow us to perform most any type of data cleaning or analysis we will need to do. We will use the following packages from the tidyverse ggplot2for data visualisation. dplyrfor data manipulation. tidyrfor data tidying. readrfor data import. purrrfor functional programming. tibblefor tibbles, a modern re-imagining of data frames. For us to use tidyverse and any other package that is not included in Base R, we must install them first. The easiest way to install packages is to use the install.packages() command. For example, lets go ahead and install the tidyverse package on your machine: install.packages(&quot;tidyverse&quot;) On your own computer, type that line of code in the console, and then press enter to run it. R will download the packages from CRAN and install them on to your computer. If you have problems installing, make sure that you are connected to the internet, and that https://cloud.r-project.org/ isnt blocked by your firewall or proxy. 2.2.5 Other packages There are many other excellent packages that are not part of the tidyverse, because they solve problems in a different domain, or are designed with a different set of underlying principles. This doesnt make them better or worse, just different. In other words, the complement to the tidyverse is not the messyverse, but many other universes of interrelated packages (Wickham and Grolemund 2016). As you tackle more data science projects with R, youll learn new packages and new ways of thinking about data. In this course well use several data packages from outside the tidyverse: course.packages = c(&quot;metR&quot;, &quot;cowplot&quot;, &quot;ggspatial&quot;, &quot;patchwork&quot;, &quot;ggrepel&quot;, &quot;oce&quot;, &quot;tmap&quot;, &quot;leaflet&quot;, &quot;bookdown&quot;, &quot;blogdown&quot;, &quot;rmarkdown&quot;, &quot;tinytex&quot;) install.packages(course.packages) 2.2.6 Loading installed packages With exception to base R package, add on package that are installed must be called with either library or require functions to make their tools accessible in R session. Lets us load the tidyverse package we just installed require(tidyverse) You notice that when we load tidyverse, it popup a notification message showing the loaded packages and the conflicts they bring in. These conflicts happen when packages have functions with the same names as other functions. This is OK when you prefer the function in a package like tidyverse rather than some other function. Basically the last package loaded in will mask over other functions if they have common names. 2.2.7 Exploring R with the {swirl} Package If you were able to install the {dataedu} package without any issues or concerns, and youre eager to get started exploring everything that R can do, you can supplement your learning through {swirl} (https://swirlstats.com/students.html). You can install {swirl} by running the following code: install.packages(&quot;swirl&quot;) {swirl} is set of packages (see more on packages in Chapter 6) that you can download, providing an interactive method for learning R by using R in the RStudio Console. Since youve already installed R, RStudio, and the {swirl} package, you can follow the instructions on the {swirl} webpage or run the following code in your console pane to get started with a beginner-level course in {swirl}: library(swirl) install_course(&quot;R_Programming_E&quot;) swirl() There are multiple courses available on {swirl}, and you can access them by installing them and then running the swirl() command in your console. We are not affiliated with {swirl} in any way, nor is it required to use {swirl} in order to progress through this text, but its a great resource that we want to make sure is on your radar! 2.2.8 Conclusion It would be impossible for us to cover everything you can do with R in a single chapter of a book, but it is our hope that this chapter gives you a strong foundation from which to explore both subsequent chapters as well as additional R resources. Appendix extends some of the techniques introduced in References "],["dataTypes.html", "Chapter 3 Understanding Data in R 3.1 Data Types 3.2 Vectors 3.3 Data Frame 3.4 Matrix 3.5 Arrays 3.6 Dealing with Misiing Values", " Chapter 3 Understanding Data in R In tutorial 1, we got a glimpse of GIS and the software used in this field. In this tutorial we focus on lower level data types that R handles. This includes understanding how to manage the numeric type (integer vs. double) and strings. The series of data called vectors and tabular format of data storage called data frames. But before we move further, lets us clean our working environment by clicking a combination of Ctrl+L. Clearing the workspace is always recommended before working on a new R project to avoid name conflicts with provious projects. We can also clear all figures using graphics.off() function. It is a good code practice that a new R project start with the code in the chunk below: rm(list = ls()) graphics.off() 3.1 Data Types R language is a flexible language that allows to work with different kind of data format (R Core Team 2020). This include integer, numeric, character, complex, dates and logical. The default data type or class in R is double precisionnumeric. In a nutshell, R treats all kind of data into five categories but we deal with only four in this book. Before proceeding, we need to clear the workspace by typing rm(list = ls()) after the prompt in the in a console. 3.2 Vectors Ofen times we want to store a set of numbers in once place. One way to do this is using the vectors in R. Vector is the most basic data structure in R. It is a sequence of elements of the same data type. if the elements are of different data types, they be coerced to a commontype that can accommodate all the elelements. Vector are generally created using the c() function widely called concatenate, though depending on the type vector being created, other method. Vectors store several numbers a set of numbers in one container. let us look on the example below id = c(1,2,3,4,5) people = c(158,659,782,659,759) street = c(&quot;Dege&quot;, &quot;Mchikichini&quot;, &quot;Mwembe Mdogo&quot;, &quot;Mwongozo&quot;, &quot;Cheka&quot;) Notice that the c() function, which is short for concatenate wraps the list of numbers. The c() function combines all numbers together into one container. Notice also that all the individual numbers are separated with a comma. The comma is referred to an an item-delimiter. It allows R to hold each of the numbers separately. This is vital as without the item-delimiter, R will treat a vector as one big, unseperated number. 3.2.1 Numeric Vector The numeric class holds the set of real numbers  decimal place numbers. The numeric class is more general than the integer class, and inclused the integer numbers. We create a numeric vector using a c() function but you can use any function that creates a sequence of numbers. These could be any number (whole or decimal number). You can check if the data is integer with is.integer() sst = c(25.4, 26, 28, 27.8, 29, 24.8, 22.3) is.numeric(sst) [1] TRUE 3.2.2 Integer vector Unlike numeric, integer values do not have decimal places. They are commonly used for counting or indexing. Creating an integer vector is similar to numeric vector except that we need to instruct R to treat the data as integer and not numeric or double. To command R creating integer, we specify a suffix L to an element depth = c(5L, 10L, 15L, 20L, 25L,30L) is.vector(depth);class(depth) [1] TRUE [1] &quot;integer&quot; aa = c(20,68,78,50) You can check if the data is integer with is.integer() and can convert numeric value to an integer with as.integer() is.integer(aa) [1] FALSE You can query the class of the object with the class() to know the class of the object class(aa) [1] &quot;numeric&quot; Although the object bb is integer as confirmed with as.integer() function, the class() ouput the answer as numeric. This is because the defaul type of number in r is numeric. However, you can use the function as.integer() to convert numeric value to integer class(as.integer(aa)) [1] &quot;integer&quot; 3.2.3 Character vector In programming terms, we usually call text as string. This often are text data like names. A character vector may contain a single character , a word or a group of words. The elements must be enclosed with a single or double quotations mark. sites = c(&quot;Pemba Channel&quot;, &quot;Zanzibar Channnel&quot;, &quot;Pemba Channel&quot;) is.vector(sites); class(sites) [1] TRUE [1] &quot;character&quot; We can be sure whether the object is a string with is.character() or check the class of the object with class(). countries = c(&quot;Kenya&quot;, &quot;Uganda&quot;, &quot;Rwanda&quot;, &quot;Tanzania&quot;) class(countries) [1] &quot;character&quot; 3.2.4 Factor vector These are strings from finite set of values. For example, we might wish to store a variable that records gender of people. You can check if the data is factor with is.factor() and use as.factor() to convert string to factor sex = c(&quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;) sex = as.factor(sex) class(sex) [1] &quot;factor&quot; Often times we need to know the possible groups that are in the factor data. This can be achieved with the levels() function levels(sex) [1] &quot;Female&quot; &quot;Male&quot; levels(countries) NULL Often we wish to take a continuous numerical vector and transform it into a factor. The function cut() takes a vector of numerical data and creates a factor based on your give cut-points. Let us make a fictional income of 508 people with rnorm() function. income = rnorm(n = 508, mean = 500, sd = 80) hist(income, col = &quot;green&quot;, main = &quot;&quot;, las = 1, xlab = &quot;Individual Income&quot;) Figure 3.1: Income distribution #mosaic::plotDist(dist = &quot;norm&quot;, mean = 500, sd = 80) We can now breaks the distribution into groups and make a simple plot as shown in figure 3.2, where those with income less than 400 were about 50, followed with a group with income range between 400 and 500 of about 200 and 250 people receive income above 500 group = cut(income, breaks = c(300,400,500,800), labels = c(&quot;Below 400&quot;, &quot;400-500&quot;, &quot;Above 500&quot;)) is.factor(group) [1] TRUE levels(group) [1] &quot;Below 400&quot; &quot;400-500&quot; &quot;Above 500&quot; barplot(table(group), las = 1, horiz = FALSE, col = c(&quot;blue&quot;, &quot;red&quot;, &quot;blue&quot;), ylab = &quot;Frequency&quot;, xlab = &quot;Group of Income&quot;) Figure 3.2: Barplot of grouped income data = data.frame(group, income) 3.2.5 Logical Vector A vector of logical values will either contain TRUE or FALSE or both. This is a special case of a factor that can only take on the values TRUE and FALSE. R is case-sensitive, therefore you must always capitalize TRUE and FALSE in function in R. presence = c(TRUE,TRUE, FALSE, TRUE, FALSE) is.vector(presence);class(presence) [1] TRUE [1] &quot;logical&quot; 3.2.6 Date and Time Date and time are also treated as vector in R date.time = seq(lubridate::dmy(010121), lubridate::dmy(250121), length.out = 5) date.time [1] &quot;2021-01-01&quot; &quot;2021-01-07&quot; &quot;2021-01-13&quot; [4] &quot;2021-01-19&quot; &quot;2021-01-25&quot; 3.2.7 Indexing the element One advantage of vector is that you can extract individual element in the vector object by indexing, which is accomplished using the square bracket as illustrated below. id[5] [1] 5 people[5] [1] 759 street[5] [1] &quot;Cheka&quot; Apart from extracting single element, indexing allows to extract a range of element in a vector. This is extremely important because it allows to subset a portion of data in a vector. A colon operator is used to extract a range of data street[2:4] [1] &quot;Mchikichini&quot; &quot;Mwembe Mdogo&quot; &quot;Mwongozo&quot; 3.2.8 Adding and Replacing an element in a vector It is possible to add element of an axisting vecor. Here ia an example id[6] = 6 people[6] = 578 street[6] = &quot;Mwongozo&quot; Sometimes you may need to replace an element from a vector, this can be achieved with indexing people[1] = 750 3.2.9 Number of elements in a vector Sometimes you may have a long vector and want to know the numbers of elements in the object. R has length() function that allows you to query the vector and print the answer length(people) [1] 6 3.2.10 Generating sequence of vectors Numbers There are few R operators that are designed for creating vecor of non-random numbers. These functions provide multiple ways for generating sequences of numbers The colon : operator, explicitly generate regular sequence of numbers between the lower and upper boundary numbers specified. For example, generating number beween 0 and 10, we simply write; vector.seq = 0:10 vector.seq [1] 0 1 2 3 4 5 6 7 8 9 10 However, if you want to generate a vector of sequence number with specified interval, let say we want to generate number between 0 and 10 with interval of 2, then the seq() function is used regular.vector = seq(from = 0,to = 10, by = 2) regular.vector [1] 0 2 4 6 8 10 unlike the seq() function and : operator that works with numbers, the rep() function generate sequence of repeated numbers or strings to create a vector id = rep(x = 3, each = 4) station = rep(x = &quot;Station1&quot;, each = 4) id;station [1] 3 3 3 3 [1] &quot;Station1&quot; &quot;Station1&quot; &quot;Station1&quot; &quot;Station1&quot; The rep() function allows to parse each and times arguments. The each argument allows creation of vector that that repeat each element in a vector according to specified number. sampled.months = c(&quot;January&quot;, &quot;March&quot;, &quot;May&quot;) rep(x = sampled.months, each = 3) [1] &quot;January&quot; &quot;January&quot; &quot;January&quot; &quot;March&quot; &quot;March&quot; [6] &quot;March&quot; &quot;May&quot; &quot;May&quot; &quot;May&quot; But the times argument repeat the whole vector to specfied times rep(x = sampled.months, times = 3) [1] &quot;January&quot; &quot;March&quot; &quot;May&quot; &quot;January&quot; &quot;March&quot; [6] &quot;May&quot; &quot;January&quot; &quot;March&quot; &quot;May&quot; 3.2.11 Generating vector of normal distribution The central limit theorem that ensure the data is normal distributed is well known to statistician. R has a rnorm() function which makes vector of normal distributed values. For example to generate a vector of 40 sea surface temperature values from a normal distribution with a mean of 25, and standard deviation of 1.58, we simply type this expression in console; sst = rnorm(n = 40, mean = 25,sd = 1.58) sst [1] 23.53896 25.41704 29.05143 21.88331 27.80019 [6] 25.32484 26.53750 24.83934 21.90669 26.40397 [11] 27.10452 24.54453 22.89453 22.55194 25.47323 [16] 24.43212 25.06017 24.42305 25.67659 25.61061 [21] 21.57773 22.59587 25.81376 25.55776 27.67474 [26] 26.95813 23.90916 27.80628 26.68281 24.84558 [31] 26.03314 23.99131 22.24853 26.00972 24.28766 [36] 25.71115 23.96549 23.49277 27.59002 26.28672 3.2.12 Rounding off numbers There are many ways of rounding off numerical number to the nearest integers or specify the number of decimal places. the code block below illustrate the common way to round off: require(magrittr) chl = rnorm(n = 20, mean = .55, sd = .2) chl %&gt;% round(digits = 2) [1] 0.82 0.65 0.54 0.52 0.64 0.74 0.60 0.59 0.42 1.02 [11] 0.73 0.49 0.77 0.74 0.49 0.59 0.36 0.63 0.63 0.73 3.3 Data Frame data.frame is very much like a simple Excel spreadsheet where each column represents a variable type and each row represent observations. A data frame is the most common way of storing data in R and, generally, is the data structure most often used for data analyses. A data frame is a list of equallength vectors with rows as records and columns as variables. This makes data frames unique in data storing as it can store different classes of objects in each column (i.e. numeric, character, factor, logic, etc). In this section, we will create data frames and add attributes to data frames. 3.3.1 Creating data frames Perhaps the easiest way to create a data frame is to parse vectors in a data.frame() function. For instance, in this case we create a simple data frame dt and assess its internal structure # create vectors Name = c(&#39;Bob&#39;,&#39;Jeff&#39;,&#39;Mary&#39;) Score = c(90, 75, 92) Grade = c(&quot;A&quot;, &quot;B&quot;, &quot;A&quot;) ## use the vectors to make a data frame dt = data.frame(Name, Score, Grade) ## assess the internal structure str(dt) &#39;data.frame&#39;: 3 obs. of 3 variables: $ Name : Factor w/ 3 levels &quot;Bob&quot;,&quot;Jeff&quot;,&quot;Mary&quot;: 1 2 3 $ Score: num 90 75 92 $ Grade: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 2 1 Note how Variable Name in dt was converted to a column of factors . This is because there is a default setting in data.frame() that converts character columns to factors . We can turn this off by setting the stringsAsFactors = FALSE argument: ## use the vectors to make a data frame df = data.frame(Name, Score, Grade, stringsAsFactors = FALSE) df %&gt;% str() &#39;data.frame&#39;: 3 obs. of 3 variables: $ Name : chr &quot;Bob&quot; &quot;Jeff&quot; &quot;Mary&quot; $ Score: num 90 75 92 $ Grade: chr &quot;A&quot; &quot;B&quot; &quot;A&quot; Now the variable Name is of character class in the data frame. The inherited problem of data frame to convert character columns into a factor is resolved by introduction f advanced data frames called tibble (Müller and Wickham 2020), which provides sticker checking and better formating than the traditional data.frame. ## use the vectors to make a tibble tb = tibble(Name, Score, Grade) ## check the internal structure of the tibble tb%&gt;% glimpse() Rows: 3 Columns: 3 $ Name &lt;chr&gt; &quot;Bob&quot;, &quot;Jeff&quot;, &quot;Mary&quot; $ Score &lt;dbl&gt; 90, 75, 92 $ Grade &lt;chr&gt; &quot;A&quot;, &quot;B&quot;, &quot;A&quot; Table 3.1 show the the data frame created by fusing the two vectors together. Table 3.1: Variables in the data frame Name Score Grade Bob 90 A Jeff 75 B Mary 92 A Because the columns have meaning and we have given them column names, it is desirable to want to access an element by the name of the column as opposed to the column number.In large Excel spreadsheets I often get annoyed trying to remember which column something was. The $sign and []are used in R to select variable from the data frame. dt$Name [1] Bob Jeff Mary Levels: Bob Jeff Mary dt[,1] [1] Bob Jeff Mary Levels: Bob Jeff Mary dt$Score [1] 90 75 92 dt[,2] [1] 90 75 92 R has build in dataset that we can use for illustration. For example, Longley (1967) created a longley dataset, which is data frame with 7 economic variables observed every year from 1947 ti 1962 (Table 3.2). We can add the data in the workspace with data() function data(longley) longley %&gt;% kableExtra::kable(caption = &quot;Longleys&#39; Economic dataset&quot;, align = &quot;c&quot;, row.names = F) %&gt;% kableExtra::column_spec(1:7, width = &quot;3cm&quot;) Table 3.2: Longleys Economic dataset GNP.deflator GNP Unemployed Armed.Forces Population Year Employed 83.0 234.289 235.6 159.0 107.608 1947 60.323 88.5 259.426 232.5 145.6 108.632 1948 61.122 88.2 258.054 368.2 161.6 109.773 1949 60.171 89.5 284.599 335.1 165.0 110.929 1950 61.187 96.2 328.975 209.9 309.9 112.075 1951 63.221 98.1 346.999 193.2 359.4 113.270 1952 63.639 99.0 365.385 187.0 354.7 115.094 1953 64.989 100.0 363.112 357.8 335.0 116.219 1954 63.761 101.2 397.469 290.4 304.8 117.388 1955 66.019 104.6 419.180 282.2 285.7 118.734 1956 67.857 108.4 442.769 293.6 279.8 120.445 1957 68.169 110.8 444.546 468.1 263.7 121.950 1958 66.513 112.6 482.704 381.3 255.2 123.366 1959 68.655 114.2 502.601 393.1 251.4 125.368 1960 69.564 115.7 518.173 480.6 257.2 127.852 1961 69.331 116.9 554.894 400.7 282.7 130.081 1962 70.551 Sometimes you may need to create set of values and store them in vectors, then combine the vectors into a data frame. Let us see how this can be done. First create three vectors. One contains id for ten individuals, the second vector hold the time each individual signed in the attendane book and the third vector is the distance of each individual from office. We can concatenate the set of values to make vectors. id = c(1,2,3,4,5,6,7,8,9,10) time = lubridate::ymd_hms(c(&quot;2018-11-20 06:35:25 EAT&quot;, &quot;2018-11-20 06:52:05 EAT&quot;, &quot;2018-11-20 07:08:45 EAT&quot;, &quot;2018-11-20 07:25:25 EAT&quot;, &quot;2018-11-20 07:42:05 EAT&quot;, &quot;2018-11-20 07:58:45 EAT&quot;, &quot;2018-11-20 08:15:25 EAT&quot;, &quot;2018-11-20 08:32:05 EAT&quot;, &quot;2018-11-20 08:48:45 EAT&quot;, &quot;2018-11-20 09:05:25 EAT&quot;), tz = &quot;&quot;) distance = c(20, 85, 45, 69, 42, 52, 6, 45, 36, 7) Once we have the vectors that have the same length dimension, we can use the function data.frame() to combine the the three vectors into one data frame shown in table 3.3 arrival = data.frame(id, time, distance) Table 3.3: The time employees enter into the office with the distance from their residential areas to the office IDs Time Distance 1 2018-11-20 06:35:25 20 2 2018-11-20 06:52:05 85 3 2018-11-20 07:08:45 45 4 2018-11-20 07:25:25 69 5 2018-11-20 07:42:05 42 6 2018-11-20 07:58:45 52 7 2018-11-20 08:15:25 6 8 2018-11-20 08:32:05 45 9 2018-11-20 08:48:45 36 10 2018-11-20 09:05:25 7 3.4 Matrix A matrix is defined as a collection of data elements arranged in a twodimensional rectangular layout. R is very strictly when you make up a matrix as it must be with equal dimensionall columns in a matrix must be of the same length. Unlike data frame and list that can store numeric or character.etc in columns, matrix columns must be numeric or characters in a matrix file. 3.4.1 Creating Matrices The base R has a matrix() function that construct matrices columnwise. In other language, element in matrix are entered starting from the upper left corner and running down the columns. Therefore, one should take serious note of specifying the value to fill in a matrix and the number of rows and columns when using the matrix() function.For example in the code block below, we create an imaginary month sst value for five years and obtain an atomic vector of 60 observation. sst = rnorm(n = 60, mean = 25, 3) Once we have the atomic vector of sst value, we can convert it to matrix with the matrix() function. We put the observation as rowsmonths and the columns as years. Therefore, we have 12 rows and 5 years and the product of number of months and years we get 60equivalent to our sst atomic vector we just created above. sst.matrix = matrix(data = sst, nrow = 12, ncol = 5) We then check whether we got the matrix with is.matrix() function is.matrix(sst);is.matrix(sst.matrix) [1] FALSE [1] TRUE sst [1] 24.12749 27.48300 24.59168 24.84240 20.26587 [6] 23.02858 22.43177 30.98097 28.34107 23.57099 [11] 25.93713 22.69718 25.63378 21.60883 17.71580 [16] 27.87434 25.45653 24.72419 25.02352 25.05497 [21] 26.12613 25.58128 25.03653 23.32987 25.79478 [26] 23.22245 21.21783 23.63734 26.58929 25.76394 [31] 25.85704 26.88392 26.31193 20.80500 26.28160 [36] 26.09082 21.99118 26.15753 24.70482 30.74588 [41] 22.86493 24.05142 24.90224 20.61996 28.10089 [46] 25.76837 24.73541 26.83625 20.82912 26.17858 [51] 23.55351 26.71082 21.55226 23.82109 28.73656 [56] 21.09010 21.98741 23.24176 23.50238 21.40391 We can check whether the dimension we just defined while creating this matrix is correct. This is done with the dim() function from base R. dim(sst.matrix) [1] 12 5 If you have large vector and you you want the matrix() function to figure out the number of columns, you simply define the nrow and tell the function that you do not want those element arranged by rows i.e you want them in column-wise. That is done by parsing the argument byrow = FALSE inside the matrixt() function. sst.matrixby = sst %&gt;% matrix(nrow = 12, byrow = FALSE) 3.4.2 Adding attributes to Matrices Often times you may need to add additional attributes to the maxtrixobservation names, variable names and comments in the matrix. We can add columns, which are years from 2014 to 2018 years = 2014:2018 colnames(sst.matrix) = years sst.matrix 2014 2015 2016 2017 2018 [1,] 24.12749 25.63378 25.79478 21.99118 20.82912 [2,] 27.48300 21.60883 23.22245 26.15753 26.17858 [3,] 24.59168 17.71580 21.21783 24.70482 23.55351 [4,] 24.84240 27.87434 23.63734 30.74588 26.71082 [5,] 20.26587 25.45653 26.58929 22.86493 21.55226 [6,] 23.02858 24.72419 25.76394 24.05142 23.82109 [7,] 22.43177 25.02352 25.85704 24.90224 28.73656 [8,] 30.98097 25.05497 26.88392 20.61996 21.09010 [9,] 28.34107 26.12613 26.31193 28.10089 21.98741 [10,] 23.57099 25.58128 20.80500 25.76837 23.24176 [11,] 25.93713 25.03653 26.28160 24.73541 23.50238 [12,] 22.69718 23.32987 26.09082 26.83625 21.40391 and add the month for rows, which is January to December. Now the matrix has names for the rowsrecords and for columnsvariables months = seq(from = lubridate::dmy(010115), to = lubridate::dmy(311215), by = &quot;month&quot;) %&gt;% lubridate::month(abbr = TRUE, label = TRUE) rownames(sst.matrix) = months sst.matrix 2014 2015 2016 2017 2018 Jan 24.12749 25.63378 25.79478 21.99118 20.82912 Feb 27.48300 21.60883 23.22245 26.15753 26.17858 Mar 24.59168 17.71580 21.21783 24.70482 23.55351 Apr 24.84240 27.87434 23.63734 30.74588 26.71082 May 20.26587 25.45653 26.58929 22.86493 21.55226 Jun 23.02858 24.72419 25.76394 24.05142 23.82109 Jul 22.43177 25.02352 25.85704 24.90224 28.73656 Aug 30.98097 25.05497 26.88392 20.61996 21.09010 Sep 28.34107 26.12613 26.31193 28.10089 21.98741 Oct 23.57099 25.58128 20.80500 25.76837 23.24176 Nov 25.93713 25.03653 26.28160 24.73541 23.50238 Dec 22.69718 23.32987 26.09082 26.83625 21.40391 3.5 Arrays array(data = sst, dim = c(3,5,4)) , , 1 [,1] [,2] [,3] [,4] [,5] [1,] 24.12749 24.84240 22.43177 23.57099 25.63378 [2,] 27.48300 20.26587 30.98097 25.93713 21.60883 [3,] 24.59168 23.02858 28.34107 22.69718 17.71580 , , 2 [,1] [,2] [,3] [,4] [,5] [1,] 27.87434 25.02352 25.58128 25.79478 23.63734 [2,] 25.45653 25.05497 25.03653 23.22245 26.58929 [3,] 24.72419 26.12613 23.32987 21.21783 25.76394 , , 3 [,1] [,2] [,3] [,4] [,5] [1,] 25.85704 20.80500 21.99118 30.74588 24.90224 [2,] 26.88392 26.28160 26.15753 22.86493 20.61996 [3,] 26.31193 26.09082 24.70482 24.05142 28.10089 , , 4 [,1] [,2] [,3] [,4] [,5] [1,] 25.76837 20.82912 26.71082 28.73656 23.24176 [2,] 24.73541 26.17858 21.55226 21.09010 23.50238 [3,] 26.83625 23.55351 23.82109 21.98741 21.40391 This can be done with the indexing. For example, in the sst.matrix we just create, it has twelve rows representing monthly average and five columns representing years. We then obtain data for the six year and we want to add it into the matrix. Simply done with indexing sst.matrix[1:12,5] Jan Feb Mar Apr May Jun 20.82912 26.17858 23.55351 26.71082 21.55226 23.82109 Jul Aug Sep Oct Nov Dec 28.73656 21.09010 21.98741 23.24176 23.50238 21.40391 3.6 Dealing with Misiing Values Just as we can assign numbers, strings, list to a variable, we can also assign nothing to an object, or an empty value to a variable. IN R, an empty object is defined with NULL. Assigning a value oof NULL to an object is one way to reset it to its original, empty state. You might do this when you wanto to preallocate an object without any value, especially when you iterate the process and you want the outputs to be stored in the empty object. sst.container = NULL You can check whether the object is an empty with the is.null() function, which return a logical ouputs indicating whther is TRUE or FALSE is.null(sst.container) [1] TRUE You can also check for NULL in an if satement as well, as highlighted in the following example; if (is.null(sst.container)){ print(&quot;The object is empty and hence you can use to store looped outputs!!!&quot;) } [1] &quot;The object is empty and hence you can use to store looped outputs!!!&quot; And empty element (value) in object is represented with NA in R, and it is the absence of value in an object or variable. sst.sample = c(26.78, 25.98,NA, 24.58, NA) sst.sample [1] 26.78 25.98 NA 24.58 NA To identify missing values in a vector in R, use the is.na() function, which returns a logical vector with TRUE of the corresponding element(s) with missing value is.na(sst.sample) [1] FALSE FALSE TRUE FALSE TRUE and computing statistics of the variable with NA always will give out the NA ouputs mean(sst.sample); sd(sst.sample);range(sst.sample) [1] NA [1] NA [1] NA NA However, we can exclude missing value in these mathematical operations by parsing , na.rm = TRUE argument mean(sst.sample, na.rm = TRUE);sd(sst.sample, na.rm = TRUE);range(sst.sample, na.rm = TRUE) [1] 25.78 [1] 1.113553 [1] 24.58 26.78 you can also exclude the element with NA value using the `na.omit() sst.sample %&gt;% na.omit() [1] 26.78 25.98 24.58 attr(,&quot;na.action&quot;) [1] 3 5 attr(,&quot;class&quot;) [1] &quot;omit&quot; Finally is a NaN, which is closely related to NA, which is used to assign non-floating numbers. For example when we have the anomaly of sea surface temperature and we are interested to use sqrt() function to reduce the variability of the dataset. sst.anomaly = c(2.3,1.25,.8,.31,0,-.21) sqrt(sst.anomaly) [1] 1.5165751 1.1180340 0.8944272 0.5567764 0.0000000 [6] NaN We notice that the sqrt of -0.21 gives us a NaN elements. References "],["readr.html", "Chapter 4 Importing data with readr 4.1 Getting Data into R 4.2 Comma-Separated (.csv) 4.3 Microsoft Excel(.xlsx) 4.4 Writing t a File 4.5 Basic Data Manipulation", " Chapter 4 Importing data with readr As statisticians or data analysts we need to collect data from many sources. We then need to tame it and bring it into a useful format which we control. This section will prepare you to get started with data wrangling! 4.1 Getting Data into R Much of the data we download or receive from researchers is in the form of delimited files. Whether that be a comma separated (csv) or a tab delimited file, there are multiple functions that can read these data into R. We will stick to loading these data from the tidyverse packages but be aware these are not the only methods for doing this. We will use the tidyverse functions just to maintain consistency with everything else we do. The first package in tidyverse we will use is called readr (Wickham and Hester 2020), which is a collection of functions to load the tabular data from working directory in our machine into R session. Some of its functions include: read_csv(): comma separated (CSV) files read_tsv(): tab separated files read_delim(): general delimited files read_fwf(): fixed width files read_table(): tabular files where columns are separated by white-space. read_log(): web log files readxl reads in Excel files. You can lean R with the dataset it comes with when you install it in your machine. But sometimes you want to use the real data you or someone gathered already. One of critical steps for data processing is to import data with special format into R workspace.Data import refers to read data from the working directory into the workspace (Wickham and Hester 2020). In this chapter you will learn how to import common files into R. We will only focus on two common types of tabular data storage formatThe comma-seprated .csv and excell spreadsheet (.xlsx). In later chapter we will explain how to read other types of data into R. 4.2 Comma-Separated (.csv) The most commonly format that R like is the comma-separated files. Although Base R provides various functions like read.table(), read.csv(), read.table() and read.csv2() to import data from the local directories into R workspace, for this book we use an read_csv() function from readr. Before we import the data, we need to load the packages that we will use their functions in this chapter require(dplyr) require(readr) require(lubridate) require(readxl) require(haven) require(ggplot2) require(kableExtra) Consider a tabular data stored in my working directory in the .csv format in figure 4.1. Figure 4.1: A screenshot of the sample dataset We can import it with the read_csv() functions as: algoa.ctd = read_csv(&quot;data/algoa_ctd.csv&quot;) -- Column specification ------------------------------ cols( station = col_character(), time = col_datetime(format = &quot;&quot;), lon = col_double(), lat = col_double(), pressure = col_double(), temperature = col_double(), salinity = col_double(), oxygen = col_double(), fluorescence = col_double(), spar = col_double(), par = col_double(), density = col_double() ) When read_csv() has imported the data into R workspace, it prints out the name and type of of data for each variable. By simply glimpse the dataset, we see the format of the data is as expected. It has six variables(columns) and 177 observations (rows) similar to figure 4.1. Table 4.1 show sample of imported dataset. Table 4.1: CTD profiles Station Time Lon Lat Pressure Temperature Salinity Oxygen Fluorescence st1 2004-08-18 40.61 -10.54 5 25.17 33.92 3.93 0.56 st1 2004-08-18 40.61 -10.54 10 25.13 34.86 4.49 0.60 st1 2004-08-18 40.61 -10.54 15 25.11 34.86 4.50 0.65 st1 2004-08-18 40.61 -10.54 20 25.04 34.86 4.51 0.68 st1 2004-08-18 40.61 -10.54 25 24.95 34.86 4.51 0.76 st1 2004-08-18 40.61 -10.54 30 24.91 34.86 4.50 0.73 st1 2004-08-18 40.61 -10.54 35 24.88 34.87 4.49 0.74 st1 2004-08-18 40.61 -10.54 40 24.85 34.87 4.48 0.69 st1 2004-08-18 40.61 -10.54 45 24.80 34.88 4.46 0.70 st1 2004-08-18 40.61 -10.54 50 24.61 34.89 4.44 0.75 4.3 Microsoft Excel(.xlsx) Commonly our data is stored as a MS Excel file. we can import the file with read_xlsx() function of readxl package. The readxl package provides a function read_exel() that allows us to specify which sheet within the Excel file to read and what character specifies missing data (it assumes a blank cell is missing data if you dont specifying anything). The function automatically convert the worksheet into a .csv file and read it. Lets us import the the data in first sheet of the primary_productivity.xlsx. The dataset contain primary productivity value. We will use this file to illustrate how to import the excel file into R workspace with readxl package (Wickham and Bryan 2019). sheet1 = readxl::read_xlsx(&quot;data/primary_productivity.xlsx&quot;, sheet = 1) sheet1 %&gt;% sample_n(5) # A tibble: 5 x 3 DateTime `MODIS 1 Month ~ &lt;dttm&gt; &lt;dbl&gt; 1 2016-06-01 00:00:00 2014 2 2016-12-01 00:00:00 2016 3 2016-01-01 00:00:00 2015 4 2016-02-01 00:00:00 2003 5 2016-11-01 00:00:00 2018 # ... with 1 more variable: `MODIS 1 Month PP # (value)` &lt;dbl&gt; By printing the sheet1, we notice that the sheet contains monthly average value of primary productivity from the Pemba channel. sheet2 = readxl::read_xlsx(&quot;./data/primary_productivity.xlsx&quot;, sheet = 2) sheet2 %&gt;% sample_n(5) # A tibble: 5 x 3 DateTime `MODIS 1 Month ~ &lt;dttm&gt; &lt;dbl&gt; 1 2016-06-01 00:00:00 2012 2 2016-12-01 00:00:00 2005 3 2016-08-01 00:00:00 2017 4 2016-12-01 00:00:00 2003 5 2016-06-01 00:00:00 2011 # ... with 1 more variable: `MODIS 1 Month PP # (value)` &lt;dbl&gt; sheet2 contains monthly average value of primary productivity from the Zanzibar channel. sheet3 = readxl::read_xlsx(&quot;./data/primary_productivity.xlsx&quot;, sheet = 3) sheet3 %&gt;% sample_n(5) # A tibble: 5 x 3 DateTime `MODIS 1 Month ~ &lt;dttm&gt; &lt;dbl&gt; 1 2016-12-01 00:00:00 2012 2 2016-04-01 00:00:00 2018 3 2016-06-01 00:00:00 2013 4 2016-01-01 00:00:00 2015 5 2016-09-01 00:00:00 2004 # ... with 1 more variable: `MODIS 1 Month PP # (value)` &lt;dbl&gt; sheet3 contains monthly average value of primary productivity from the Mafia channel. We look on the internal structure of the sheet3 file with the glimpse() function. You can interact with the table that show all variables and observations (Table ??) sheet3%&gt;%glimpse() Rows: 192 Columns: 3 $ DateTime &lt;dttm&gt; 2016-01-01, 2... $ `MODIS 1 Month PP (y)` &lt;dbl&gt; 2003, 2004, 20... $ `MODIS 1 Month PP (value)` &lt;dbl&gt; 1311.5010, 106... sheet2 %&gt;% DT::datatable(rownames = FALSE, caption = &quot;An Interactive table of primary productivity in the Zanzibar channel&quot;) 4.4 Writing t a File Sometimes you work in the document and you want to export to a file. readr has write_csv() and write_tsv() functions that allows to export data frames from workspace to working directory write_csv(x = sheet1, path = &quot;./data/Primary_productivity_Pemba.csv&quot;) Wickham and Bryan (2019) recommend the use of write_excel_csv() function when you want to export a data frame to Excel. readr has other tools that export files to other software like SAS, SPSS and more  write_excel_csv(x = sheet1, path = &quot;./data/Primary_productivity_Pemba.csv&quot;) 4.5 Basic Data Manipulation In this section, we briefly introduce some basic data handling and manipulation techniques, which are mostly associated with data frame. A data frame is a a tabular shaped contains columns and rows of equal length. In general a data frame structure with rows representing observations or measurements and with columns containing variables. 4.5.1 Explore the Data Frame We can visualize the table by simply run the name of the data flights octopus = read_csv(&quot;./data/octopus_data.csv&quot;) we can use class() to check if the data is data frame octopus %&gt;% class() [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; [4] &quot;data.frame&quot; We can use names() to extract the variable names octopus %&gt;% names() [1] &quot;date&quot; &quot;village&quot; &quot;port&quot; &quot;ground&quot; &quot;sex&quot; [6] &quot;dml&quot; &quot;tl&quot; &quot;weight&quot; &quot;lat&quot; &quot;lon&quot; We can explore the internal structure of flights object with a dplyr()s function glimpse() octopus %&gt;% glimpse() Rows: 1,079 Columns: 10 $ date &lt;date&gt; 2018-02-12, 2018-01-30, 2018-02-... $ village &lt;chr&gt; &quot;Somanga&quot;, &quot;Bwejuu&quot;, &quot;Somanga&quot;, &quot;... $ port &lt;chr&gt; &quot;Mbuyuni&quot;, &quot;Kusini&quot;, &quot;Mbuyuni&quot;, &quot;... $ ground &lt;chr&gt; &quot;CHAMBA CHA MACHANGE&quot;, &quot;NYAMALILE... $ sex &lt;chr&gt; &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;... $ dml &lt;dbl&gt; 14.0, 14.5, 17.0, 20.0, 12.0, 16.... $ tl &lt;dbl&gt; 110.0, 115.0, 115.0, 130.0, 68.0,... $ weight &lt;dbl&gt; 1.385, 1.750, 1.000, 2.601, 0.670... $ lat &lt;dbl&gt; -8.397838, -7.915809, -8.392644, ... $ lon &lt;dbl&gt; 39.28079, 39.65424, 39.28153, 39.... We can check how rows (observations/measurements) and columns (variables/fields) are in the data octopus %&gt;% dim() [1] 1079 10 The number of rows (observation) can be obtained using nrow() function octopus %&gt;% nrow() [1] 1079 The number of columns (variables) can be obtained using ncol() function octopus %&gt;% ncol() [1] 10 The length of the data frame is given by octopus %&gt;% length() [1] 10 Count the number of sample at each sex of octopus octopus %$% table(sex) sex F M 581 498 Count the number and compute the proportion of sample at each sex of octopus octopus %$% table(sex) %&gt;% prop.table() %&gt;% round(digits = 2) sex F M 0.54 0.46 4.5.2 simmple summary statistics The most helpful function for for summarizing rows and columns is summary(), which gives a collection of basim cummary statistics. The first method is to calculate some basic summary statistics (minimum, 25th, 50th, 75th percentiles, maximum and mean) of each column. If a column is categorical, the summary function will return the number of observations in each category. octopus %&gt;% summary() date village Min. :2017-12-18 Length:1079 1st Qu.:2018-01-14 Class :character Median :2018-01-20 Mode :character Mean :2018-01-26 3rd Qu.:2018-02-15 Max. :2018-03-12 port ground Length:1079 Length:1079 Class :character Class :character Mode :character Mode :character sex dml tl Length:1079 Min. : 6.0 Min. : 11.00 Class :character 1st Qu.:10.0 1st Qu.: 68.00 Mode :character Median :12.0 Median : 82.00 Mean :12.8 Mean : 86.01 3rd Qu.:15.0 3rd Qu.:100.00 Max. :24.0 Max. :180.00 weight lat lon Min. :0.055 Min. :-8.904 Min. : 0.00 1st Qu.:0.600 1st Qu.:-8.523 1st Qu.:39.28 Median :0.915 Median :-8.392 Median :39.50 Mean :1.232 Mean :-8.069 Mean :38.69 3rd Qu.:1.577 3rd Qu.:-7.973 3rd Qu.:39.67 Max. :5.210 Max. : 0.000 Max. :39.75 You noticed that the summary() function provide the common metric for central tendency and measure of dispersion. We will look at them later. Now we turn to our favourite package dplyr References "],["tidy.html", "Chapter 5 Reshaping data with tidyr 5.1 What is Tidy Data? 5.2 Gatherfrom wide to long format. 5.3 spreadfrom long to wide format 5.4 separate columns 5.5 unite columns", " Chapter 5 Reshaping data with tidyr 5.1 What is Tidy Data? Most statistical datasets are data frames made up of rows and columns. The columns are almost always labeled and the rows are sometimes labeled. One of the key task in data preparation is to organize a dataset in a way that makes analysis and plottng easier. In practice, the data is often not stored like that and the data comes to us with repeated observations included on a single row. This is often done as a memory saving technique or because there is some structure in the data that makes the wide format attractive. As a result, we need a way to convert data from wide1 to long2 and vice-versa (Semba and Peter 2020). Structuring data frames to have the desired shape can be the most daunting part of statistical analysis, visualization, and modeling. Several studies reported that 80% of data analysis is spent on the cleaning and preparing data. Tidy in this context means organize the data in a presentable and consistent format that facilitate data analysis and visualization. When you are doing data preparation in R for analysis or plottng, the first thing you do is a thoroughly mental thought on the desired structure of that data frame. You need to determine what each row and column will represent, so that you can consistently and clearly manipulate that data (e.g., you know what you will be selecting and what you will be filtering). There are basically three principles that we can follow to make a tidy dataset. First each variable must have its own a column, second each observation must have its own row, and finally, each cell must have its own value. The tidyr package is designed to structure and work with data frames that follow three principles of tidy data. There are advantages of using tidy data in R. First, having a consistent, uniform data structure is very important. Popular packages like dplyr (Wickham, François, et al. 2020), ggplot2 (Wickham 2016), and all the other packages in the tidyverse (Wickham, Averick, et al. 2019) and their extensions like sf (Pebesma 2018), metR (Campitelli 2019), ggspatial (Dunnington 2020), ggrepel (Slowikowski 2020) etc are designed to work with tidy data (Wickham 2020). So consistent of tidy data ensure efficient processing, analysis and plotting of data. Third, placing variables into columns, facilities easy vectorization in R. Unfortunate, Many datasets that you receive are untidy and will require some work on your end. There are several reasons why a dataset is messy. Often times the people who created the dataset arent familiar with the principles of tidy data. Another common reason that most datasets are messy is that data is often organized to facilitate something other than analysis. Data entry is perhaps the most common of the reasons that fall into this category. To make data entry as easy as possible, people will often arrange data in ways that arent tidy. So, many datasets require some sort of tidying before you can begin your analysis. As Wickham and Grolemund (2016) put it tidy data means that yo can plot or summarize the data efficiently. In othet words, it comes down to which data is represented as columns in a data frame and which is not.In principle, this means that there is column in the data frame that you can work with for the analysis you want to do. For example, if I want to look at the ctd dataset and see how the fluorescence varies among station in the upper water surface, we simply plot a boxplot of the station column against the fluorescence column shown in figure 5.1 Figure 5.1: Fluorescence variation against stations This works because I have a column for the x-axis and another for the y-axis. But what happens if I want to plot the different measurements of the irises to see how those are? Each measurement is a separate column. They are Petal.Length, Petal.Width, and so on.Now I have a bit of a problem because the different measurements are in different columns in my data frame. I cannot easily map them to an x-axis and a y-axis.The tidyr package addresses that. It contains functions for both mapping columns to values, widely recognised as long format and for mapping back from values to columnswide format. We are going to look for the function that are regularly used to tidy the data frames. These inlude: Gathering Spreading Separating Uniting 5.2 Gatherfrom wide to long format. Look at example of dataset. It has one common problem that the column names are not variables but rather values of a variable. In the table 5.1, the columns are actually values of the variable pressure. Each row in the existing table actually represents five observations from each station. Table 5.1: Wide format format Water Depth (meters) station 10 20 30 40 50 60 70 80 90 100 110 st1 0.599 0.678 0.729 0.693 0.752 1.098 0.857 0.481 0.313 0.221 0.138 st2 0.631 0.733 0.992 1.114 0.988 0.715 0.496 0.524 0.280 0.277 0.225 st3 0.980 0.934 1.149 1.200 1.187 1.035 0.854 0.530 0.437 0.347 0.324 st4 NA 0.623 0.603 0.742 0.724 0.799 0.914 0.819 0.801 0.692 0.444 st5 NA 0.350 0.415 0.421 0.566 0.592 0.591 0.634 0.751 0.774 0.575 The tidyr package can be used to gather these existing columns into a new variable. In this case, we need to create a new column called pressure and then gather the existing values in the these variable columns into the new pressure variable ctd.long = ctd.wide %&gt;% gather (key = &quot;depth&quot;, value = &quot;fluorescence &quot;, 2:12) ctd.long # A tibble: 55 x 3 station depth `fluorescence ` &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 st1 10 0.599 2 st2 10 0.631 3 st3 10 0.98 4 st4 10 NA 5 st5 10 NA 6 st1 20 0.678 7 st2 20 0.733 8 st3 20 0.934 9 st4 20 0.623 10 st5 20 0.350 # ... with 45 more rows As you can see from the chunk above, there are three arguments required in the gather() function. First is the key, which takes the variable names. Second, the valuethe name of the variable whose values are spread over the cells. Finnaly, then you specify the set of columns that hold the values and not the variable names 5.3 spreadfrom long to wide format A second tidy tool is spread(), which does the opposite of gather() function. It is used to convert a long format data frame to wide format. What this function does is to spread observation across multiple rows. ctd.long %&gt;% spread(key = &quot;depth&quot;, value = `fluorescence `) # A tibble: 5 x 12 station `10` `100` `110` `20` `30` `40` `50` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st1 0.599 0.221 0.138 0.678 0.729 0.693 0.752 2 st2 0.631 0.277 0.225 0.733 0.992 1.11 0.988 3 st3 0.98 0.347 0.324 0.934 1.15 1.20 1.19 4 st4 NA 0.692 0.444 0.623 0.603 0.742 0.724 5 st5 NA 0.774 0.575 0.350 0.415 0.421 0.566 # ... with 4 more variables: `60` &lt;dbl&gt;, `70` &lt;dbl&gt;, # `80` &lt;dbl&gt;, `90` &lt;dbl&gt; The spread() function takes two arguments: the column that contains variable names, known as the key and a column that contains values from multiple variables  the value. 5.4 separate columns Another common in tidyr package is a separate ()function, which split the variable into two or more variables. For example, the dataset below has a date column that actually contains the date and time variables separated by a space. ctd # A tibble: 115 x 12 station time lon lat pressure &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st1 2004-08-18 15:27:46 40.6 -10.5 5 2 st1 2004-08-18 15:27:46 40.6 -10.5 10 3 st1 2004-08-18 15:27:46 40.6 -10.5 15 4 st1 2004-08-18 15:27:46 40.6 -10.5 20 5 st1 2004-08-18 15:27:46 40.6 -10.5 25 6 st1 2004-08-18 15:27:46 40.6 -10.5 30 7 st1 2004-08-18 15:27:46 40.6 -10.5 35 8 st1 2004-08-18 15:27:46 40.6 -10.5 40 9 st1 2004-08-18 15:27:46 40.6 -10.5 45 10 st1 2004-08-18 15:27:46 40.6 -10.5 50 # ... with 105 more rows, and 7 more variables: # temperature &lt;dbl&gt;, salinity &lt;dbl&gt;, oxygen &lt;dbl&gt;, # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, # density &lt;dbl&gt; We use the separate() function splits the datetime column into two variables: date and time ctd %&gt;% separate(col = time, into = c(&quot;Date&quot;, &quot;Time&quot;), sep = &quot; &quot;) # A tibble: 115 x 13 station Date Time lon lat pressure &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st1 2004~ 15:2~ 40.6 -10.5 5 2 st1 2004~ 15:2~ 40.6 -10.5 10 3 st1 2004~ 15:2~ 40.6 -10.5 15 4 st1 2004~ 15:2~ 40.6 -10.5 20 5 st1 2004~ 15:2~ 40.6 -10.5 25 6 st1 2004~ 15:2~ 40.6 -10.5 30 7 st1 2004~ 15:2~ 40.6 -10.5 35 8 st1 2004~ 15:2~ 40.6 -10.5 40 9 st1 2004~ 15:2~ 40.6 -10.5 45 10 st1 2004~ 15:2~ 40.6 -10.5 50 # ... with 105 more rows, and 7 more variables: # temperature &lt;dbl&gt;, salinity &lt;dbl&gt;, oxygen &lt;dbl&gt;, # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, # density &lt;dbl&gt; The separate() function accepts arguments for the name of the variable to separate. You also need to specify the names of the variable to separate into, and an optional separator. 5.5 unite columns The unite()function is the exact opposite of separate() in that it combines multiple columns into a single column. While not used nearly as often as separate() , there may be times when you need the functionality provided by unite(). For example, we can combine the variable Date and Time to form siku.muda, and separate them with :-: symbol between the two variables. ctd.un %&gt;% unite(col = &quot;siku_muda&quot;, c(&quot;Date&quot;, &quot;Time&quot;), sep = &quot;:-:&quot;) # A tibble: 115 x 12 station siku_muda lon lat pressure temperature &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st1 2004-08-~ 40.6 -10.5 5 25.2 2 st1 2004-08-~ 40.6 -10.5 10 25.1 3 st1 2004-08-~ 40.6 -10.5 15 25.1 4 st1 2004-08-~ 40.6 -10.5 20 25.0 5 st1 2004-08-~ 40.6 -10.5 25 24.9 6 st1 2004-08-~ 40.6 -10.5 30 24.9 7 st1 2004-08-~ 40.6 -10.5 35 24.9 8 st1 2004-08-~ 40.6 -10.5 40 24.9 9 st1 2004-08-~ 40.6 -10.5 45 24.8 10 st1 2004-08-~ 40.6 -10.5 50 24.6 # ... with 105 more rows, and 6 more variables: # salinity &lt;dbl&gt;, oxygen &lt;dbl&gt;, fluorescence &lt;dbl&gt;, # spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt; References "],["dplyr.html", "Chapter 6 Manipulating Data with dplyr 6.1 Why use dplyr? 6.2 Core dplyr Functions 6.3 Data 6.4 Choosing rows: Filtering observations 6.5 select 6.6 Summarizing and Grouping", " Chapter 6 Manipulating Data with dplyr Before a dataset can be analysed in R, its often manipulated or transformed in various ways. For years manipulating data in R required more programming than actually analyzing data. That has improved dramatically with the dplyr package. It provides programmers with an intuitive vocabulary for executing data management and analysis tasks. Hadley Wickham (2020), the original creator of the dplyr package, refers to it as a Grammar of Data Manipulation. Because the package provides a set of functions (verbs) that let you modify data and perform common data preparation tasks. The key challenge in programming is mapping from questions about a data set to specific programming operations. With dplyrs verbs, makes this process smoother, as it enables you to use the same vocabulary to both ask questions and write your code. In other words, the dplyr verbs lets you easily talk with data and transform a dataset in various ways with easy. 6.1 Why use dplyr? Using this packages functions will allow you to code expressivelycode that are easy to write and read, which make you effective and efficient data scientists. Great for data exploration and manipulation Intuitive to write and easy to read, especially when using the chaining syntax Fast on data frametabular dataset 6.2 Core dplyr Functions I will not go through all of the dplyr functions in this chapter. I will demonstrate the core functions that are used regularly for manipulating data. The five core functions also called verbs include: select() to select columns based on their names filter() to rows in data frame arrange() to re-order or arrange the rows in ascending or descending order mutate() to create new columnsadd new variable summarise() to make a summary of variable(s) group_by() to group observation sample_n() and rename()to make random sample from the data set The group_by() function perform other common task which are related to the split-apply-combine concept. You can use these verbs when you describe the algorithm or process for interrogating data, and then use dplyr verbs to write code that will closely follow your plain language description because it uses functions and procedures that share the same language. For most of us who are familiar with the R base function, you will find that most dplyr functions on data frames can be expressed succinctly because you dont need to repeat the name of the data frame. This becomes handy in operation, because dplyr package comes with the pipe operateor %&gt;% from the magrittr package (Bache and Wickham 2020), which allows to combine several functions in a chain to manipulate data. To use dplyr functions to manipulate your data, it must be already installed in your machine so that you can load with a require () function. Once the package is loaded, its functions are available for use. dplyr is a key package of the tidyverse ecosystema collection of R packages, which also includes other packages like, readr (Wickham and Hester 2020), purr,tibble (Müller and Wickham 2020), stringr (Wickham 2019), forcats, tidyr (Wickham 2020) and ggplot2 (Wickham 2016). require(tidyverse) 6.3 Data Data frames are ideal for representing data where each row is an observations and each column is a variable. Nearly all packages in a tidyverse work on data frames new version called tibble. A tibble provides stricter checking and better formatting than the traditional data frame. To demonstrate the usefulness of the dplyr package for manipulating data, we will use the CTD data of 22 stations casted along the coastal water of Tanzania. I have prepared the data, cleaned and align the profile into 5 meter standard depth for each cast and merged them into a single .csv file. You need to load the file into your R session. We can import the file with read_csv() function from the readr package (Wickham and Hester 2020). The read_csv() function gives out a tibble (Müller and Wickham 2020). ctd = read_csv(&quot;data/algoa_ctd.csv&quot;) 6.4 Choosing rows: Filtering observations The first dplyr verb well explore is filter(). This function is primarily used to create a subset of observations that meet a specified conditions. The filter() function lets you pick out rows based on logical expressions. You give the function a predicate, specifying what a row should satisfy to be included. For instance, take a look at the chunk below: surface = ctd %&gt;% filter(pressure &lt; 150) surface # A tibble: 637 x 12 station time lon lat pressure &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st1 2004-08-18 15:27:46 40.6 -10.5 5 2 st1 2004-08-18 15:27:46 40.6 -10.5 10 3 st1 2004-08-18 15:27:46 40.6 -10.5 15 4 st1 2004-08-18 15:27:46 40.6 -10.5 20 5 st1 2004-08-18 15:27:46 40.6 -10.5 25 6 st1 2004-08-18 15:27:46 40.6 -10.5 30 7 st1 2004-08-18 15:27:46 40.6 -10.5 35 8 st1 2004-08-18 15:27:46 40.6 -10.5 40 9 st1 2004-08-18 15:27:46 40.6 -10.5 45 10 st1 2004-08-18 15:27:46 40.6 -10.5 50 # ... with 627 more rows, and 7 more variables: # temperature &lt;dbl&gt;, salinity &lt;dbl&gt;, oxygen &lt;dbl&gt;, # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, # density &lt;dbl&gt; The expression calls the ctd dataset and feed into the filter()and pick all observations with pressure below 150meters and create a new datase called surface. This is an expression where a single conditional statement is used. We can also limit the of the variable of interest by combining multiple conditional expressions as part of the filter(). Each expression (argument) is combined with an AND clause by default. This means that all expressions must be matched for a recorded to be returned. For instance we want to pick observations that were measured between 5 and 10 meters water only. We combine theses expressions with &amp; operator; ctd %&gt;% filter(pressure &gt;= 0 &amp; pressure &lt;= 10) # A tibble: 44 x 12 station time lon lat pressure &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st1 2004-08-18 15:27:46 40.6 -10.5 5 2 st1 2004-08-18 15:27:46 40.6 -10.5 10 3 st2 2004-08-18 17:00:01 40.8 -10.5 5 4 st2 2004-08-18 17:00:01 40.8 -10.5 10 5 st3 2004-08-18 20:32:54 41.0 -10.5 5 6 st3 2004-08-18 20:32:54 41.0 -10.5 10 7 st4 2004-08-18 22:44:56 41.1 -10.5 5 8 st4 2004-08-18 22:44:56 41.1 -10.5 10 9 st5 2004-08-19 00:59:59 41.3 -10.5 5 10 st5 2004-08-19 00:59:59 41.3 -10.5 10 # ... with 34 more rows, and 7 more variables: # temperature &lt;dbl&gt;, salinity &lt;dbl&gt;, oxygen &lt;dbl&gt;, # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, # density &lt;dbl&gt; We can also use the between() function, which is equivalent to pressure &gt;= 0 &amp; pressure &lt;= 10 in above chunk to achive the same result. ctd %&gt;% filter(between(pressure, 5,10)) # A tibble: 44 x 12 station time lon lat pressure &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st1 2004-08-18 15:27:46 40.6 -10.5 5 2 st1 2004-08-18 15:27:46 40.6 -10.5 10 3 st2 2004-08-18 17:00:01 40.8 -10.5 5 4 st2 2004-08-18 17:00:01 40.8 -10.5 10 5 st3 2004-08-18 20:32:54 41.0 -10.5 5 6 st3 2004-08-18 20:32:54 41.0 -10.5 10 7 st4 2004-08-18 22:44:56 41.1 -10.5 5 8 st4 2004-08-18 22:44:56 41.1 -10.5 10 9 st5 2004-08-19 00:59:59 41.3 -10.5 5 10 st5 2004-08-19 00:59:59 41.3 -10.5 10 # ... with 34 more rows, and 7 more variables: # temperature &lt;dbl&gt;, salinity &lt;dbl&gt;, oxygen &lt;dbl&gt;, # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, # density &lt;dbl&gt; In the next example, two conditional expressions are passed. The first is used to filter surface water below 200 m, and the second statement filter records that above latitude 6°S ctd %&gt;% filter(pressure &lt; 200 &amp; lat &gt; -6) # A tibble: 223 x 12 station time lon lat pressure &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st17 2004-08-23 19:42:30 40.1 -5.49 5 2 st17 2004-08-23 19:42:30 40.1 -5.49 10 3 st17 2004-08-23 19:42:30 40.1 -5.49 15 4 st17 2004-08-23 19:42:30 40.1 -5.49 20 5 st17 2004-08-23 19:42:30 40.1 -5.49 25 6 st17 2004-08-23 19:42:30 40.1 -5.49 30 7 st17 2004-08-23 19:42:30 40.1 -5.49 35 8 st17 2004-08-23 19:42:30 40.1 -5.49 40 9 st17 2004-08-23 19:42:30 40.1 -5.49 45 10 st17 2004-08-23 19:42:30 40.1 -5.49 50 # ... with 213 more rows, and 7 more variables: # temperature &lt;dbl&gt;, salinity &lt;dbl&gt;, oxygen &lt;dbl&gt;, # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, # density &lt;dbl&gt; In this case, the surface.transect dataset has records where both conditions are metthe pressure is blow 200 meter and latitude above -6. Note that when two or more conditions are paased, the &amp; operator is used. You may sometimes want to know stations and at what depth a particular variable has missing values. You can pick all variable in the data frame using is.na() function. ctd %&gt;% filter(is.na(fluorescence)) # A tibble: 7 x 12 station time lon lat pressure &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st3 2004-08-18 20:32:54 41.0 -10.5 5 2 st4 2004-08-18 22:44:56 41.1 -10.5 5 3 st4 2004-08-18 22:44:56 41.1 -10.5 10 4 st5 2004-08-19 00:59:59 41.3 -10.5 5 5 st5 2004-08-19 00:59:59 41.3 -10.5 10 6 st10 2004-08-19 19:36:50 39.7 -8.83 5 7 st10 2004-08-19 19:36:50 39.7 -8.83 10 # ... with 7 more variables: temperature &lt;dbl&gt;, # salinity &lt;dbl&gt;, oxygen &lt;dbl&gt;, fluorescence &lt;dbl&gt;, # spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt; You can also drop the observation with missing values in the data frame using the !is.na() operator ctd %&gt;% filter(!is.na(fluorescence)) # A tibble: 2,789 x 12 station time lon lat pressure &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st1 2004-08-18 15:27:46 40.6 -10.5 5 2 st1 2004-08-18 15:27:46 40.6 -10.5 10 3 st1 2004-08-18 15:27:46 40.6 -10.5 15 4 st1 2004-08-18 15:27:46 40.6 -10.5 20 5 st1 2004-08-18 15:27:46 40.6 -10.5 25 6 st1 2004-08-18 15:27:46 40.6 -10.5 30 7 st1 2004-08-18 15:27:46 40.6 -10.5 35 8 st1 2004-08-18 15:27:46 40.6 -10.5 40 9 st1 2004-08-18 15:27:46 40.6 -10.5 45 10 st1 2004-08-18 15:27:46 40.6 -10.5 50 # ... with 2,779 more rows, and 7 more variables: # temperature &lt;dbl&gt;, salinity &lt;dbl&gt;, oxygen &lt;dbl&gt;, # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, # density &lt;dbl&gt; When you have string variable in the data frame with character or factor format, you can filter the certain observation with %in% statement. For example, to obtain profiles from three stations: st1, st8, and st13, we can write the code as; ctd %&gt;% filter(station %in% c(&quot;st1&quot;, &quot;st8&quot;, &quot;st13&quot;)) # A tibble: 347 x 12 station time lon lat pressure &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st1 2004-08-18 15:27:46 40.6 -10.5 5 2 st1 2004-08-18 15:27:46 40.6 -10.5 10 3 st1 2004-08-18 15:27:46 40.6 -10.5 15 4 st1 2004-08-18 15:27:46 40.6 -10.5 20 5 st1 2004-08-18 15:27:46 40.6 -10.5 25 6 st1 2004-08-18 15:27:46 40.6 -10.5 30 7 st1 2004-08-18 15:27:46 40.6 -10.5 35 8 st1 2004-08-18 15:27:46 40.6 -10.5 40 9 st1 2004-08-18 15:27:46 40.6 -10.5 45 10 st1 2004-08-18 15:27:46 40.6 -10.5 50 # ... with 337 more rows, and 7 more variables: # temperature &lt;dbl&gt;, salinity &lt;dbl&gt;, oxygen &lt;dbl&gt;, # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, # density &lt;dbl&gt; 6.5 select The second verb we are going to demonstrate is the select() function. Often you work with large datasets with many columns but only a few are actually of interest to you. The select() function selects columns of the data frame. select() function allows you to choose variables that are of interest. You can use it to pick out a some columns from the dataset. For instance, fi we want pressure, temprature, salinity, fluorescence and ovygen variables from the data frame, we can simply write a code as; ctd %&gt;% select (pressure, temperature, salinity, fluorescence, oxygen) # A tibble: 2,796 x 5 pressure temperature salinity fluorescence oxygen &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 5 25.2 33.9 0.560 3.93 2 10 25.1 34.9 0.599 4.49 3 15 25.1 34.9 0.650 4.50 4 20 25.0 34.9 0.678 4.51 5 25 24.9 34.9 0.760 4.51 6 30 24.9 34.9 0.729 4.50 7 35 24.9 34.9 0.740 4.49 8 40 24.9 34.9 0.693 4.48 9 45 24.8 34.9 0.703 4.46 10 50 24.6 34.9 0.752 4.44 # ... with 2,786 more rows Besides just selecting columns, you can use a minus sign to remove variables you do not need from the data frame. ctd %&gt;% select(-spar, -par, -density, -time) # A tibble: 2,796 x 8 station lon lat pressure temperature salinity &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st1 40.6 -10.5 5 25.2 33.9 2 st1 40.6 -10.5 10 25.1 34.9 3 st1 40.6 -10.5 15 25.1 34.9 4 st1 40.6 -10.5 20 25.0 34.9 5 st1 40.6 -10.5 25 24.9 34.9 6 st1 40.6 -10.5 30 24.9 34.9 7 st1 40.6 -10.5 35 24.9 34.9 8 st1 40.6 -10.5 40 24.9 34.9 9 st1 40.6 -10.5 45 24.8 34.9 10 st1 40.6 -10.5 50 24.6 34.9 # ... with 2,786 more rows, and 2 more variables: # oxygen &lt;dbl&gt;, fluorescence &lt;dbl&gt; ## or you can bind the variable you want to remove ctd %&gt;% select(-c(spar, par, density, time)) # A tibble: 2,796 x 8 station lon lat pressure temperature salinity &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st1 40.6 -10.5 5 25.2 33.9 2 st1 40.6 -10.5 10 25.1 34.9 3 st1 40.6 -10.5 15 25.1 34.9 4 st1 40.6 -10.5 20 25.0 34.9 5 st1 40.6 -10.5 25 24.9 34.9 6 st1 40.6 -10.5 30 24.9 34.9 7 st1 40.6 -10.5 35 24.9 34.9 8 st1 40.6 -10.5 40 24.9 34.9 9 st1 40.6 -10.5 45 24.8 34.9 10 st1 40.6 -10.5 50 24.6 34.9 # ... with 2,786 more rows, and 2 more variables: # oxygen &lt;dbl&gt;, fluorescence &lt;dbl&gt; You can drop a range of variables in the data frame with select() function. For instance, the code below drop all variables beween temperature to fluorescence. You can also select those variables in range by removing the negative sign # hide a range of columns ctd %&gt;% select(-(temperature:fluorescence)) # A tibble: 2,796 x 8 station time lon lat pressure &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st1 2004-08-18 15:27:46 40.6 -10.5 5 2 st1 2004-08-18 15:27:46 40.6 -10.5 10 3 st1 2004-08-18 15:27:46 40.6 -10.5 15 4 st1 2004-08-18 15:27:46 40.6 -10.5 20 5 st1 2004-08-18 15:27:46 40.6 -10.5 25 6 st1 2004-08-18 15:27:46 40.6 -10.5 30 7 st1 2004-08-18 15:27:46 40.6 -10.5 35 8 st1 2004-08-18 15:27:46 40.6 -10.5 40 9 st1 2004-08-18 15:27:46 40.6 -10.5 45 10 st1 2004-08-18 15:27:46 40.6 -10.5 50 # ... with 2,786 more rows, and 3 more variables: # spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt; Just like you can pick columns with the matching name, you can also drop any column with a matching name ctd %&gt;% select(-contains(&quot;t&quot;)) # A tibble: 2,796 x 6 lon pressure oxygen fluorescence spar par &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 40.6 5 3.93 0.560 1177. 53.9 2 40.6 10 4.49 0.599 1151. 40.3 3 40.6 15 4.50 0.650 1135. 31.3 4 40.6 20 4.51 0.678 1124. 25.6 5 40.6 25 4.51 0.760 1111. 21.1 6 40.6 30 4.50 0.729 1103. 17.2 7 40.6 35 4.49 0.740 1097. 13.9 8 40.6 40 4.48 0.693 1091. 11.2 9 40.6 45 4.46 0.703 1087. 9.05 10 40.6 50 4.44 0.752 1084. 7.30 # ... with 2,786 more rows Because of the naming conventions, many of the column names that you import dont make sense. You will often need to change the name of the variable. select() function allows you to accomplish that. For example, we want to select station, pressure and fluoresence, but we need also change the name of station to be Cast, pressure to Depth and fluorescence to Chlorophyll. You can achieve that with code written as; ctd %&gt;% select(Cast = station, Depth = pressure, Chlorophyll = fluorescence) # A tibble: 2,796 x 3 Cast Depth Chlorophyll &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st1 5 0.560 2 st1 10 0.599 3 st1 15 0.650 4 st1 20 0.678 5 st1 25 0.760 6 st1 30 0.729 7 st1 35 0.740 8 st1 40 0.693 9 st1 45 0.703 10 st1 50 0.752 # ... with 2,786 more rows There are also a number of handy helper functions that you can use with the select() function to filter the returned columns. These include starts_with(), ends_with(), contains(), matches(), and num_range(). I wont illustrate them here, however, you can consult the help document for more information. 6.5.1 Adding new variables: mutate, transmute, add_rownames Besides selecting sets of existing columns, its often useful to add new columns that are functions of existing columns. This is the job of mutate(): Any new variable created with the mutate() function will be added to the end of the data frame. For example, raw fluorescence values are often skewed (Figure 6.1a) and we often transform them to have normal distribution (figure 6.1b). Figure 6.1: Histogram showing the distribution of a) raw fluorence and b) log-transformed fluorescence values At this situation, its handy to add a new column with transformed values in the data frame as shown in the code; ctd %&gt;% select(pressure, fluorescence) %&gt;% mutate(log.fluorescence = fluorescence %&gt;% log10()) # A tibble: 2,796 x 3 pressure fluorescence log.fluorescence &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 5 0.560 -0.251 2 10 0.599 -0.223 3 15 0.650 -0.187 4 20 0.678 -0.169 5 25 0.760 -0.119 6 30 0.729 -0.138 7 35 0.740 -0.131 8 40 0.693 -0.159 9 45 0.703 -0.153 10 50 0.752 -0.124 # ... with 2,786 more rows The code tells important two steps: the first steps involved selecting the pressure and fluorescence variables, once these variables were selected fromt he ctd data frame were fed into a mutate() function, which computed the logarithmic of fluorescence and assign a new log.fluorescence variable into the data frame. In a similar way above, we can create a new variable of anomaly as the code below shows; ctd %&gt;% select(pressure, fluorescence) %&gt;% mutate(anomaly = fluorescence - mean(fluorescence, na.rm = TRUE)) # A tibble: 2,796 x 3 pressure fluorescence anomaly &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 5 0.560 0.425 2 10 0.599 0.464 3 15 0.650 0.515 4 20 0.678 0.542 5 25 0.760 0.624 6 30 0.729 0.593 7 35 0.740 0.604 8 40 0.693 0.557 9 45 0.703 0.568 10 50 0.752 0.617 # ... with 2,786 more rows 6.5.2 Arranging rows The arrange() function in the dplyr package can be used to order the rows in a data frame. This function accepts a set of columns to order by with the default row ordering being in ascending order. ctd %&gt;% arrange(pressure) # A tibble: 2,796 x 12 station time lon lat pressure &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st1 2004-08-18 15:27:46 40.6 -10.5 5 2 st2 2004-08-18 17:00:01 40.8 -10.5 5 3 st3 2004-08-18 20:32:54 41.0 -10.5 5 4 st4 2004-08-18 22:44:56 41.1 -10.5 5 5 st5 2004-08-19 00:59:59 41.3 -10.5 5 6 st6 2004-08-19 11:49:08 40.3 -8.83 5 7 st7 2004-08-19 13:33:31 40.2 -8.83 5 8 st8 2004-08-19 15:28:18 40.0 -8.83 5 9 st9 2004-08-19 17:39:39 39.8 -8.83 5 10 st10 2004-08-19 19:36:50 39.7 -8.83 5 # ... with 2,786 more rows, and 7 more variables: # temperature &lt;dbl&gt;, salinity &lt;dbl&gt;, oxygen &lt;dbl&gt;, # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, # density &lt;dbl&gt; By default, it orders numerical values in increasing order, but you can ask for decreasing order using the desc() function: ctd %&gt;% arrange(pressure %&gt;% desc()) # A tibble: 2,796 x 12 station time lon lat pressure &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st3 2004-08-18 20:32:54 41.0 -10.5 1015 2 st3 2004-08-18 20:32:54 41.0 -10.5 1010 3 st3 2004-08-18 20:32:54 41.0 -10.5 1005 4 st3 2004-08-18 20:32:54 41.0 -10.5 1000 5 st3 2004-08-18 20:32:54 41.0 -10.5 995 6 st3 2004-08-18 20:32:54 41.0 -10.5 990 7 st3 2004-08-18 20:32:54 41.0 -10.5 985 8 st3 2004-08-18 20:32:54 41.0 -10.5 980 9 st3 2004-08-18 20:32:54 41.0 -10.5 975 10 st3 2004-08-18 20:32:54 41.0 -10.5 970 # ... with 2,786 more rows, and 7 more variables: # temperature &lt;dbl&gt;, salinity &lt;dbl&gt;, oxygen &lt;dbl&gt;, # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, # density &lt;dbl&gt; 6.6 Summarizing and Grouping Summary statistics for a data frame can be produced with the summarise() function. The summarise() function produces a single row of data containing summary statistics from a data frame. For example, you can compute for the mean of fluorescence values: ctd %&gt;% summarise(fl.mean = mean(fluorescence, na.rm = TRUE)) # A tibble: 1 x 1 fl.mean &lt;dbl&gt; 1 0.118 By itself, its not that useful until chained with the group_by() verb to compute summary statistics. There you can split the data into different groups and compute the summaries for each group.For example, you can ask for the mean of and standard deviation values of fluorescence for each station in the data frame: ctd %&gt;% group_by(station) %&gt;% summarise(Mean = mean(fluorescence, na.rm = TRUE), STD = sd(fluorescence, na.rm = TRUE)) # A tibble: 5 x 3 station Mean STD &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st1 0.304 0.319 2 st13 0.0897 0.179 3 st18 0.101 0.287 4 st4 0.0970 0.233 5 st8 0.125 0.381 You can group by one or more variables; you just specify the columns you want to separate into different subsets to the function. It works best when grouping by factors or discrete numbers; there isnt much fun in grouping by real numbers. ctd %&gt;% group_by(station, lon)%&gt;% summarise(Mean = mean(fluorescence, na.rm = TRUE), STD = sd(fluorescence, na.rm = TRUE)) # A tibble: 5 x 4 # Groups: station [5] station lon Mean STD &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st1 40.6 0.304 0.319 2 st13 40.1 0.0897 0.179 3 st18 39.9 0.101 0.287 4 st4 41.1 0.0970 0.233 5 st8 40.0 0.125 0.381 summarise() can be used to count the number of rows in each group with nc()which just counts how many observations you have in a subset of your data: You only need to parse the argument n() in the summarise()` function as; ctd %&gt;% group_by(station) %&gt;% summarise(frequency = n()) # A tibble: 5 x 2 station frequency &lt;chr&gt; &lt;int&gt; 1 st1 50 2 st13 135 3 st18 163 4 st4 186 5 st8 162 References "],["plotting-in-r-with-ggplot2.html", "Chapter 7 Plotting in R with ggplot2 7.1 A dataset 7.2 Components of ggplot objects {components} 7.3 Building a plot 7.4 Customize glidline and axis labels 7.5 Modify the position of the legend 7.6 Basic plots 7.7 Linegraph 7.8 Advanced plots 7.9 Colour 7.10 size 7.11 scaling 7.12 Guides 7.13 Add-on packages 7.14 ggridges 7.15 Varying fill colors along the x axis 7.16 metR", " Chapter 7 Plotting in R with ggplot2 ggplot2 is a package well be using a lot for graphing our education datasets. ggplot2 is designed to build graphs layer by layer, where each layer is a building block for your graph. Making graphs in layers is useful because we can think of building up our graphs in separate parts: the data comes first, then the x-axis and y-axis, and finally other components like text labels and graph shapes. When something goes wrong and your ggplot2 code returns an error, you can learn about whats happening by removing one layer at a time and running it again until the code works properly. Once you know which line is causing the problem, you can focus on fixing it. The ability to create visualizationsgraphical representations of data is an important step to convey your resultsinformation and findings to others. In this chapter, we illustrate how you can use visualize your data and create elegant graphics. One of things that makes R such a great tools is its data visualization capabilities. R has many systems for visualization and creating plots, some of which arebase R graphics, lattice and ggplot2, but we focus on the use of ggplot2. The ggplot2 package is a phenomenal tool for creating graphics in R. It provide a unifying frameworka grammar of graphics for describing and building graphs. Just as the grammar of language helps you construct meaningful sentences out of words, the Grammar of Graphics helps you construct graphics out of different visual elements. This grammar provides a way to talk about parts of a visual plot: all the circles, lines, arrows, and text that are combined into a diagram for visualizing data. Originally developed by Leland Wilkinson, the Grammar of Graphics was adapted by Hadley Wickham (2016) to describe the components of a plot The ggplot2 package provides a powerful and flexible approach to data visualization, and its is suitable both for rapdi exploration of different visualization approaches and for producing carefully crafted publicationquality figures. However, getting ggplot2 to make figures that look exactly the way you want them to, can sometimes be challenging and beginners and expert alike can get consufed by themes, scales, coords, guides or facets. ggplot2 further organizes these components into layers, where each layer displays a single type of (highly configurable) geometric object. Even the most experienced R users need to refer to ggplot2 Cheat Sheet while creating elegant graphics, we will demonstrate stepbystep how to get the most out of ggplot2 package, including how to choose and customize scales, how to theme plots, and when to use add-in packages that extend the ggplot2 capabilities. Some of the extension of ggplot2 that we will introuduce to your include ggspatial (Dunnington 2020), metR (Campitelli 2019), ggrepel (Slowikowski 2020), cowplot (Wilke 2018), patchwork (Pedersen 2020), etc. Rather than loading these extension packages with require() function, well call their functions using the :: notation. This will help make it clear which funcions are built into ggplot2, and which comes from extensions. require(tidyverse) require(magrittr) load addition functions source(&quot;e:/Data Manipulation/semba_functions.R&quot;) 7.1 A dataset Were using a CTD profile data along the coastal water of Tanzania collected in August 2004 with Algoa. I have processed and cleaned the profile data from a CTD instrument, and created a very basic and simple dataset for us to start with. I have tidy the data into a data frame and it contains 10 variables collected at 22 stations (Table 7.1). These variables include the time, longitude and latitude coordinates and the name at each station. It also contains the measured profile of temperature, salinity, oxygen, and fluorescence, spar, par and density as function of pressure. Table 7.1: A sample of CTD profile dataset. For visibility of all the 22 stations, only variables measured at 5 meter standard depth are shown Cast Time Cast Location Measured Variabes Station Date Hour Lon Lat Pressure Temperature Salinity Oxygen Fluorescence st1 2004-08-18 15 40.61 -10.54 5 25.17 33.92 3.93 0.56 st2 2004-08-18 17 40.77 -10.54 5 25.16 34.85 4.47 0.62 st3 2004-08-18 20 40.95 -10.54 5 NA NA NA NA st4 2004-08-18 22 41.12 -10.54 5 NA NA NA NA st5 2004-08-19 0 41.28 -10.54 5 NA NA NA NA st6 2004-08-19 11 40.34 -8.83 5 25.21 34.86 4.48 0.24 st7 2004-08-19 13 40.18 -8.83 5 25.25 34.87 4.52 0.44 st8 2004-08-19 15 40.00 -8.83 5 25.02 34.86 4.59 1.14 st9 2004-08-19 17 39.82 -8.83 5 25.11 34.86 4.64 1.53 st10 2004-08-19 19 39.67 -8.83 5 NA NA NA NA st11 2004-08-22 16 39.60 -9.03 5 25.44 34.91 4.98 1.71 st12 2004-08-23 3 40.26 -7.04 5 25.12 34.87 4.50 0.99 st13 2004-08-23 6 40.10 -7.05 5 25.18 34.87 4.49 0.45 st14 2004-08-23 7 39.93 -7.05 5 25.28 34.88 4.60 0.84 st15 2004-08-23 9 39.76 -7.04 5 25.26 34.89 4.66 1.06 st16 2004-08-23 11 39.59 -7.04 5 25.92 34.88 4.31 0.62 st17 2004-08-23 19 40.07 -5.49 5 25.64 35.19 4.40 0.82 st18 2004-08-23 22 39.90 -5.49 5 25.28 34.90 4.52 0.85 st19 2004-08-24 1 39.56 -5.49 5 25.27 34.90 4.53 0.98 st20 2004-08-24 3 39.40 -5.47 5 25.23 34.89 4.81 1.11 st21 2004-08-24 4 39.24 -5.48 5 25.82 34.93 4.34 0.21 st22 2004-08-24 5 39.10 -5.48 5 26.05 34.95 4.32 0.42 ## make a variable with labelled latitude ctd = ctd %&gt;% filter(lat &gt; -6) %&gt;% mutate(transect = &quot;transect 1&quot;, Lat.label = median(lat)) %&gt;% bind_rows(ctd %&gt;% filter(lat &gt; -8 &amp; lat &lt; -6) %&gt;% mutate(transect = &quot;transect 2&quot;, Lat.label = median(lat)), ctd %&gt;% filter(lat &gt; -10 &amp; lat &lt; -8) %&gt;% mutate(transect = &quot;transect 3&quot;, Lat.label = median(lat)), ctd %&gt;% filter(lat &lt; -10) %&gt;% mutate(transect = &quot;transect 4&quot;, Lat.label = median(lat))) %&gt;% mutate(Lat.label = metR::LatLabel(Lat.label%&gt;% round(2)) %&gt;%as.factor()) 7.2 Components of ggplot objects {components} I have created a simple plot of this data that show scatterplot of temperature versus fluorescence at the four different transects (Fig @fig:fig1299). The plot show the concentration of fluorescence against temperature for the twenty one stations along the coastal water of Tanzania. Figure 7.1: Association of fluorescence concentration per temperature collected in August 2004 Lets explore in details the key elements used to make figure 7.1: data: The data like the one illustrated in table 7.1. It must be a data frame for ggplot2 read and understand. aesthetics: is used to map the x and y axis for 2dimensional plot and add the z value for 3dimensionla plots. It is also used to define visual properties like color, size, shapes or height etc, and. For instance in the figure 7.1, the position along the y-axis is mapped to the concentration of fluorescence and the x - axis is mapped to temperature values. For the points, the color is mapped to the geogrphical location along the transects. Other aestheticslike size, shape, and transparency have been left at their default settings. geometry; a layer which define the type of plot you want to make, whether is histogram, boxplot, barplot, scatterplot, lineplot etc. coordinate system: used to set a limit for the plot. The cartesian coordinate is the most familiar and common system that is widely used to zoom the plot and does not change the underlying data. scales: scales allows to customize the plot. For instance in figure 7.1 both x and y-axis used continuous data and hence the scale_x_continuous() and scale_y_continuous() were used to modiy the values of the axis. For color, I simply stick on scale_colour_discrete() and customize the legend name. labels: The plot is well labelled and easy to understand. It has title, subtitle, axes and caption for the courtesy of the data. theme: the plot stick on the default theme_gray theme, which has a gray background color and white gridlines, a sans serif font family, and a base font size of 11. We can customize all the propoerties in the theme to suit our standard. 7.3 Building a plot Since you now have a clue of the different layers added to create a plot, its time to work around to create a plot with the ggplot2 package. We use the same profile dataset that used to make figure 7.1. First, you neeed to import the data into your R session. 7.3.1 Plotting layers To create a data visualization using ggplot2 package, we will add layers for each of the plot elements described in section ??. I will take you through step by step of the key ines needed to make such a plot. First make sure the ggplot2 or tidyverse packages are loaded in your Rs session. You can load the package with this code; require(tidyverse) The ggplot2 create a ggplot object, and you initialize the object with the ggplot() function ggplot() The plot above is black with grey background. This is because we have not specified the data and aesthetic arguments inside the ggplot() function. Lets specify the data, which in our case is the ctd dataset and also specify the x-axis with temperature and y-axis with fluorescence. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence)) Now the plot has gridlines and axis with values and labelsx-axis show the value of temperature and the y-axis show the value of fluorescence concentrations. However, there is no graphics. This is because we have not added any geom yet. Therefore, since we have already specified the data and the aesthetic values, now we can add the geom where we map the aesthetics to columns in the dataset. Lets add the geom_point() and specify the size to 3 ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence)) + geom_point(size = 3) 7.3.2 Customize legend The plot now show points distributed in the panel plot. But our interest is to color the point at each transect. Next, we add an argument col in the aesthetic that map points that fall along a certain transect with similar color. Since we have a variable called transect in the dataset, we specify this as the code below ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) The plot above show the points are colorcoded to refrect the transect in the legend. Note that the colors scheme used in this plot is the default one. Sometimes you will want to customize the aesthetic for all the points in the plot. To change a color scale, you can use the scale_color_viridis_d(). The _d here represent the discrete variable. If you were using the continuous data, you would use scale_color_viridis_c() scheme instead. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d() To change the title of the legend you must specify the name argument in your scale_* function. For instance, we specified scale_colour_viridis_d(name = \"Transects\") to change the legend title for this plot ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects\\n of CTD Casts&quot;) If you want to remove the legend title, we can add a theme layer and specify theme(legend.title = element_blank()) ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;) + theme(legend.title = element_blank()) We can also change the legend title by specifying theme(legend.title = element_blank()) ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects\\nof CTD Casts&quot;) + theme(legend.title = element_text(size = 12, colour = &quot;chocolate&quot;, face = &quot;bold&quot;)) 7.3.3 Working with titles and labels Often times you need to customize the axis labels. By default, the scale label for each scale is the name of the variable in the dataset. We can change the labels with labs() function. The other elements we would like to add are the plot title and subtitle. If you want to label SI unit in the ggplot, use the expression() function. Note that I used the expression() function to express mathematical symbols in the x and yaxis of the plot. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;) 7.4 Customize glidline and axis labels The other layer that we would like to add to customize our plot are the scale_x_continous(), which change the gridline of the x-axis and the scale_y_continuous(), which change the gridlines of the y-axis. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2)) 7.4.1 Remove the gray box of points on legend The default ggplot always put a gray background of the scatterplot. You can remove it by adding a theme layer and specify the argument legend.key = element_blank() to get rid of them ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank()) 7.5 Modify the position of the legend By default, ggplot2 place the legend on the right position. You can decided to place either left, right, top or bottom. For example, if we want to place the legend at the bottom, we simply specifying the argument legend.position = \"bottom\" in the theme layer. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), legend.position = &quot;bottom&quot;) We can also position the legend insite the plot and specify the x and y coordinates. The coordinates range from 0 to 1, from left to right or bottom to top of the plot. For instance, we can place the legend at the top right corner of the plot by specifying legend.position = c(.9,.75) in the theme layer. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), legend.position = c(.9,.75)) 7.5.1 Change the legend background and stroke color By default, ggplot legend has a white fill for background and without color for stroke. We can customize the look of the legend in the theme layer. For example, we want our legend to have an ivory fill and black stroke of 0.25 size. This can be achieved by adding and argumentlegend.background = element_rect(fill = \"ivory\", colour = \"black\", size = .25) in the theme layer. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), legend.position = c(.9,.75), legend.background = element_rect(fill = &quot;ivory&quot;, colour = &quot;black&quot;, size = .25)) 7.5.2 Modify background colors You can remove the background fill and stroke color with the panel.background = element_blank() ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), legend.position = c(.9,.75), legend.background = element_rect(fill = &quot;ivory&quot;, colour = &quot;black&quot;, size = .25), panel.background = element_blank()) You can customize the the background fill and stroke colors to your what fit you best with the panel.background = element_rect() and specify the stroke with color and background with the fill arguments. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), legend.position = c(.9,.75), legend.background = element_rect(fill = &quot;ivory&quot;, colour = &quot;black&quot;, size = .25), panel.background = element_rect(fill = &quot;lightblue&quot;, colour = &quot;black&quot;)) 7.5.3 change the gridlines You can also customize the color and shape of the gridlines. You can remove the gridline with panel.grid = element_blank() argument in the theme layer. But if you want to customize the gridlines, then you use the panel.grid = element_line() and specify the color and linetype etc. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), legend.position = c(.9,.75), legend.background = element_rect(fill = &quot;ivory&quot;, colour = &quot;black&quot;, size = .25), panel.background = element_blank(), panel.grid = element_line(colour = &quot;grey60&quot;, linetype = &quot;dotted&quot;, size = .25)) 7.5.4 Change the font size for axis, labels and titles The fonts for axis ticks, axis-title text have a default font size of 11, if we want to increase the font size to 12 then then we can do that inside the theme(). We can also change the font size for titles, subtitles and legend text and title as the chunk below highlight. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), axis.text = element_text(size = 11), axis.title = element_text(size = 13), plot.subtitle = element_text(size = 12), plot.title = element_text( size = 20), legend.text = element_text(size = 11), legend.title = element_text(size = 12), plot.caption = element_text(size = 11)) 7.5.5 Limit axis to a range We use the coord_cartesian() function to adjust the visible area of the plot. You should specify the range of the ylim=c() and xlim = c() and expand = FALSE to show only the area of the plot you want to visualize. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), axis.text = element_text(size = 11), axis.title = element_text(size = 13), plot.subtitle = element_text(size = 12), plot.title = element_text( size = 20), legend.text = element_text(size = 11), legend.title = element_text(size = 12), plot.caption = element_text(size = 11)) + coord_cartesian(xlim = c(24.9,26.05), ylim = c(0.1,2), expand = FALSE) 7.5.6 Adding labels ggplot2 provide geom_tex() and geom_label function for adding label on the plot. geom_text() adds text directly to the plot. geom_label() draws a rectangle behind the text, making it easier to read. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + geom_text(aes(label = station), nudge_x = 0.05, nudge_y = .04)+ scale_colour_viridis_d(name = &quot;Transects&quot;) I find it difficult to position label with geom_text(), especially when you have a lot of points to label. I often use the ggrepel package, which provides text and label geoms for ggplot2 that help to avoid overlapping text labels. These function makes labels repel away from each other and away from the data points. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + ggrepel::geom_text_repel(aes(label = station))+ scale_colour_viridis_d(name = &quot;Transects&quot;) 7.5.7 Add text annotation There are times you may need to add a text on top of the plot. In ggplot2 you can place a label using the real values on the plot instead of the hard-core coorinates to specify the location based on scaled coordinates where 0 is low and 1 is high. This is useful for adding small annotations (such as text labels) or if you have your data in vectors, and for some reason dont want to put them in a data frame. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), axis.text = element_text(size = 11), axis.title = element_text(size = 13), plot.subtitle = element_text(size = 12), plot.title = element_text( size = 20), legend.text = element_text(size = 11), legend.title = element_text(size = 12), plot.caption = element_text(size = 11)) + coord_cartesian(xlim = c(24.9,26.05), ylim = c(0.1,2), expand = FALSE) + annotate(geom = &quot;text&quot;, x = 25.95 , y = 1.82, label = &quot;My label&quot;) 7.5.8 Make the x and y axis the same The coord_cartesian() is the coordinate system that you will use most of the time because most of the plot are created from two or more variables that have different scales. However, there are situations where you create plots that use the same scale of x-and y-coordinates. For example, ploting temperature profiles from two stations, then an appropriate coordinae system in that case is the coord_equal() ## tidy the temperature from long to wide format ctd.temprature.wide = ctd %&gt;% select(station, temperature, pressure) %&gt;% spread(key = &quot;station&quot;, value = &quot;temperature&quot;) ## make aplot ggplot(data = ctd.temprature.wide, aes(x = st3, y = st8)) + geom_point() + coord_equal() 7.5.9 Faceting Creating multipanel plots Facets are ways of arranging plots into multiple different pieces (subplots). This allows you to view a separate plot for each unique value in a categorical variable. The ggplot2 package has a nice function for creating multi-panel plots. The facet_wrap creates essentially a ribbon of plots based on a single variable. For example the plot below, I first filtered only profile below 201 meters and classify them into bins of below 50 meters, 50 to 100 and above 100 meters. This computed depth class was used to make multiple plot of boxplot of fluorescence at the four transects. in the facet_wrap() layer, I specified the ~depth.class to use this variable for faceting and nrow = 1, for the canvas to have one row with multiple columns depending on the groups, four our case we get three columns from the depth classes. ## filter the pressure and break them into class of 50,100,200 ctd.class.depth = ctd %&gt;% filter(pressure &lt; 201) %&gt;% mutate(depth.class = cut(pressure, breaks = c(0,50,100,200), labels = c(&quot;Below 50&quot;, &quot;50-100&quot;, &quot;Above 100&quot;))) ggplot(data = ctd.class.depth, aes(x = transect,y = fluorescence, fill = transect)) + geom_boxplot() + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) + scale_fill_discrete(limits = c(&quot;transect 1&quot;, &quot;transect 2&quot;, &quot;transect 3&quot;, &quot;transect 4&quot;), labels = c(&quot;Pemba&quot;, &quot;Kimbiji&quot;, &quot;Lindi&quot;, &quot;Mtwara&quot;)) + labs(x = NULL, y = expression(Fluorescence~(mgm^{-3})))+ facet_wrap(~depth.class, nrow = 1) 7.5.10 Allow scales to roam free (scales) The default for multi-panel plots in ggplot2 is to use equivalent scales in each panel. But sometimes you want to allow a panels own data to determine the scale. This may mislead to the audience since it give creaes plots with different scales. You can specify the scales=\"free\" in the facet_wrap() layer written as; ## filter the pressure and break them into class of 50,100,200 ctd.class.depth = ctd %&gt;% filter(pressure &lt; 201) %&gt;% mutate(depth.class = cut(pressure, breaks = c(0,50,100,200), labels = c(&quot;Below 50&quot;, &quot;50-100&quot;, &quot;Above 100&quot;))) ggplot(data = ctd.class.depth, aes(x = transect,y = fluorescence, fill = transect)) + geom_boxplot() + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) + scale_fill_discrete(limits = c(&quot;transect 1&quot;, &quot;transect 2&quot;, &quot;transect 3&quot;, &quot;transect 4&quot;), labels = c(&quot;Pemba&quot;, &quot;Kimbiji&quot;, &quot;Lindi&quot;, &quot;Mtwara&quot;)) + labs(x = NULL, y = expression(Fluorescence~(mgm^{-3})))+ facet_wrap(~depth.class, nrow = 1, scales = &quot;free_y&quot;) 7.6 Basic plots The ggplot2 package provides a set of functions that mirror the Grammar of Graphics, enabling you to efficaciously specify what you want a plot to look like. To have a glimpse of ggplot2, we start with five basic types of plots that are familiar to most people. These include: scatterplot linegraphs boxplots histograms barplots Note that the four graphs works with quantitative data and barplots are appropriate for categorical data. Thus, understanding the type of data you want to plot is a fundamental principle before you throw the variable into ggplot2 to make plot for you. 7.6.1 scatterplot Scatterplots are also called bivariate, allows you to visualize the association between two numerical variables. They are among the widely used plot because they can provide an immediate way to see the pattern of association between two numerical variables. Figure 7.2: scatterplot from base R Most of us are familiar with scatterplot shown in figure and made several of them using base R, but probably you have not made one based on the fundamental theorem of grammar of graphics. We will visualize the relationship between temperature and fluorescence. Because the ctd is profile data frame with variable as function of pressure, we want to check for the association of all the 22 station but at fixed depth of 10 meters. What this means is that we will take the ctd data and filter all variable at all station measured at water depth of 10 meters from the surface and save this in a new data frame called ctd10d. This can be written as: ctd10d = ctd %&gt;% filter(pressure == 10) Lets now dive into the code of using the *grammar of graphics to create the scatterplot. We use the ggplot() function from ggplot2** package. The code to create figure ?? is written as; ggplot(data = ctd10d, aes(x = temperature, y = fluorescence)) + geom_point() Lets explore the block above piece-by-piece The plotting in ggplot2 begin with ggplot() function, where the two components of grammar of graphics are required. in the data component we specify the data frame of variables measured at 10 meter water below the surface by setting data = ctd10d. Then the second argument aesthetic that map the plot with coordinate was set by aes(x = temperature, y = fluorescence)). In a nutshell, the aes() define the variableaxis specifications. For the code above we set the variables temperature into the x coordinate and the variable fluorescence into the y-axis. We then add a layer to the ggplot() function calll using the + sign. The added layer specify the third part of the *grammarthe geometric component. Becasue we want to plot scatterplot, the appropriate geom for this case is the geom_point(). Figure 7.3: Scatterplot showing the association between temperature and fluorescence at 10 meter water from the surface Once the code is run, it produce two outputs: a warning message and figure ??. The warning message notify us that out of the 22 stations, there are three stations with missing data at 10 meter water. We can check station with missing values with code written below; ctd10d %&gt;% filter(is.na(temperature)) %&gt;% select(station, pressure, temperature, fluorescence) # A tibble: 3 x 4 station pressure temperature fluorescence &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st10 10 NA NA 2 st4 10 NA NA 3 st5 10 NA NA 7.6.2 adding regression line you can simply add the regression line by addign a geom_smooth() layer and specify the method = \"lm\" and we dont need to show the confidence errors, hence we specify se = FALSE ggplot(data = ctd10d, aes(x = temperature, y = fluorescence)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) While the geom_point() and geom_smooth() layers in this code both use the same aesthetic mappings, theres no reason you couldnt assign different aesthetic mappings to each geometry. Note that if the layers do share some aesthetic mappings, you can specify those as an argument to the ggplot() function Looking on the generated scatterplot (Figure ??), we notice a negative relation exist between temperature and fluorescencestations with high temperature have relatively low concentration of fluorescence and viceversa. We also notice that 16 stations are clustered within \\(25-25.25^{\\circ}\\) C and only three are found at temperature range between \\(25.75-26^{\\circ}\\) C. We can identify these pattern by adding a col argument in aes by setting aes(x = temperature, y = fluorescence, col = Lat.label)). The code for creating figure ?? is written as; ggplot(data = ctd10d, aes(x = temperature, y = fluorescence, col = Lat.label)) + geom_point() Looking on the color of thelegend, it clearly indicate the two stations with temperatur above \\(25.75^{\\circ}\\) C are found along the transect lies along latitude 5.49°S in Pemba Channel and and one at latitde7.04°S off the Kimbiji. We may also be interested to knwo whether dissolved oxygen has influrence on the the association of temperature and fluoresce. We can achieve that by simply parsing the the shape = oxygen, arguments into the aesthetic component. Figure ?? clearly shows the stations with relatively high chlorophyll value have bigger shape (high DO value) and found in relatively less warmer waters in contrast to stations with low chl-value have smaller shape size, indicating low DO and found in relatively warmer water. The chunk for making figure ?? is written as: ggplot(data = ctd10d, aes(x = temperature, y = fluorescence, size = oxygen)) + geom_point() Figure 7.4: Scatterplot showing the association between temperature and fluorescence at 10 meter water from the surface and the influence of dissolved oxygen 7.7 Linegraph # I donwloaded the data using the code below and save it into the project directory. This chunk will never run but rather the proceeding step will use the data that was downloade and processed with code in this chunk. result = xtractomatic::searchData(&quot;datasetname:mday&quot;) ## sst, Aqua MODIS, NPP, L3SMI, Global,0.041, Science Quality (Monthly Composite) xtractomatic::getInfo(dtype = &quot;mhsstdmday&quot;) ## Chlorophyll-a, Aqua MODIS, NPP, L3SMI, Global,0.041, Science Quality (Monthly Composite) xtractomatic::getInfo(dtype = &quot;mhchlamday&quot;) sst = xtractomatic::xtracto_3D(dtype = &quot;mhsstdmday&quot;, xpos = c(38.79272,40.24292), ypos = c(-5.692232,-5.451759), tpos = c(&quot;2014-11-02&quot;, &quot;2019-04-16&quot;)) chl = xtractomatic::xtracto_3D(dtype = &quot;mhchlamday&quot;, xpos = c(38.79272, 40.24292), ypos = c(-5.692232, -5.451759), tpos = c(&quot;2003-01-17&quot;, &quot;2019-04-16&quot;)) The next basic graph of ggplot2 is the linegraph. Linegraphs is similar to drawing points, except that it conncets the points with line. often times you dont show the points. Linegraphs are commonly used to show time series data. Its inappropriate to use the linegraphs for data that has no clear sequential ordering . Lets illustrate how to create linegraphs using another dataset ofchlorophyll concentration in the Pemba channel. This is a monthly average data from MODIS satellite. I processed the data and stored them in .RData format. We can load this dataset from the working directory with the load() function as shown in the chunk below; load(&quot;data/modis_pemba.RData&quot;) Exploring further the dataset, we notice that its an array with 35 longitude spacing and 7 latitude spacing and 196 matrix collected every month from 2003-01-16 to 2019-04-16. Being an array, this dataset is not in the right format, since ggplot2 only accept the data frame format. chl$data %&gt;% class();chl$data %&gt;% dim() [1] &quot;array&quot; [1] 35 7 196 Therefore, we need to tidy this dataset from the array into the data frame and find average chlorophyll concentration for each month for 196 months. Because there are 196 matrix, that is a lot of work to do it one after the other. The easiest solution is to loop the process with a for loop. If you are not familiar with how loop works in R, I suggest you check section__ of chapter__, which describe in details. The chunk below highlight the for loop used to convert an array to matrix ## preallocate object sst.tb = list() chl.tb = list() ## loop sst for (i in 1:length(sst$time)){ sst.tb[[i]] = matrix_tb(x = sst$longitude, y = sst$latitude, data = sst$data[,,i]) %&gt;% mutate(time = sst$time[i] %&gt;%as.Date()) %&gt;% select(date = time,lon = x, lat = y, sst = value) } ## loop chl for (j in 1:length(chl$time)){ chl.tb[[j]] = matrix_tb(x = chl$longitude, y = chl$latitude, data = chl$data[,,j]) %&gt;% mutate(time = chl$time[j] %&gt;%as.Date()) %&gt;% select(date = time,lon = x, lat = y, chl = value) } ## unlist the listed sst and chl files sst.tb = sst.tb %&gt;% bind_rows(sst.tb) chl.tb = chl.tb %&gt;% bind_rows(chl.tb) We then used the function in lubridate::year() and lubridate::month() to create the year and month variables from the date variables chl.decompose = chl.tb %&gt;% mutate(month = lubridate::month(date, label = TRUE, abb = TRUE), year = lubridate::year(date) %&gt;% as.integer()) To make a plot of annual average of chlorophyll concentration in the Pemba Channel, we need to monthly mean. We exclude data for year 2003 and 2019 because there only seven months in 2003 and four months in 2019, which can affect the analysis. computing annual average can be written as; chl.yearly = chl.decompose %&gt;% filter( year &gt;= 2003 &amp; year &lt;= 2018)%&gt;% group_by(year) %&gt;% summarise(chl = mean(chl, na.rm = TRUE)) Lets create linegraph of annual average chlorophyll-a concentration in the Pemba Channel. Like the scatterplot we made earlier, where supply the data frame in data argument and specified the aesthetic mapping with x and y cooridnates, but instead of using geom_point(), we use the geom_line(). The code to make the line graphy of annual average chlorophyll-a conceration shown in figure ?? are written as; ggplot(data = chl.yearly, aes(x = year, y = chl))+ geom_line() Since the selected area is affected with monsoon season, we can further decode the months into their respective southeast (SE) and northeast (NE) and inter-monsoon (IN) seasons using the decode() function from dplyr package as the chunk below show; chl.season = chl.decompose %&gt;% mutate(season = month %&gt;% as.character(), season = recode(.x = season, Jan = &quot;NE&quot;), season = recode(.x = season, Feb = &quot;NE&quot;), season = recode(.x = season, Mar = &quot;NE&quot;), season = recode(.x = season, Apr = &quot;IN&quot;), season = recode(.x = season, May = &quot;SE&quot;), season = recode(.x = season, Jun = &quot;SE&quot;), season = recode(.x = season, Jul = &quot;SE&quot;), season = recode(.x = season, Aug = &quot;SE&quot;), season = recode(.x = season, Sep = &quot;SE&quot;), season = recode(.x = season, Oct = &quot;IN&quot;), season = recode(.x = season, Nov = &quot;NE&quot;), season = recode(.x = season, Dec = &quot;NE&quot;)) We then compute the year average of chlorophyll-a at each season as written in the chunk below chl.season = chl.season %&gt;% filter( year &gt;= 2003 &amp; year &lt; 2019)%&gt;% group_by(year,season) %&gt;% summarise(chl = mean(chl, na.rm = TRUE)) To make a linegraphy that show annual average of chlorophyll-a concetration for each season as in figure ??, we simply parse the argument col == season in the aes() component as the chunk below hightlight ggplot(data = chl.season, aes(x = year, y = chl, col = season))+ geom_line()+ scale_x_continuous(breaks = seq(2002,2019,2)) 7.7.1 histogram A histogram is a plot that can be used to examine the shape and spread of continuous data. It looks very similar to a bar graph and organized in intervals or classes. . It divides the range of the data into bin equal intervals (also called bins or classes), count the number of observations n in each bin, and display the frequency distribution of observations as a bar plot. Such histogram plots provide valuable information on the characteristics of the data, such as the central tendency, the dispersion and the general shape of the distribution. Lets consider the chl variable in the chl.tb data frame. Since histogram works for single variable that contains quantitative values, you can not bother looking for relationship as we have seen in previous plots, but histogram offers an opportunity to answer question like What are the smallest and largest values of chl-a? What is the center value? 3 How does these values spread out? We can make a histogram shown in figure ?? by simply setting aes(x = chl) and add geom_histogram(). Within the geom_histogram(), we simply specify the number of bins bins = 28 and also the color seprating each column of the histogram with col == \"white\". The code to create figure ??, which displays a histogram with twenty eight classes, is written as; ggplot(data = chl.decompose %&gt;% filter(chl &lt; .5), aes(x = chl))+ geom_histogram(bins = 28, col = &quot;white&quot;) Figure 7.5: Histogram showing the distribution of chlorophyll concentration 7.7.2 boxplot The box plot is a standardized way of displaying the distribution of data based on the five number summary: minimum, first quartile, median, third quartile, and maximum. Box plots are useful for detecting outliers and for comparing distributions. These five number summary also called the 25th percentile, median, and 75th percentile of the quantitative data. The whisker (vertical lines) capture reoungly 99% of a distribution, and observaion outside this range are plottted as points representing outliers as shown in figure 7.6. knitr::include_graphics(&quot;./images/boxplot-01.png&quot;) Figure 7.6: Boxplot five numbers Box plots can be created by using the geom_boxplot() function and specify the data variable. ggplot(data = chl.decompose %&gt;% filter( chl &lt; .25), aes(y = chl))+ geom_boxplot() The firstinput must be a categorical variable and the second must be a continuous variable. ggplot(data = chl.season %&gt;% filter( chl &lt; .25), aes(x = season, y = chl))+ geom_boxplot() the geom_boxplot() has outlier_ arguments that allows to highlight and modify the color, shape, size, alpha  etc of outliers extreme observation. For instance, you can highlight the outlier with; ggplot(data = chl.season %&gt;% filter( chl &lt; .25), aes(x = season, y = chl))+ geom_boxplot( outlier.colour = &quot;red&quot;, outlier.shape = 8, outlier.size = 4) We can also map the fill and color to variable in to distinguish boxplot. for example, we can specify the fill = season argument in the aes() to fill the boxplot based on season. ggplot(data = sst.season , aes(x = month, y = sst, fill = season))+ geom_boxplot( outlier.colour = &quot;black&quot;, outlier.shape = 8, outlier.size =1.2)+ scale_fill_manual(values = c(4,2,3)) We can add the points on top of the boxplot with the geom_jitter(). It also allows for specifying other arguments like colors and width of the points. ggplot(data = chl.season %&gt;% filter( chl &lt; .25), aes(x = season, y = chl))+ geom_boxplot()+ geom_jitter(width = .05, height = .05, col = &quot;blue&quot;) 7.7.3 barplot Bar graphs are perhaps the widely used plot. They are typically used to display numeric values on the y-axis for different groups on the x-axis. There is an important distiction you should be aware of when making bar graphs. The heigh of a bar in barplot may represent either the counts or values of elements in the dataset. Lets begin with the formercount. We use the drifter observations, which passed at the confluencewhere the South Equatorial Current splits to form the northward flowing called the East Africa Coastal Current and the southward flowing the Mozambique Current. We first load the daset as the code highlight; drifter_confluence = read_csv(&quot;data/drifter_confluence.csv&quot;) 7.7.3.1 barplot for count To make the bar graph that show the number of drifters crossed the ares per month over the entire period you you simply specify the the variable month in the x coordinates in the aesthetic and add the geom_bar() ggplot(data = drifter_confluence, aes(x = month)) + geom_bar() Sometimes you may wish to stack bars. For instance drifter have droguedan anchor, which reduce speed of the drifter wind effect. When the drogue is lost, drifter slip and tend to overestimate the current speed. In our dataset, the drogue are coded with 0 == Absent, and 1 = Present. Because zero and ones make no sense, we create a new variables of that replace zero with LOST and ones with PRESENT. we use the if_else() function from dplyr to convert these values and assign the object as drifter.drogue. The conversion can be attained with code written as; drifter.drouge = drifter_confluence %&gt;% mutate(drogue.status = if_else(condition = drogue==1, true = &quot;PRESENT&quot;, false = &quot;LOST&quot;)) Then to stack the bar based on the drogue status, we simpy add the fill = drogue.status in aes() part ggplot(data = drifter.drouge, aes(x = month, fill = drogue.status))+ geom_bar() You can flip the order of bar with position = position_stack(reverse = TRUE) ggplot(data = drifter.drouge, aes(x = month, fill = drogue.status))+ geom_bar(position = position_stack(reverse = TRUE)) Instead of stacking, you can dodge the bar with position = position_dodge() argument ggplot(data = drifter.drouge, aes(x = month, fill = drogue.status))+ geom_bar(position = position_dodge()) To add a black stroke color of the bar, add the argument col = \"black\" inside the geom_bar() ggplot(data = drifter.drouge, aes(x = month, fill = drogue.status))+ geom_bar(position = position_dodge(), col = &quot;black&quot;) And to specify the width of the bar you specify a value in width argument in geom_bar() ggplot(data = drifter.drouge, aes(x = month, fill = drogue.status))+ geom_bar(position = position_dodge(), col = &quot;black&quot;, width = .75) Sometimes you want to map bar with different colors for negative and positive values. For this case we will use the sea surface temperature from drifter observation and create a new variable called anomalyindicate the temperature value above (positive value) and below (negative) the climatological mean. The code for computing anomaly is; sst.anomaly = drifter_confluence %&gt;% group_by(year) %&gt;% summarise(average = mean(sst, na.rm = TRUE)) %&gt;% mutate(anomaly = average - mean(average), anomaly.status = if_else(anomaly &gt; 0 , &quot;Above&quot;, &quot;Below&quot;)) %&gt;% filter(year &gt; 1996) once we have computed the anomaly of sea surface temperature for each year, we can plot the withe geom_col() and specify x = year, and y = anomaly and fill = anomaly.status. We also need to specify the position == \"identity\" to prevent notification message of poor defined stacking for bar with negative values. ggplot(data =sst.anomaly, aes(x = year, y = anomaly, fill = anomaly.status)) + geom_col(position = &quot;identity&quot;, width = .8) Although the plot looks good but sometimes we may wish to reverse the filled color.We can reorder the color with the scale layer. Because the variable used to fill the bar is categorical, the appropriate scale is scale_fill_discrete() and you specify the limits with limits = c(\"Below\", \"Above\"). The legend is this plot is not important and we can remove from th plot with guides(fill = FALSE) ggplot(data =sst.anomaly, aes(x = year, y = anomaly, fill = anomaly.status)) + geom_col(position = &quot;identity&quot;, width = .8)+ scale_fill_discrete(limits = c(&quot;Below&quot;, &quot;Above&quot;))+ guides(fill = FALSE) The red 7.7.4 barplot for values We have seen how to make barplot that show the count with geom_bar(). You can also use the barplot to show the values with the geom_col() function and specify what variables you want on the x and y axis. For instance, we want to show how surface current varies over twelve months. Because the geom_col() requires summarized statistics, we need to compute the average current speed for each month. The chunk below highlight how to compute the mean, maximum, minimum and standard deviation of surface current velocity for each month. current.speed.monthly = drifter.drouge %&gt;% filter() %&gt;% mutate(speed = sqrt(u^2 + v^2)) %&gt;% group_by(month, drogue.status) %&gt;% summarise(speed_min = min(speed, na.rm = TRUE), speed_mean = mean(speed, na.rm = TRUE), speed_max = max(speed, na.rm = TRUE), speed_sd = sd(speed, na.rm = TRUE)) %&gt;% ungroup() Once we have computed the statistics, we can use them to make barplot. Note that unlike the geom_bar() that need only the x variable, geom_col() requires x and y variables specified. For illustration, we specified the x = month, and y = speed_mean in the aes() to make a barplot that show the monthly monthly mean surface current. ggplot(data = current.speed.monthly, aes(x = month,y = speed_mean))+ geom_col() Although ggplot2 allows to stack the barplot that present values, although they seems appealing, you must avoid making such kind of plot, because they tend to mislead and difficult to distinguish betwen the groups. ggplot(data = current.speed.monthly, aes(x = month,y = speed_max, fill = drogue.status))+ geom_col() The appropriate way if you want to compare two or more groups with geom_col(), you better doge the bar instead of stacking them. This makes easier to compare. For instance, in the case here, its cleary to see months of which drifter with lost drogues move relatively faster than those with droguue. ggplot(data = current.speed.monthly, aes(x = month,y = speed_max, fill = drogue.status))+ geom_col(position = position_dodge()) Note: the basic barplot that present count or frequency has one categorical variable on the x axis and you can make with geom_bar()And the barplot that present the values for instance the average of the variable has one categorical variable on the x axis and continuos variable on the y axis and you create them with geom_col() function. You can make a grouped bar plot by mapping that variable to fill, which represent the fill color of the bars. Like the variables mapped to the x axis, variables that are specified to the fill color of bars must be categorical instead of continuous. drifter.interest = drifter_confluence %&gt;% group_by(id) %&gt;% summarise(count = n(), begin = dplyr::first(date), end = dplyr::last(date), period = lubridate::interval(start = begin, end = end, tzone = &quot;&quot;) %&gt;% lubridate::as.duration() %&gt;% as.numeric(&quot;hours&quot;)) %&gt;% filter(period &gt;1000) drifter.interest.point = drifter.interest %&gt;% left_join(drifter_confluence, by = &quot;id&quot;) %&gt;% mutate(id.drifter = paste(&quot;ids&quot;, id, sep = &quot;&quot;)) ggplot(data = drifter.interest.point , aes(x = lon, y = lat, col = id.drifter))+geom_path()+geom_point()+ metR::scale_x_longitude(ticks = .3)+ metR::scale_y_latitude(ticks = .4) 7.8 Advanced plots ggplot2 with its add-in packages provides extensive capabilities for visualizing your data. You can produce 2D plots, 3D plots, and animations; you can view images; and you can create histograms, contour and surfaces plots, and other graphical representations of your data. itss fun to explore pattern over time with linegraphy. Time is so embedded in our daytoday life tha so many aspects of visualizing temporal data are fairly intuitive. You understand thing changing and evolvingthe hard part is figuring out by how much is changing and look for that change in the graph. Its easy to glance over some lines on a plot and say something is increasing, which is good as that is the core function of visualization to help you see the patterns. But, because of averaging, we sometimes miss some subtle changes that hidden when we lumped data together. chl.tb %&gt;% mutate(month = lubridate::month(date), year = lubridate::year(date)) %&gt;% group_by(year, month) %&gt;% summarise(chl = mean(chl, na.rm = TRUE))%&gt;% filter(year &lt; 2019) %$% interpolate2(x = year , y = month, z = chl, n = 16)%&gt;% rename(year = x, month = y, chl = z) %&gt;% filter(chl &lt; .4)%&gt;% ggplot(aes(x =year, y = month, z = chl))+ metR::geom_contour_fill(na.fill = TRUE, bins = 20)+ scale_fill_gradientn(colours = oce::oce.colorsJet(n = 120))+ scale_y_reverse(breaks = 1:12, label = seq(lubridate::dmy(010119), lubridate::dmy(311219), by = &quot;month&quot;) %&gt;% lubridate::month(abbr = TRUE, label = TRUE))+ scale_x_continuous(breaks = seq(2002,2017,2))+ coord_cartesian(expand = FALSE)+ labs(x = NULL, y = NULL)+ theme_bw()%+% theme(axis.text = element_text(size = 11, colour = &quot;black&quot;))+ guides(fill = guide_colorbar(raster = FALSE, barheight = 15, barwidth = 1.1, title.theme = element_text(angle = 90, size = 13), title.position = &quot;right&quot;, title.hjust = .5, label.theme = element_text(size = 10), title = expression(Chlorophyll~concetration~(mgm^{-3})))) I used oce::oce.colorsJet(120) and specify 120 color gradient to obtain color scheme similar to the Matlab Jet scheme. It widely used for maps but works great for general visualization like heatmaps. 7.8.1 Facets Sometimes you may wish to split your plot into facetssubplots that each display one subset of the data. Faceting is used when you wish make subplots from categorical variables. It creates multiple copies of the same type of plot with matching x and y axes. There is a scale(), which allows you to adjust the x and y axes according to groups in the categorical variable. ## list files from the working directory tafiri = list.files(path = &quot;data/&quot;,pattern = &quot;tafiri_&quot;, full.names = TRUE, recursive = TRUE) ## make a vector of variables. The order must be consistency with the files order in var = c(&quot;chl&quot;, &quot;pp&quot;, &quot;sst&quot;) ## make a vector of site. The order must be consistency with the sheets in files sites = c(&quot;Pemba&quot;, &quot;Zanzibar&quot;, &quot;Mafia&quot;, &quot;EEZ&quot;) ## preallocate an empty object tafiri.data = NULL for (i in 1:length(var)){ for (j in 1:length(sites)){ data = readxl::read_excel(path = tafiri[i], sheet = j)%&gt;% rename(date = 1, year = 2, value = 3) %&gt;% mutate(month = lubridate::month(date), day = 15, site = sites[j], variable = var[i], date = lubridate::make_date(year = year, month = month, day = day)) %&gt;% arrange(date) ## stitch processed data frame from each sheet tafiri.data = tafiri.data %&gt;% bind_rows(data) } } For instance, suppose were interested in looking at how the hovmoller of monthly primary productivity varies over time across the four stations, We split this heatmaps by the four stations. We achieve this by simply adding facet_wrap(~site, nrow = 2) layer; ggplot()+ metR::geom_contour_fill(data = tafiri.data %&gt;% filter(variable == &quot;pp&quot;), aes(x = year, y = month, z = value), na.fill = TRUE, bins = 20)+ coord_cartesian(expand = FALSE)+ scale_y_reverse(breaks = seq(2,11,2), label = c(&quot;February&quot;, &quot;April&quot;, &quot;June&quot;, &quot;August&quot;, &quot;October&quot;))+ scale_x_continuous(breaks = seq(2004,2017,4))+ scale_fill_gradientn(colors = oce::oce.colors9A(120), breaks = seq(400,1600,200))+ theme_bw() %+replace% theme(axis.text = element_text(size = 12, colour = 1))+ guides(fill = guide_colorbar(title = expression(mgm^{-3}), title.position = &quot;top&quot;, title.hjust = 0.5, direction = &quot;vertical&quot;, reverse = FALSE, barwidth = unit(.4, &quot;cm&quot;), barheight = unit(4, &quot;cm&quot;)))+ labs(x = NULL, y = NULL)+ guides(fill = guide_colorbar(raster = FALSE, barheight = 15, barwidth = 1.1, title.theme = element_text(angle = 90, size = 13), title.position = &quot;right&quot;, title.hjust = .5, label.theme = element_text(size = 10), title = expression(Primary~Productivity~(Cm^{-3}~yr^{-1}))))+ facet_wrap(~site, nrow = 2) Note the use of the tilde ~ before the site in facet_wrap(). The tilde is required when you want specify the variable that will be used to split the plots into subplots. We can add other arguments in the facet_wrap() function. Let say we want our plot to be in one rows and four columns by simply adding the argument nrow = 1 7.8.2 Sea Surface Temperature ggplot()+ metR::geom_contour_fill(data = tafiri.data %&gt;% filter(variable == &quot;sst&quot;), aes(x = year, y = month, z = value), na.fill = TRUE, bins = 20)+ coord_cartesian(expand = FALSE)+ scale_y_reverse(breaks = seq(2,11,2), label = c(&quot;February&quot;, &quot;April&quot;, &quot;June&quot;, &quot;August&quot;, &quot;October&quot;))+ scale_x_continuous(breaks = seq(2004,2017,4))+ scale_fill_gradientn(colors = oce::oce.colors9A(120))+ theme_bw() %+replace% theme(axis.text = element_text(size = 11, colour = 1), axis.title = element_blank())+ guides(fill = guide_colorbar(title = expression(mgm^{-3}), title.position = &quot;top&quot;, title.hjust = 0.5, direction = &quot;vertical&quot;, reverse = FALSE, barwidth = unit(.4, &quot;cm&quot;), barheight = unit(4, &quot;cm&quot;)))+ guides(fill = guide_colorbar(raster = FALSE, barheight = 10, barwidth = 1.1, title.theme = element_text(angle = 90, size = 13), title.position = &quot;right&quot;, title.hjust = .5, label.theme = element_text(size = 10), title = expression(Sea~Surface~Temperature~(degree*C))))+ facet_wrap(~site, nrow = 1) 7.8.3 Subplot There are occasions when it is convenient to display several plots side-by-side. In these instances, you will want to use the cowplot package. we first create ggplot2 object of Primary productivity for the channels we are interested: for Mafia Channel we assign the name mafia.pp and for Zanzibar Channel zanzibar.pp. The chunk below show how to create the these object. mafia.pp = ggplot()+ metR::geom_contour_fill(data = tafiri.data %&gt;% filter(variable == &quot;pp&quot; &amp; site == &quot;Mafia&quot;), aes(x = year, y = month, z = value), na.fill = TRUE, bins = 20)+ coord_cartesian(expand = FALSE)+ scale_y_reverse(breaks = 1:12, label = seq(lubridate::dmy(010119), lubridate::dmy(311219), by = &quot;month&quot;) %&gt;% lubridate::month(abbr = TRUE, label = TRUE))+ scale_x_continuous(breaks = seq(2004,2017,4))+ scale_fill_gradientn(colors = oce::oce.colors9A(120),limits = c(300,1700), breaks = seq(400,1600,200))+ theme_bw() %+replace% theme(axis.text = element_text(size = 12, colour = 1), legend.position = &quot;none&quot;)+ guides(fill = guide_colorbar(title = expression(mgm^{-3}), title.position = &quot;top&quot;, title.hjust = 0.5, direction = &quot;vertical&quot;, reverse = FALSE, barwidth = unit(.4, &quot;cm&quot;), barheight = unit(4, &quot;cm&quot;)))+ labs(x = NULL, y = NULL)+ guides(fill = guide_colorbar(raster = FALSE, barheight = 15, barwidth = 1.1, title.theme = element_text(angle = 90, size = 13), title.position = &quot;right&quot;, title.hjust = .5, label.theme = element_text(size = 10), title = expression(Primary~Productivity~(Cm^{-3}~yr^{-1})))) zanzibar.pp = ggplot()+ metR::geom_contour_fill(data = tafiri.data %&gt;% filter(variable == &quot;pp&quot; &amp; site == &quot;Zanzibar&quot;), aes(x = year, y = month, z = value), na.fill = TRUE, bins = 20)+ coord_cartesian(expand = FALSE)+ scale_y_reverse(breaks = 1:12, label = seq(lubridate::dmy(010119), lubridate::dmy(311219), by = &quot;month&quot;) %&gt;% lubridate::month(abbr = TRUE, label = TRUE))+ scale_x_continuous(breaks = seq(2004,2017,4))+ scale_fill_gradientn(colors = oce::oce.colors9A(120), limits = c(300,1700), breaks = seq(400,1600,200))+ theme_bw() %+replace% theme(axis.text = element_text(size = 12, colour = 1), axis.text.y = element_blank())+ guides(fill = guide_colorbar(title = expression(mgm^{-3}), title.position = &quot;top&quot;, title.hjust = 0.5, direction = &quot;vertical&quot;, reverse = FALSE, barwidth = unit(.4, &quot;cm&quot;), barheight = unit(4, &quot;cm&quot;)))+ labs(x = NULL, y = NULL)+ guides(fill = guide_colorbar(raster = FALSE, barheight = 15, barwidth = 1.1, title.theme = element_text(angle = 90, size = 13), title.position = &quot;right&quot;, title.hjust = .5, label.theme = element_text(size = 10), title = expression(Primary~Productivity~(Cm^{-3}~yr^{-1})))) Once we have the ggplot2 objects, we can now combine them side by side with the cowplot::plot_grid() function. cowplot::plot_grid(mafia.pp, zanzibar.pp, nrow = 1, rel_widths = c(.8,1), labels = c(&quot;Mafia&quot;, &quot;Zanzibar&quot;), label_x = c(.2,.001), label_y = .98, label_fontface = &quot;plain&quot;, label_size = 12) profile = ctd %&gt;% filter(station == &quot;st2&quot;) ggplot(data = profile, aes(x = temperature, y = pressure)) + geom_point() This produce a scatterplot shown in figure ??, which shows a strong correlation as the water depth increase the temperature decrease. Using the *grammar of graphics of the ggplot2**, the structure of the code used to make figure ?? is defined by; Data: profile, Aesthetic mapping: temperature values mapped to x position, pressure values mapped to y position Geometry: points In ggplot2, a plot is created with the ggplot() function, where data and aesthetic mappings are supplied as arguments, then layers are added on with +. 7.9 Colour The aesthetic component in ggplot2 allows to add additional variables to a graph. For instance, We can map the colors of the points to the temperature variable to reveal the gradient of dissolved oxygen as a function of depth ggplot(data = profile, aes(x = temperature, y = pressure, col = oxygen)) + geom_point() 7.10 size The aesthetic component in ggplot2 also allows to distinguish the plot with size of the variableh. For instance, We can map the size of the points to the temperature variable to reveal the gradient of dissolved oxygen as a function of depth ggplot(data = profile %&gt;% sample_frac(.25), aes(x = temperature, y = pressure, col = oxygen, size = oxygen)) + geom_point() Although we can map the temperature profile as points, it is appropriate to plot this profile as line. We can replace the geom_point() with geom_path() for that purpose ggplot(data = profile, aes(x = temperature, y = pressure, col = oxygen)) + geom_path() 7.11 scaling A scale controls the mapping from data to aesthetic attributes. Its recommended to manipulate the scale for every aesthetic used on a plot. For example we want the x-axis labels to be position at the top and reverse the y-axis. The code can be writeen as; ggplot(data = profile, aes(x = temperature, y = pressure, col = oxygen)) + geom_path()+ scale_y_reverse(name = &quot;Pressure [m]&quot;, breaks = seq(0,810,100))+ scale_x_continuous(position = &quot;top&quot;, name = expression(Temperature~(degree*C)), breaks = seq(6,29,3)) We might want to change the default color provided with ggplot2 on the legend. Since we commanded the plot to display gradient colors from dissolved oxygen, We use the scale_color_gradientn() function. The code can be writeen as; ggplot(data = profile, aes(x = temperature, y = pressure, col = oxygen)) + geom_path()+ scale_y_reverse(name = &quot;Pressure [m]&quot;, breaks = seq(0,810,100))+ scale_x_continuous(position = &quot;top&quot;, name = expression(Temperature~(degree*C)), breaks = seq(6,29,3))+ scale_color_gradientn(colours = rainbow(11) %&gt;% rev()) 7.12 Guides Context is provided by guides. A guide help a human reader to understand the meaning of the visual cues. For example ggplot(data = profile, aes(x = temperature, y = pressure, col = oxygen)) + geom_path()+ scale_y_reverse(name = &quot;Pressure [m]&quot;, breaks = seq(0,810,100))+ scale_x_continuous(position = &quot;top&quot;, name = expression(Temperature~(degree*C)), breaks = seq(6,29,3))+ scale_color_gradientn(colours = rainbow(11) %&gt;% rev())+ # theme(legend.position = &quot;right&quot;) + guides(color = guide_colorbar(title = &quot;Dissolved oxygen (ml/L)&quot;, title.position = &quot;right&quot;, title.theme = element_text(angle = 90), barheight = 15, barwidth = .95, title.hjust = .5, ticks.colour = &quot;black&quot;)) ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = lon, y = lat))+ geom_point()+ ggrepel::geom_text_repel(aes(label = station)) 7.13 Add-on packages The R community has developed packages that extend the capability of ggplot2. Some of the packages include: metR: Provide addition tools for plotting filled contour, and label contour lines ggrepel: Contains tools for automatically position non-overlapping text labels ggspatial: Spatial Data Framework for ggplot2 RcolorBrewer: Contains color palettes for continuous and discrete plots cowplot: Contains addition themes and tools to combine ggplot2 plots in one panel egg: Provide tools for plot aligning and symmetrised ggplot2 plots oce: Provide color pallete for visualization of Oceanographic Data ggsn: Provide tools for mapping North symbols and scale bars on maps created with ggplot2 gganimate: convert static ggplot2 plots to animations ggformula: adds some additional plot options to ggplot2 sf : Add capabilities of ggplot2 to map spatial data such as simple features ggthemes: contains extra themes, scales, and geoms, and functions for and related to ggplot2 ggridges: extend the geom_density function by plotiing closed polygons insted of ridgelines 7.14 ggridges Althoug ehe ggridges package provides geom_ridgeline and geom_density_ridges, we focus on the latter because it has ability to estimates data densities and then draws those using ridgelines.The geom geom_density_ridges calculates density estimates from the provided data and then plots those, using the ridgeline visualization. ctd = ctd %&gt;% mutate(strata = cut(x = pressure, breaks = c(0,80,120,200), labels = c(&quot;Upper&quot;, &quot;Middle&quot;, &quot;Lower&quot;))) ctd.strata = ctd %&gt;% group_by(strata, pressure, lon,lat) %&gt;% summarise(temperature = mean(temperature, na.rm = TRUE), salinity = mean(salinity, na.rm = TRUE), oxygen = mean(oxygen, na.rm = TRUE), fluorescence = mean(fluorescence, na.rm = TRUE)) %&gt;% ungroup() %&gt;% filter(!is.na(strata)) ggplot(data = ctd.strata, aes(x = temperature, y = strata))+ ggridges::geom_density_ridges2()+ scale_y_discrete(limits = c(&quot;Lower&quot;, &quot;Middle&quot;, &quot;Upper&quot;)) Trailing tails can be cut off using the rel_min_height() aesthetic. This aesthetic sets a percent cutoff relative to the highest point of any of the density curves. A value of 0.01 usually works well, but you may have to modify this parameter for different datasets. ggplot(data = ctd.strata, aes(x = temperature, y = strata))+ ggridges::geom_density_ridges2(rel_min_height = 0.01)+ scale_y_discrete(limits = c(&quot;Lower&quot;, &quot;Middle&quot;, &quot;Upper&quot;)) ggplot(data = ctd.strata, aes(x = temperature, y = strata))+ ggridges::geom_density_ridges2(scale = 8,rel_min_height = 0.01)+# scale =80, substantial overlap scale_y_discrete(limits = c(&quot;Lower&quot;, &quot;Middle&quot;, &quot;Upper&quot;)) 7.15 Varying fill colors along the x axis Sometimes we would like to have the area under a ridgeline not filled with a single solid color but rather with colors that vary in some form along the x axis. This effect can be achieved with the geoms geom_ridgeline_gradient and geom_density_ridges_gradient. Both geoms work just like geom_ridgeline and geom_density_ridges, except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill. For technical reasons, we can have changing fill colors or transparency but not both. Here is a simple example of changing fill colors with geom_ridgeline_gradient: ggplot(data = ctd.strata, aes(x = temperature, y = strata, fill = strata))+ ggridges::geom_density_ridges_gradient(scale = 5,rel_min_height = 0.01)+# scale =5, substantial overlap scale_y_discrete(limits = c(&quot;Lower&quot;, &quot;Middle&quot;, &quot;Upper&quot;)) ggplot(data = ctd.strata, aes(x = temperature, y = strata, fill = ..x..))+ ggridges::geom_density_ridges_gradient(scale = 5,rel_min_height = 0.01)+# scale =5, substantial overlap scale_y_discrete(limits = c(&quot;Lower&quot;, &quot;Middle&quot;, &quot;Upper&quot;))+ scale_fill_gradientn(colours = oce::oce.colorsTemperature(120), breaks = seq(14,26,2))+ guides(fill = guide_colorbar(title =expression(Temperature~~(degree*C)), title.position = &quot;right&quot;, title.hjust = .5, raster = TRUE, title.theme = element_text(angle = 90, size = 12), label.theme = element_text(size = 11), barheight = 15, barwidth = .95)) 7.16 metR transect2 = ctd %&gt;% filter( lat &gt;= -8 &amp;lat &lt; -6 &amp; pressure &lt; 205) ggplot(data = transect2, aes(x = lon, y = pressure, z = fluorescence))+ metR::geom_contour_fill(na.fill = TRUE, bins = 20)+ # metR::geom_contour2()+ scale_y_reverse(breaks = seq(0, 205,30))+ scale_x_continuous(breaks = transect2 %&gt;% distinct(lon) %&gt;% pull(), labels = metR::LonLabel(transect2 %&gt;% distinct(lon) %&gt;% pull()%&gt;%round(digits = 2)))+ scale_fill_gradientn(colours = oce::oceColors9A(120), breaks = seq(0,2,.2))+ coord_cartesian(expand = FALSE)+ guides(fill = guide_colorbar(title =expression(Chlorophyll~concentration~(mgm^{-3})), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 12), label.theme = element_text(size = 11), barheight = 15, barwidth = .95))+ labs(x = NULL, y = &quot;Water depth [m]&quot;)+ geom_vline(xintercept = transect2 %&gt;% distinct(lon) %&gt;% pull(), linetype = &quot;dashed&quot;, col = &quot;ivory&quot;)+ theme_bw()+ theme(axis.text = element_text(size = 11), axis.title = element_text(size = 12)) depth = c(10,50,100,200) algoa = NULL for (i in seq_along(depth)){ strata = ctd %&gt;% filter(lon &gt; 39.3 &amp; lon &lt; 40.5 &amp; lat &gt;= -10 &amp; pressure == depth[i]) %&gt;% select(3:9) Lon = strata %&gt;% pull(lon) Lat = strata %&gt;% pull(lat) data = strata %&gt;% select(4:7) for (j in seq_along(data)){ algoa.interp = strata %$% oce::interpBarnes(x = Lon, y = Lat, z = data[j]%&gt;% pull()) algoa.tb = algoa.interp %$% matrix_tb(x = xg, y = yg, data = zg) %&gt;% rename(lon = x, lat = y) %&gt;% mutate(variable = colnames(data[j]), pressure = depth[i]) algoa = algoa %&gt;% bind_rows(algoa.tb) } } # algoa %&gt;% group_by(pressure, variable) %&gt;% summarise(average = mean(value, na.rm = TRUE)) ggplot()+ metR::geom_contour_fill(data = algoa %&gt;% filter(variable == &quot;fluorescence&quot;), aes(x = lon, y = lat, z = value), na.fill = TRUE, bins = 20)+ scale_fill_gradientn(colours = oce::oceColors9A(120))+ scale_y_continuous(breaks = seq(-8.5,-6,2.5), labels = metR::LatLabel(seq(-8.5,-6,2.5)))+ scale_x_continuous(breaks = seq(39.55,40.25,length.out = 3) %&gt;% round(2), labels = metR::LonLabel(seq(39.55,40.25,length.out = 3)))+ facet_wrap(~pressure)+ labs(x = NULL, y = NULL)+ theme_bw()+ theme(axis.text = element_text(size = 11))+ coord_cartesian(expand = FALSE)+ guides(fill = guide_colorbar(title =expression(Chlorophyll~concentration~(mgm^{-3})), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 12), label.theme = element_text(size = 11), barheight = 15, barwidth = .95)) ggplot()+ metR::geom_contour_fill(data = algoa %&gt;% filter(variable == &quot;temperature&quot;), aes(x = lon, y = lat, z = value), na.fill = TRUE, bins = 20)+ scale_fill_gradientn(colours = oce::oceColors9A(120))+ scale_y_continuous(breaks = seq(-8.5,-6,2.5), labels = metR::LatLabel(seq(-8.5,-6,2.5)))+ scale_x_continuous(breaks = seq(39.55,40.25,length.out = 3) %&gt;% round(2), labels = metR::LonLabel(seq(39.55,40.25,length.out = 3)))+ facet_wrap(~pressure)+ labs(x = NULL, y = NULL)+ theme_bw()+ theme(axis.text = element_text(size = 11))+ coord_cartesian(expand = FALSE)+ guides(fill = guide_colorbar(title =expression(Temperature~(degree*C)), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 12), label.theme = element_text(size = 11), barheight = 15, barwidth = .95)) ggplot()+ metR::geom_contour_fill(data = algoa %&gt;% filter(variable == &quot;oxygen&quot;), aes(x = lon, y = lat, z = value), na.fill = TRUE, bins = 20)+ scale_fill_gradientn(colours = oce::oceColors9A(120))+ scale_y_continuous(breaks = seq(-8.5,-6,2.5), labels = metR::LatLabel(seq(-8.5,-6,2.5)))+ scale_x_continuous(breaks = seq(39.55,40.25,length.out = 3) %&gt;% round(2), labels = metR::LonLabel(seq(39.55,40.25,length.out = 3)))+ facet_wrap(~pressure)+ labs(x = NULL, y = NULL)+ theme_bw()+ theme(axis.text = element_text(size = 11))+ coord_cartesian(expand = FALSE)+ guides(fill = guide_colorbar(title =expression(Dissolved~oxygen~(mgm^{-3})), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 12), label.theme = element_text(size = 11), barheight = 15, barwidth = .95)) ggplot()+ metR::geom_contour_fill(data = algoa %&gt;% filter(variable == &quot;salinity&quot;), aes(x = lon, y = lat, z = value), na.fill = TRUE, bins = 20)+ # scale_fill_gradientn(colours = oce::oceColors9A(120))+ # scale_y_continuous(breaks = seq(-8.5,-6,2.5), # labels = metR::LatLabel(seq(-8.5,-6,2.5)))+ # scale_x_continuous(breaks = seq(39.55,40.25,length.out = 3) %&gt;% round(2), # labels = metR::LonLabel(seq(39.55,40.25,length.out = 3)))+ facet_wrap(~pressure)+ labs(x = NULL, y = NULL)+ theme_bw()+ theme(axis.text = element_text(size = 11, colour = &quot;black&quot;))+ coord_cartesian(expand = FALSE)+ guides(fill = guide_colorbar(title =expression(Salinity), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 12), label.theme = element_text(size = 11), barheight = 15, barwidth = .95)) References "],["a-grammar-for-graphics.html", "Chapter 8 A grammar for graphics 8.1 geoms 8.2 aes() 8.3 Axis and labels 8.4 Text annotaion 8.5 Scales 8.6 Guides 8.7 Themes", " Chapter 8 A grammar for graphics 8.1 geoms In this section, we will create some of the most routinely used plots to explore data using the geom_ functions. We will use the following libraries in this post: readr ggplot2 tibble dplyr Which are part of the tidyverse. By loading the tidyverse, we also load all the packages mentioned above require(tidyverse) All the data sets used in this post can be found here and code can be downloaded from here. octopus = read_csv(&quot;./data/octopus_data.csv&quot;) The variables representing the X and Y axis can be specified either in ggplot() or in geom_point(). We will learn to modify the appearance of the points in a different post. ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point() 8.1.1 Regression Line You can fit the regression on the scatterplot with geom_smooth() lm = ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;lm&quot;, se = TRUE)+ labs(x = NULL, y = NULL, title = &quot;linear model&quot;) gam = ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;gam&quot;, se = TRUE)+ labs(x = NULL, y = NULL, title = &quot;GAM&quot;) loess = ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;loess&quot;, se = FALSE)+ labs(x = NULL, y = NULL, title = &quot;LOESS&quot;) egg::ggarrange(lm,gam, loess, nrow = 1) 8.1.2 Horizontal/ vertical lines A segment of horizontal or vertical line can be added on the plot using egg::ggarrange 8.1.2.1 Vertical Line For the vertical line, the x axis intercept must be specified in geom_vline() ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_vline(xintercept = 100, linetype = 1, size = .5, col = &quot;red&quot;) 8.1.2.2 Vertical Line In similar manner, for the horizontal line, the y axis intercept must be specified in geom_hline() ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_hline(yintercept = 2, linetype = 1, size = .5, col = &quot;red&quot;) 8.2 aes() In this section, we focus on the aesthetics i.e. color, shape, size, alpha, line type, line width etc. We can map these to variables or specify values for them. If we want to map the above to variables, we have to specify them within the aes() function. We will look at both methods in the following sections. Explore aesthetics such as color shape size fill alpha width 8.2.1 Color In ggplot2, when we mention color or colour, it usually refers to the color of the geoms. The fill argument is used to specify the color of the shapes in certain cases. In this section, we will see how we can specify the color for the different geoms we learnt in the previous post. For points, the color argument specifies the color of the point for certain shapes and border for others. The fill argument is used to specify the background for some shapes and will not work with other shapes. Let us look at an example: ggplot(data = octopus, aes(x = tl, y = weight, col = sex)) + geom_point() If you do not want to map a variable to color, you can specify it separately using the color argument but in this case it should be outside the aes() function. ggplot(data = octopus, aes(x = tl, y = weight, col = sex)) + geom_point(col = &quot;blue&quot;) 8.2.2 shape ggplot(data = octopus, aes(x = tl, y = weight, shape = sex)) + geom_point() Let us map size of points to a variable. It is advised to map size only to continuous variables and not categorical variables. ggplot(data = octopus, aes(x = tl, y = weight, col = sex, size = dml)) + geom_point() 8.3 Axis and labels In this section, we learn about about aesthetic and focus on add title and subtitle to the plot modify axis labels modify axis range remove axis format axis Let us start with a simple scatter plot. We will continue to use the octopus data set and examine the relationship between total length and body weight using geom_point(). oct = ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;gam&quot;, se = TRUE) We add the axis labels, title and subtitle for the plot using the labs() ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;gam&quot;, se = TRUE) + labs(x = &quot;Total length (cm)&quot;, y = &quot;Weight (g)&quot;, title = &quot;Octopus&quot;, subtitle = &quot;The total length and body weight of octopus&quot;) ## Axis Range Often times, you may want to modify the range of the axis value. In ggplot2, we can achieve this using scale_function ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;gam&quot;, se = TRUE) + labs(x = &quot;Total length (cm)&quot;, y = &quot;Weight (g)&quot;, title = &quot;Octopus&quot;, subtitle = &quot;The total length and body weight of octopus&quot;)+ scale_x_continuous(breaks = seq(30,180,30))+ scale_y_continuous(breaks = seq(0,5,1)) Sometimes the axis label become a reduntat ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;gam&quot;, se = TRUE) + theme(axis.title = element_blank()) 8.4 Text annotaion Annotation help to add custom text to the plot. ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;gam&quot;, se = TRUE) + annotate(geom = &quot;text&quot;, x = 20, y = 2.2, label = &quot;outlier&quot;, color = &quot;red&quot;) 8.5 Scales Whenever you specify an aesthetic mapping, ggplot2 uses a particular scale to determine the range of values that the data encoding should be mapped to. However, there times you need to customize the scale. ggplot2 has scales_*() function that allows to modify titles, labels, limits, breaks and position of the axis. Each scale can be represented by a function named in the following format: scale_, followed by the name of the aesthetic property (e.g., x or color), followed by an _ and the type of the scale (e.g., continuous or discrete). A continuous scale will handle values such as numeric data (where there is a continuous set of numbers), whereas a discrete scale will handle values such as colors (since there is a small discrete list of distinct colors). In simple language, the x and y-axis of a continuous data is modified with the scale_x_continuous() and scale_y_continuous() functions. ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;gam&quot;, se = TRUE)+ scale_x_continuous(limits = c(50, 150), breaks = seq(50,150,20))+ scale_y_continuous(limits = c(0,3), breaks = seq(.5, 3, .5)) The x and y-axis of a continuous data is modified with the scale_x_discrete() and scale_y_continuous() functions. ggplot(data = mafia.chl.season , aes(x = season, y = chl))+ geom_boxplot( outlier.colour = &quot;red&quot;, outlier.shape = 8, outlier.size = 4)+ scale_x_discrete(limits = c(&quot;NE&quot;, &quot;IN&quot;, &quot;SE&quot;))+ scale_y_continuous(breaks = seq(0.5,2.5,.4)) when the data has been transformed, for instance because of the low value, chlorophyll-a are often stretched with the log-tranformation for visual appeal. But the log-transformed values make no sense about concentration and hence the real values must replace them. We can change the tick labels using the labels argument. When adding labels, tick breaks and labels must have the same length. ggplot(data = mafia.chl.season , aes(x = season, y = chl %&gt;% log()))+ geom_boxplot( outlier.colour = &quot;red&quot;, outlier.shape = 8, outlier.size = 4)+ scale_x_discrete(limits = c(&quot;NE&quot;, &quot;IN&quot;, &quot;SE&quot;), labels = c(&quot;Inter&quot;, &quot;Northeast&quot;, &quot;Southeast&quot;))+ scale_y_continuous(breaks = seq(-0.5,1,length.out = 5), labels = seq(0.5,2.6,length.out = 5)) The position of the axes can be changed using the position argument. For instance, to move the the x-axis to the top of the plot you only need to specify position = top as written in code below; ggplot(data = mafia.chl.season , aes(x = season, y = chl %&gt;% log()))+ geom_boxplot( outlier.colour = &quot;red&quot;, outlier.shape = 8, outlier.size = 4)+ scale_x_discrete(position = &quot;top&quot;, limits = c(&quot;NE&quot;, &quot;IN&quot;, &quot;SE&quot;), labels = c(&quot;Inter&quot;, &quot;Northeast&quot;, &quot;Southeast&quot;))+ scale_y_continuous(breaks = seq(-0.5,1,length.out = 5), labels = seq(0.5,2.6,length.out = 5)) Fill the boxplot with season to specify the colors and arrange the colors manual with scale_fill_manual() function as written below. ggplot(data = mafia.chl.season , aes(x = season, y = chl %&gt;% log(), fill = season))+ geom_boxplot( outlier.colour = &quot;red&quot;, outlier.shape = 8, outlier.size = 4)+ scale_x_discrete(position = &quot;top&quot;, limits = c(&quot;NE&quot;, &quot;IN&quot;, &quot;SE&quot;), labels = c(&quot;Inter&quot;, &quot;Northeast&quot;, &quot;Southeast&quot;))+ scale_y_continuous(breaks = seq(-0.5,1,length.out = 5), labels = seq(0.5,2.6,length.out = 5)) Note the order of layers matter here: you scale_fill_manual() function must start before scale_x_discrete() function. Otherwise the colours you specify mismatch with legend colors as shown ggplot(data = mafia.chl.season , aes(x = season, y = chl %&gt;% log(), fill = season))+ geom_boxplot( outlier.colour = &quot;red&quot;, outlier.shape = 8, outlier.size = 4)+ scale_x_discrete(position = &quot;top&quot;, limits = c(&quot;NE&quot;, &quot;IN&quot;, &quot;SE&quot;), labels = c(&quot;Inter&quot;, &quot;Northeast&quot;, &quot;Southeast&quot;))+ scale_fill_manual(values = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;))+ scale_y_continuous(breaks = seq(-0.5,1,length.out = 5), labels = seq(0.5,2.6,length.out = 5)) The scale_*_reverse() allows to reverse the order of the axis. For instance, when plotting profiles, we reverse y-xis and position the label of x-axis at the top ggplot(data = algoa.average, aes(x = value, y = pressure))+ geom_path() + scale_y_reverse(limits = c(800,0))+ scale_x_continuous(position = &quot;top&quot;) + facet_wrap(~variable, scales = &quot;free_x&quot;, nrow = 1) 8.6 Guides guides() helps to set, modify and remove legend for a specific aesthetic. It has two functionsguide_legend() or guide_colorbar(). Let make a section plot of the fluorescence with the default options for the legend section =ggplot(data = algoa %&gt;% filter(lat &lt; -10), aes(x = lon, y = pressure, z = fluorescence)) + metR::geom_contour_fill(na.fill = TRUE, bins = 60) + metR::scale_x_longitude(ticks = .15) + scale_y_reverse(limits = c(200,0))+ scale_fill_gradientn(colours = oce::oce.colors9A(120), breaks = seq(0.15,1.25, length.out = 6) %&gt;%round(2))+ labs(subtitle = paste(&quot;Section of oxygen along latitude&quot;, metR::LatLabel(-10.54)))+ coord_cartesian(expand = TRUE, ylim = c(180,15)) section We can add the contour labels and remove the legend in a graph section + metR::geom_contour2() + # metR::geom_text_contour() + guides(fill = FALSE) The guide_colorbar() modify the look and appearance of the legend to smooth colorbar. for instane the code of lines below highlight the key arguments that one has to specify to modify the legend of colorbar. section + guides(fill = guide_colorbar(title = expression(Chlorophyll~concentration~(mgm^{-3})), title.position = &quot;right&quot;, title.theme = element_text(angle = 90, size = 13), title.hjust = .5, label.theme = element_text(angle = 0, size = 11), label.position = &quot;right&quot;, raster = FALSE, nbin = 12, reverse = FALSE, barwidth = 1.1, barheight = 15)) If you want the legend to appear as individual key, use guide_colorbar() as written in the code below section + theme(legend.position = &quot;bottom&quot;) + guides(fill = guide_legend(title = expression(Chlorophyll~concentration~(mgm^{-3})), title.position = &quot;top&quot;, title.theme = element_text(angle = 0, size = 13), title.hjust = .5,nrow = 1, reverse = FALSE, keywidth = 3., keyheight = .8, direction = &quot;horizontal&quot;, label.theme = element_text(angle = 0, size = 11), label.position = &quot;bottom&quot;)) 8.7 Themes Themes in ggplot modify the appearance of all non data compoments in the plot like: axis, legend, panel, plot area, background, margin, facets etc. Lets create a profile plot of temperature with default theme settings profile = ggplot(data = algoa %&gt;% filter(lat &lt; -10), aes(x = temperature, y = pressure, color = station))+ geom_path() + scale_y_reverse(limits = c(800,0))+ scale_x_continuous(position = &quot;top&quot;) + labs(y = &quot;Pressure [m]&quot;,x = expression(Temperature~(degree*C))) profile We can modify the size and color of axis label with the axis.text() and axis title with axis.title() functions. You can use axis.title.y to modify the Y axis title and to modify the title of both the axis together, use axis.title. profile + theme(axis.text = element_text(size = 11, colour = &quot;black&quot;), axis.title = element_text(size = 14, colour = &quot;black&quot;)) To modify the appearance of the axis ticks, use the axis.ticks_* argument. You can change the color, size, linetype and length of the ticks using the element_line() function as shown below. profile + theme(axis.text = element_text(size = 11, colour = &quot;black&quot;), axis.title = element_text(size = 14, colour = &quot;black&quot;), axis.ticks.length = unit(.3, &quot;cm&quot;)) The panel_grid argument is used to modify the appearance of the gridlines. You can change the color, size and linetype of the line using the element_line() function. profile + theme(axis.text = element_text(size = 11, colour = &quot;black&quot;), axis.title = element_text(size = 14, colour = &quot;black&quot;), axis.ticks.length = unit(.3, &quot;cm&quot;), panel.grid = element_line(colour = &quot;grey60&quot;, linetype = 3)) The background of the legend can be modified using the legend.background argument. You can change the background color, the border color and line type using element_rect(). profile + theme(axis.text = element_text(size = 11, colour = &quot;black&quot;), axis.title = element_text(size = 14, colour = &quot;black&quot;), axis.ticks.length = unit(.3, &quot;cm&quot;), panel.grid = element_line(colour = &quot;grey60&quot;, linetype = 3), panel.background = element_rect(fill = &quot;white&quot;, colour = &quot;black&quot;)) Now, let us look at modifying the non-data components of a legend. profile + theme(axis.text = element_text(size = 11, colour = &quot;black&quot;), axis.title = element_text(size = 14, colour = &quot;black&quot;), axis.ticks.length = unit(.3, &quot;cm&quot;), panel.grid = element_line(colour = &quot;grey60&quot;, linetype = 3), panel.background = element_rect(fill = &quot;white&quot;, colour = &quot;black&quot;), legend.key = element_blank(), legend.position = c(.9,.3), legend.background = element_rect(colour = &quot;black&quot;, fill = &quot;white&quot;)) The appearance of the text can be modified using the legend.text argument. You can change the color, size and font using the element_text() function. The position and direction of the legend can be changed using legend.position() function. profile + scale_color_discrete(name = &quot;Stations&quot;)+ theme(axis.text = element_text(size = 11, colour = &quot;black&quot;), axis.title = element_text(size = 14, colour = &quot;black&quot;), axis.ticks.length = unit(.3, &quot;cm&quot;), panel.grid = element_line(colour = &quot;grey60&quot;, linetype = 3), panel.background = element_rect(fill = &quot;white&quot;, colour = &quot;black&quot;), legend.key = element_blank(), legend.position = c(.9,.3), legend.background = element_rect(colour = &quot;black&quot;, fill = &quot;white&quot;), legend.text = element_text(size = 11, colour = &quot;black&quot;), legend.title = element_text(size = 13, colour = &quot;black&quot;)) "],["feature.html", "Chapter 9 Feature Representation 9.1 Vector 9.2 Raster 9.3 Important GIS data formats 9.4 Coordinate reference systems 9.5 Practicals 9.6 Summary 9.7 Questions 9.8 FUrther readings", " Chapter 9 Feature Representation In tutorial 3, we learned how R handles data and went further to create data frame, a fundamental data structure in R. In this tutorial we expand a bit on data frame and see how the geographical feature are presented in R. In Geographical Information System (GIS), every dataset is associated with a coordinate system, which is a system for representing the location of different geographical features and different measurements. Geographic data, geospatial data or geographic information is data that identifies the location of features on Earth. According to Lovelace, Nowosad, and Muenchow (2019), these spatial entities can be represented in a GIS as a vector data model or a raster data model. 9.1 Vector Vector data is good for representing categorical and multivariate data. It also has attribute tables where every record or row corresponds to a feature or an object and every column corresponds to different attributes. Vector data has three different geometric primitives types: points, lines, and polygons. 9.1.1 A point A point is composed of one coordinate pair representing a specific location in a coordinate system. Points represented in figure 9.1 are the most basic geometric primitives having no length or area. By definition a point cant be seen since it has no area; but this is not practical if such primitives are to be mapped. So points on a map are represented using symbols that have both area and shape (e.g. circle, square, plus signs). We seem capable of interpreting such symbols as points, but there may be instances when such interpretation may be ambiguous (e.g. is a round symbol delineating the area of a round feature on the ground such as a large oil storage tank or is it representing the point location of that tank?). require(sf) require(tidyverse) stations = tibble(lon = c(39.094923,39.186061, 39.335302), lat = c(-6.283669, -6.212201,-6.279278), names = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) stations %&gt;% ggplot(aes(x = lon, y = lat))+ geom_point(size = 3) + ggrepel::geom_text_repel(aes(label = names)) Figure 9.1: Point objects serves as sampling stations defined by their X and Y coordinate values and labelled with letters 9.1.2 Polyline A polyline as presented in figure 9.1 is composed of a sequence of two or more coordinate pairs called vertices. A vertex is defined by coordinate pairs, just like a point, but what differentiates a vertex from a point is its explicitly defined relationship with neighboring vertices. A vertex is connected to at least one other vertex. Like a point, a true line cant be seen since it has no area. And like a point, a line is symbolized using shapes that have a color, width and style (e.g. solid, dashed, dotted, etc). Roads and rivers are commonly stored as polylines in a GIS. stations %&gt;% ggplot(aes(x = lon, y = lat)) + geom_line(color = &quot;red&quot;, size = 1.2)+ geom_point(size = 3)+ ggrepel::geom_text_repel(aes(label = names)) Figure 9.2: A simple polyline object defined by connected vertices 9.1.3 Polygon A polygon is composed of three or more line segments whose starting and ending coordinate pairs are the same. Sometimes you will see the words lattice or area used in lieu of polygon. Polygons shown in figure 9.3 represent both length (i.e. the perimeter of the area) and area. They also embody the idea of an inside and an outside; in fact, the area that a polygon encloses is explicitly defined in a GIS environment. If it isnt, then you are working with a polyline feature. If this does not seem intuitive, think of three connected lines defining a triangle: they can represent three connected road segments (thus polyline features), or they can represent the grassy strip enclosed by the connected roads (in which case an inside is implied thus defining a polygon). stations %&gt;% ggplot(aes(x = lon, y = lat)) + geom_polygon(color = &quot;red&quot;, size = 1.2, fill = &quot;red&quot;, alpha = .12)+ geom_point(size = 3)+ ggrepel::geom_text_repel(aes(label = names)) Figure 9.3: A simple polygon object defined by an area enclosed by connected vertices 9.2 Raster Raster data is used for continuous data and stores data in a grid-like arrangement. A raster data model uses an array of cells, or pixels, to represent real-world objects. Raster or gridded data are stored as a grid of values which are rendered on a map as pixels. Each pixel value represents an area on the Earths surface. Raster datasets are commonly used for representing and managing imagery, sea surface temperatures, chlorophyll-a concentration, sea surface height, wind speeed and direction, elevation and bathymetry and numerous other entities. A raster can be thought of as a special case of an area object where the area is divided into a regular grid of cells. But a regularly spaced array of marked points may be a better analogy since rasters are stored as an array of values where each cell is defined by a single coordinate pair inside of most GIS environments. Implicit in a raster data model is a value associated with each cell or pixel. This is in contrast to a vector model that may or may not have a value associated with the geometric primitive. stations %&gt;% ggplot(aes(x = lon, y = lat)) + geom_polygon(color = &quot;red&quot;, size = 1.2, fill = &quot;red&quot;, alpha = .12)+ geom_point(size = 3)+ ggrepel::geom_text_repel(aes(label = names)) Figure 9.4: A simple polygon object defined by an area enclosed by connected vertices 9.3 Important GIS data formats In tutorial 9 we learn some types of geographical data and how to create them. In this tutorial we are going to import already created spatial data into our session and start working on them. There are a number of commonly used geographic data formats that store vector and raster data that you will come across during this course and its important to understand what they are, how they represent data and how you can use them. 9.3.1 Shapefiles Geographic data can be represented using a data storage format called shapefile that stores the location, shape, and attributes of geographic features such as points, lines and polygons. A shapefile is not a unique file, but consists of a collection of related files that have different extensions and a common name and are stored in the same directory. A shapefile has three mandatory files with extensions .shp, .shx, and .dbf: Shapefiles were developed by ESRI, one of the first and now certainly the largest commercial GIS company in the world. Despite being developed by a commercial company, they are mostly an open format and can be used (read and written) by a host of GIS Software applications. A shapefile is actually a collection of files  at least three of which are needed for the shapefile to be displayed by GIS software. They are: .shp - the file which contains the feature geometry .shx - an index file which stores the position of the feature IDs in the .shp file .dbf - the file that stores all of the attribute information associated with the coordinates  this might be the name of the shape or some other information associated with the feature .prj - the file which contains all of the coordinate system information (the location of the shape on Earths surface). Data can be displayed without a projection, but the .prj file allows software to display the data correctly where data with different projections might be being used knitr::include_graphics(&quot;screenshots/shapefile.png&quot;) Figure 9.5: Essential files for constituting a shapefile 9.3.2 Raster Most raster data is now provided in GeoTIFF (.tiff) format, which stands for Geostarionary Earth Orbit Tagged Image File. The GeoTIFF data format was created by NASA and is a standard public domain format. All necessary information to establish the location of the data on Earths surface is embedded into the image. This includes: map projection, coordinate system, ellipsoid and datum type. 9.3.3 Attribute Tables Non-spatial information associated with a spatial feature is referred to as an attribute. A feature on a GIS map is linked to its record in the attribute table by a unique numerical identifier (ID). Every feature in a layer has an identifier. It is important to understand the one-to-one or many-to-one relationship between feature, and attribute record. Because features on the map are linked to their records in the table, many GIS software will allow you to click on a map feature and see its related attributes in the table. Raster data can also have attributes only if pixels are represented using a small set of unique integer values. Raster datasets that contain attribute tables typically have cell values that represent or define a class, group, category, or membership. NOTE: not all GIS raster data formats can store attribute information; in fact most raster datasets you will work with in this course will not have attribute tables. 9.3.4 Geodatabase A geodatabase is a collection of geographic data held within a database. Geodatabases were developed by ESRI to overcome some of the limitations of shapefiles. They come in two main types: Personal (up to 1 TB) and File (limited to 250 - 500 MB), with Personal Geodatabases storing everything in a Microsoft Access database (.mdb) file and File Geodatabases offering more flexibility, storing everything as a series of folders in a file system. In the example below we can see that the FCC_Geodatabase (left hand pane) holds multiple points, lines, polygons, tables and raster layers in the contents tab. 9.4 Coordinate reference systems An important aspect of spatial data is the coordinate reference system (CRS) that is used to represent object on earths surface. A CRS permits us to know the origin and the unit of measurement of the coordinates. Moreover, in the case of dealing with multiple data, knowledge of CRS permits to transform all data to a common CRS. Locations on the Earth can be referenced using geographic (also called unprojected) or projected coordinate reference systems Unprojected or geographic reference systems use longitude and latitude for referencing a location on the Earths three-dimensional ellipsoid surface. Projected coordinate reference systems use easting and northing Cartesian coordinates for referencing a location on a two-dimensional representation of the Earth. Three-dimensional surface of the Earth (left), and two-dimensional representation of the Earth (right).Three-dimensional surface of the Earth (left), and two-dimensional representation of the Earth. 9.4.1 Geographic coordinate systems A geographic coordinate system specifies locations on the Earths three-dimensional surface using latitude and longitude values. Latitude and longitude are angles given in decimal degrees (DD) or in degrees, minutes, and seconds (DMS). The equator is an imaginary circle equidistant from the poles of the Earth that divides the Earth into northern and southern hemispheres. Horizontal lines parallel to the equator (running east and west) are lines of equal latitude or parallels. Vertical lines drawn from the north pole to the south pole are lines of equal longitude or meridians. The prime meridian passes through the British Royal Observatory in Greenwich, England, and determines the eastern and western hemispheres. The latitude of a point on Earths surface is the angle between the equatorial plane and the line that passes through that point and the center of the Earth. Latitude values are measured relative to the equator (0 degrees) and range from -90 degrees at the south pole to 90 degrees at the north pole. The longitude of a point on the Earths surface is the angle west or east of the prime meridian to another meridian that passes through that point. Longitude values range from -180 degrees when running west to 180 degrees when running east. 9.4.2 Projected coordinate systems A map projection is a transformation of the Earths three-dimensional surface as a flat two-dimensional plane. All map projections distort the Earths surface in some fashion and cannot simultaneously preserve all area, direction, shape and distance properties. A common projection is the Universal Transverse Mercator (UTM), which preserves local angles and shapes. The UTM system divides the Earth into 60 zones of 6 degrees of longitude in width. Each of the zones uses a transverse Mercator projection that maps a region of large north-south extent. A position on the Earth is given by the UTM zone number, the hemisphere (north or south), and the easting and northing coordinates in the zone which are measured in meters. eastings are referenced from the central meridian of each zone, and northings are referenced from the equator. The easting at the central meridian of each zone is defined to have a value of 500,000 meters. This is an arbitrary value convenient for avoiding negative easting coordinates. In the northern hemisphere, the northing at the equator is defined to have a value of 0 meters. In the southern hemisphere, the equator has a northing value of 10,000,000 meters. This avoids negative northing coordinates in the southern hemisphere. Further details about this projection can be seen in Wikipedia. 9.4.3 Setting Coordinate Reference Systems in R The Earths shape can be approximated by an oblate ellipsoid model that bulges at the equator and is flattened at the poles. There are different reference ellipsoids in use, and the most common one is the World Geodetic System (WGS84), which is used for example by the Global Positioning System (GPS). Datums are based on specific ellipsoids and define the position of the ellipsoid relative to the center of the Earth. Thus, while the ellipsoid approximates the Earths shape, the datum provides the origin point and defines the direction of the coordinate axes. A CRS specifies how coordinates are related to locations on the Earth. In R, CRS are specified using proj4 strings that specify attributes such as the projection, the ellipsoid and the datum. For example, the WGS84 longitude/latitude projection is specified as +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs. The proj4 string of the UTM zone 29 is given by +proj=utm +zone=29 +ellps=WGS84 +datum=WGS84 +units=m +no_defs and the UTM zone 29 in the south is defined as +proj=utm +zone=29 +ellps=WGS84 +datum=WGS84 +units=m +no_defs +south\" Most common CRS can also be specified by providing the EPSG (European Petroleum Survey Group) code. For example, the EPSG code of the WGS84 projection is 4326. You can simply obtain a data frame with all available CRS in R by typing rgdal::make_EPSG() %&gt;% as_tibble(). This returns a data frame with the EPSG code, notes, and the proj4 attributes for each of the projections. Details of a particular EPSG code, say 4326, can be seen by typing CRS(\"+init=epsg:4326\"), which returns +init=epsg:4326 +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0. We can find the codes of other commonly used projections here Setting a projection may be necessary when the data do not contain information about the CRS. This can be done by assigning CRS(projection) to the data, where projection is the string of projection arguments. In addition, we may wish to transform data d to data with a different projection. To do that, we use st_transform() function of the sf package (Pebesma 2019). An example on how to create a spatial dataset with coordinates given by longitude/latitude, and transform it to a dataset with coordinates in UTM zone 35 in the south using rgdal is given below. # create data with coordinates given by longitude and latitude sites = tibble(name = c(&quot;Kilondo&quot;, &quot;Ifungo&quot;, &quot;Lulandawe&quot;, &quot;Leo&quot;, &quot;Ndumbi&quot;), lat = c(-9.75569, -9.78902, -9.82691, -10.45405, -10.64491), lon = c(34.29028, 34.32203, 34.34792, 34.40365, 34.5195)) ## convert data frame into sf object sites.sf = sites %&gt;% st_as_sf(coords = c(&quot;lon&quot;, &quot;lat&quot;)) # assign CRS WGS84 longitude/latitude sites.sf = sites.sf %&gt;% st_set_crs(4326) # reproject data from longitude/latitude to UTM zone 36 south sits.utm = sites.sf %&gt;% st_transform(32736) 9.5 Practicals 9.5.1 Plotting Point Data Location is a point characterized by its coordinates. Point data comprises attributes of a location or data collected on a parameter from different points. Shapefile is a popular data format for vector data. In R, the sf package loads Shapefile as simple feature and if it contains attributes. 9.5.1.1 Loading shapefile We can import a Shapefile into R, using the st_read function of the sf package. We have to provide the path to the Shapefile as the first parameter for the st_read() function and the name of the Shapefile is the second parameter. Here, we have to provide the name of the Shapefile with the .shp extension. So, if we want to import a Shapefile containing points, we can do so by writing the following sampling.points = sf::st_read(&quot;data/shp/simple_feature.shp&quot;, quiet = TRUE) Lets explore the file sampling.points Simple feature collection with 11 features and 4 fields geometry type: POINT dimension: XY bbox: xmin: 39.50958 ymin: -8.425115 xmax: 42.00623 ymax: -6.414011 geographic CRS: WGS 84 First 10 features: id type depth sst 1 294 marker 29 27.87999 2 300 marker -604 27.97999 3 306 marker -569 27.97999 4 312 marker -485 28.03999 5 318 marker -325 28.03999 6 326 marker -461 28.03999 7 414 marker -505 28.02999 8 428 marker -132 28.23999 9 434 marker -976 28.16999 10 456 marker -3311 28.33999 geometry 1 POINT (39.50958 -6.438159) 2 POINT (39.6318 -6.621774) 3 POINT (39.65447 -6.746649) 4 POINT (39.62563 -6.805321) 5 POINT (39.58374 -6.833973) 6 POINT (39.66476 -6.837384) 7 POINT (39.95728 -7.843535) 8 POINT (39.67712 -8.136846) 9 POINT (39.74853 -8.425115) 10 POINT (42.00623 -7.025368) The short report printed gives the file name, the driver (ESRI Shapefile), mentions that there are 11 features (records, represented as rows) and 4 fields (attributes, represented as columns), the geographic coordinate system and a list of the ten features. This object is of class sampling.points %&gt;% class() [1] &quot;sf&quot; &quot;data.frame&quot; A sampling.points has two classes, sf class that store the geometry of the dataset and data.frame class that store the attribute information We can plot the sampling.points using the ggplot2 package and the map shown in figure 9.6 indicating the position of sampling points ggplot(data = sampling.points) + geom_sf() Figure 9.6: Sampling points 9.5.1.2 Importing point data from Excel In most cases, you will receive spatial data that are organized in tabular form, often times from Excel spreadsheet as shown in figure 9.7. knitr::include_graphics(&quot;screenshots/data_table.png&quot;) Figure 9.7: Tabulated spatial data When you face such a case, then st_read() function is out of space. You will need first to import the data using an appropriate read.csv() function first. stations.df = read.csv(&quot;data/sampling_points.csv&quot;) stations.df lon lat depth sst 1 39.50958 -6.438159 29 27.87999 2 39.63180 -6.621774 -604 27.97999 3 39.65447 -6.746649 -569 27.97999 4 39.62563 -6.805321 -485 28.03999 5 39.58374 -6.833973 -325 28.03999 6 39.66476 -6.837384 -461 28.03999 7 39.95728 -7.843535 -505 28.02999 8 39.67712 -8.136846 -132 28.23999 9 39.74853 -8.425115 -976 28.16999 10 42.00623 -7.025368 -3311 28.33999 11 41.84692 -6.414011 -3248 28.55999 Once the file is in our R session, we can then convert it to simple feature using the longitude and latitude information contained in the file as the chunk below highlight stations.sf = stations.df %&gt;% sf::st_as_sf(coords = c(&quot;lon&quot;, &quot;lat&quot;)) stations.sf Simple feature collection with 11 features and 2 fields geometry type: POINT dimension: XY bbox: xmin: 39.50958 ymin: -8.425115 xmax: 42.00623 ymax: -6.414011 CRS: NA First 10 features: depth sst geometry 1 29 27.87999 POINT (39.50958 -6.438159) 2 -604 27.97999 POINT (39.6318 -6.621774) 3 -569 27.97999 POINT (39.65447 -6.746649) 4 -485 28.03999 POINT (39.62563 -6.805321) 5 -325 28.03999 POINT (39.58374 -6.833973) 6 -461 28.03999 POINT (39.66476 -6.837384) 7 -505 28.02999 POINT (39.95728 -7.843535) 8 -132 28.23999 POINT (39.67712 -8.136846) 9 -976 28.16999 POINT (39.74853 -8.425115) 10 -3311 28.33999 POINT (42.00623 -7.025368) When printed, the report provide the necesary information, however,the coordinate system is missing. That is true, when we import data from table and convert them to simple feature, we also need to define the coordinate system. There is the function st_set_crs(), which can be used in a to define the CRS. Because of the complexity of the CRS, for this case, we borrow from the existing file and use it to define the missing CRS in our file. ## borrow the coordinate from existing simple feature crs = sf::st_crs(sampling.points) ## parse the borrowed crs to define the missing CRS in the file stations.sf.wgs84 = stations.sf %&gt;% sf::st_set_crs(crs) stations.sf.wgs84 Simple feature collection with 11 features and 2 fields geometry type: POINT dimension: XY bbox: xmin: 39.50958 ymin: -8.425115 xmax: 42.00623 ymax: -6.414011 geographic CRS: WGS 84 First 10 features: depth sst geometry 1 29 27.87999 POINT (39.50958 -6.438159) 2 -604 27.97999 POINT (39.6318 -6.621774) 3 -569 27.97999 POINT (39.65447 -6.746649) 4 -485 28.03999 POINT (39.62563 -6.805321) 5 -325 28.03999 POINT (39.58374 -6.833973) 6 -461 28.03999 POINT (39.66476 -6.837384) 7 -505 28.02999 POINT (39.95728 -7.843535) 8 -132 28.23999 POINT (39.67712 -8.136846) 9 -976 28.16999 POINT (39.74853 -8.425115) 10 -3311 28.33999 POINT (42.00623 -7.025368) We can also use the EPSG code, which is a a fourfive digit number that represent Coordinate Reference System, for instance the EPSG code for World Geodetic System of 1984 is 4326. stations.sf %&gt;% sf::st_set_crs(4326) Simple feature collection with 11 features and 2 fields geometry type: POINT dimension: XY bbox: xmin: 39.50958 ymin: -8.425115 xmax: 42.00623 ymax: -6.414011 geographic CRS: WGS 84 First 10 features: depth sst geometry 1 29 27.87999 POINT (39.50958 -6.438159) 2 -604 27.97999 POINT (39.6318 -6.621774) 3 -569 27.97999 POINT (39.65447 -6.746649) 4 -485 28.03999 POINT (39.62563 -6.805321) 5 -325 28.03999 POINT (39.58374 -6.833973) 6 -461 28.03999 POINT (39.66476 -6.837384) 7 -505 28.02999 POINT (39.95728 -7.843535) 8 -132 28.23999 POINT (39.67712 -8.136846) 9 -976 28.16999 POINT (39.74853 -8.425115) 10 -3311 28.33999 POINT (42.00623 -7.025368) Once we have created a simple feature, define the CRS, We can go ahead and plot the stations.sf.wgs84 shown in figure 9.8. ggplot(data = stations.sf.wgs84) + geom_sf() Figure 9.8: Sampling points 9.5.2 Geographic Coordinate Sytem In GIS, every dataset is associated with a coordinate system, which is a system for representing the locations of different geographic features and different measurements. There are two main types of coordinate systems: geographic coordinate systems (GCS) and projected coordination systems. One example of GCS is using latitude-longitude, and one example of a projected coordination system is the transverse Mercator system. Whereas GCS uses a three-dimensional spherical surface, the projected coordination system uses two dimensions for representing spatial data. 9.5.2.1 Changing projection system Data is used in GCS to different the position of the spheroid in relation to the center of the earth; a very commonly used GCS is WGS 84. We may sometimes required to change coordinate system, let us say we want to change from GCS to UTM. That is simple using a st_transform and parse crs of intent. For this case, my data is found in UTM zone 37 south and the EPSG code specific for that zone is 32737 stations.sf.wgs84 %&gt;% sf::st_transform(crs = 32737) Simple feature collection with 11 features and 2 fields geometry type: POINT dimension: XY bbox: xmin: 556349.3 ymin: 9068626 xmax: 832171.7 ymax: 9290155 projected CRS: WGS 84 / UTM zone 37S First 10 features: depth sst geometry 1 29 27.87999 POINT (556349.3 9288332) 2 -604 27.97999 POINT (569839.6 9268018) 3 -569 27.97999 POINT (572326.2 9254210) 4 -485 28.03999 POINT (569130.6 9247727) 5 -325 28.03999 POINT (564498.3 9244566) 6 -461 28.03999 POINT (573450.6 9244177) 7 -505 28.02999 POINT (605535.5 9132879) 8 -132 28.23999 POINT (574595.1 9100511) 9 -976 28.16999 POINT (582402.5 9068626) 10 -3311 28.33999 POINT (832171.7 9222380) 9.6 Summary In this tutorial, we learned about the basics of GIS and learned particularly how vector data is stored in R. We learned to import point data from an Excel file and how to map in R. We used the ggplot2 package of R to accomplish this task along with various options for plotting these. We learned to use the st_read() function of the sf package for importing shapefile containing different vector classes. In this tutorial, we also learned how to transform geographical coordinate system from spherical longitude and latitude to flat surface presented in UTM measured in meters. 9.7 Questions After completing this tutorial, readers should be comfortable to answer the following questions What are two main spatial data classes used to represent feature in GIS? How can you distinguish vector from raster data format used to represent features in GIS? Why we need Cooridnate systems? Distinguish between geographic coordinate system and projected cooridnate system 9.8 FUrther readings When going through vector data manipulation in R, we have just touched the surface of what could be possible using R. We will cover other data manipulation techniques as we step on through proceeding tutorial. But if you want to have a more in-depth discussion about these topics, you can have a look at the book Learning R for Geospatial Analysis by Michael Dorman and An Introduction to R for Spatial Analysis and Mapping by Brunsdon and Comber. References "],["creating-geospatial-data.html", "Chapter 10 Creating Geospatial Data 10.1 Getting data from the web 10.2 Creating Point Data 10.3 Summary 10.4 Questions 10.5 Further reading", " Chapter 10 Creating Geospatial Data In previous tutorial, we learned how to use R for basic geospatial tasks, such as loading vector data, visualizing it, and stylizing it. Weve learned the basics of GIS and vector data, how R store them, and more. This tutorial will introduce you to the creation of data and the editing functionalities of spatial data found in R. Well be introduced to different data sources and how to download data from one of these sites. Often, we need to create a digitized map from a printed map, and well be covering that in this chapter. After this, well learn how to create point, line, and polygon data using QGIS. Well then learn how to add features to a shapefile and how to create and use a spatial database. These skills will enable us to work more eciently with spatial data. The following topics will be covered in this chapter: Getting data from the web Creating vector data Digitizing a map Working with databases 10.1 Getting data from the web Working with geospatial data will often require us to use a spatial file that delineates country borders, roads, railways, rivers, coastlines, and so on. Sometimes, its hard to manage all of this data by yourself. Luckily, we can download free shapefile or vector data and raster from a number of websites for free. One source of free vector and raster data for use in GIS is Natural Earth. Natural Earth is a public domain map dataset available at large (1:10m), medium (1:50m), and small(1:110m) scales. The rst two types of data contain shapefile for cultural, physical, and raster data, whereas small-scale data is available for cultural and physical data only. Figure 10.1 is a screenshot of Natural earth webpage: knitr::include_graphics(&quot;screenshots/naturalearth.png&quot;) Figure 10.1: A naturalearth website preview Rather than manual download of the dataset, We can use rnaturalearth package (South 2017), which allows us to access the Natural Earth page and dowload the data programatically. This package facilitates mapping by making natural earth map data from more easily available to R users in either sp or sf format. To use the package we need to install in our machine; Install from CRAN : install.packages(&quot;rnaturalearth&quot;) or install the development version from GitHub using devtools. devtools::install_github(&quot;ropensci/rnaturalearth&quot;) Data to support much of the package functionality are stored in two data packages that you will be prompted to install when required if you do not do so here. devtools::install_github(&quot;ropensci/rnaturalearthdata&quot;) install.packages(&quot;rnaturalearthhires&quot;, repos = &quot;http://packages.ropensci.org&quot;, type = &quot;source&quot;) 10.1.1 Usage 10.1.1.1 Large Scale Large scale data is not found in the package and you may need to download separate. 10.1.1.2 Medium Scale coastline = rnaturalearth::ne_coastline(returnclass = &quot;sf&quot;, scale = &quot;medium&quot;) countries = rnaturalearth::ne_countries(returnclass = &quot;sf&quot;, scale = &quot;medium&quot;) ggplot()+ geom_sf(data = countries) + geom_sf(data = coastline)+ coord_sf(xlim = c(38.5,40.5), ylim = c(-7.,-4.5), expand = FALSE) 10.1.1.3 Small Scale coastline = rnaturalearth::ne_coastline(returnclass = &quot;sf&quot;, scale = &quot;small&quot;) countries = rnaturalearth::ne_countries(returnclass = &quot;sf&quot;, scale = &quot;small&quot;) ggplot()+ geom_sf(data = countries) + geom_sf(data = coastline)+ coord_sf(xlim = c(38.5,40.5), ylim = c(-7.,-4.5), expand = FALSE) 10.1.2 To download Natural Earth data not already in the package There are a wealth of other data available at the Natural Earth website. rnaturalearth has functions to help with download of these data. The function to perform that task is rnaturalearth::ne_download, which allows to specify scale, type and category and will construct the url and download the corresponding file. 10.1.2.1 Land # rivers land &lt;- rnaturalearth::ne_download(scale = 10, type = &#39;land&#39;, category = &#39;physical&#39;) sp::plot(land, col = &#39;blue&#39;) ggplot()+ geom_sf(data = land)+ coord_sf(xlim = c(38.5,40.5), ylim = c(-7.,-4.5), expand = FALSE) 10.1.2.2 Lakes # lakes lakes &lt;- rnaturalearth::ne_download(scale = 10, type = &#39;lakes&#39;, category = &#39;physical&#39;) ## convert to simple feature lakes.sf = lakes %&gt;% st_as_sf() ggplot()+ geom_sf(data = lakes.sf %&gt;% filter(name == &quot;Lake Victoria&quot;)) Figure 10.2: Lake Victoria ggplot()+ geom_sf(data = lakes.sf %&gt;% filter(name == &quot;Lake Tanganyika&quot;)) Figure 10.3: Lake Tanganyika 10.1.2.3 Rivers # rivers rivers &lt;- rnaturalearth::ne_download(scale = 10, type = &#39;rivers_lake_centerlines&#39;, category = &#39;physical&#39;) sp::plot(rivers110, col = &#39;blue&#39;) rivers.bongo = rivers %&gt;% filter(name %in% c(&quot;Wami&quot;, &quot;Pangani&quot;, &quot;Rufiji&quot;, &quot;Great Ruaha&quot;)) ggplot() + geom_sf(data = rivers.bongo) 10.1.2.4 Corals # rivers reefs &lt;- rnaturalearth::ne_download(scale = 10, type = &#39;reefs&#39;, category = &#39;physical&#39;) sp::plot(reefs, col = &#39;blue&#39;) reefs.bongo = rivers %&gt;% sf::st_crop(xmin = 38, xmax = 41, ymin = -11, ymax = -4) ggplot() + geom_sf(data = reefs.bongo) 10.2 Creating Point Data Creating point vector data is very easy in R. With the combination of leaflet (Cheng, Karambelkar, and Xie 2019) and mapedit (Appelhans, Russell, and Busetto 2020) package, you can easily create all three vectorspoint, lines and polygons interactively in R. You need to have a simple feature in your working directory that the tools will borrow its geographical coordinate system. Let us start by importing the shapefile in our data folder point = sf::st_read(&quot;data/shp/simple_feature.shp&quot;, quiet = TRUE) Once we have the points, we can use that as reference from which all layers we just wat to create will base on for its GCS 10.2.1 Creating point features knitr::include_graphics(&quot;screenshots/point_feature_creation.png&quot;) point.sf = simple_feature %&gt;% mapedit::editFeatures() 10.2.2 Creating line features Creating line and polygon features in R is a very similar process to the workows we followed for creating point feature. Here, well look at some of the steps that we need to take in creating line feature: knitr::include_graphics(&quot;screenshots/line_feature_creation.png&quot;) 10.2.3 Creating Polygon features knitr::include_graphics(&quot;screenshots/ploygon_feature_creation.png&quot;) 10.2.4 Adding features to vector data Weve created data of the polygon type, and we need to add features now by creating polygons and then lling feature attributes accordingly 10.2.5 Digitizing a map In this section, well learn how to digitize a map, which will allow us to work with this map for further spatial analysis. In doing so, we need to know the coordinates of some of the point locations on this image. Well use these location coordinates to digitally get coordinates of all points on the image. These points are called Ground Control Points (GCPs) 10.3 Summary In this tutorial, weve learned how to create vector feature. In doing so, this tutorial showed how we can create point, line, and polygon data. Furthermore, it also covered how we can populate different features with attribute values and how we can use the Georeferencer plugin to digitize an image. We ended the tutorial by learning how create new feature from scanned digital maps. Weve covered just enough to proceed to the next tutorials, where we will delve deep into diferent spatial operations, spatial analysis, and more. We havent talked in detail about spatial databases and many other operations that could be performed using spatial databases. But the topics covered so far should have equipped you with sufficient resources to dig deeper and, in later chapters, to start applying machine learning models in spatial research cases. 10.4 Questions How do you create point, line and polygon features in R How do you digitize an image? 10.5 Further reading Weve shown how to create vector data but havent touched upon the topic of topological error correction. Furthermore, we havent gone into detail about the various options oered by the GDAL Georeferencer plugin. The book Mastering QGIS, by Menke et al, goes into detail explaining these. References "],["working-with-vector-data-in-r.html", "Chapter 11 Working with vector data in R 11.1 Combining shapefile in R 11.2 Clipping in R 11.3 DIfferences in R 11.4 Area calculation in R 11.5 Buffer 11.6 Intersection 11.7 Statistical summary of vector layers 11.8 Summary", " Chapter 11 Working with vector data in R This tutorial introduces you to different types of spatial data manipulation in R. Well learn how to merge shapefiles and clip our data to a vector file, difference between shapefiles, how to get the intersection of point data and line data, and how to create a buffer around a feature. Well use R to demonstrate these operations. All of these techniques are very helpful to have in a geospatial analysts toolbox. After completing this tutorial, youll have hands-on knowledge of the following Combining shapefiles Clipping points to the boundary of a shapefile differences Intersection between two vector les Creating buffers Calculating the area of polygons Converting vector data types Creating statistical summaries of vector files Advanced field calculations Due to the contribution of many developers in the form of R packages, we can now use R as a spatial analysis tool to perform different operations on vector data. To master these, we need to know the basics of spatial data manipulation in R. Some R packages, such as sp, sf, and tidyverse, will be used frequently to accomplish these tasks 11.1 Combining shapefile in R In this example, well merge two shapefile of two districts called Dhaka and Gazipur. Now, BGD_adm3_data_re is a shapefile containing all of the districts of Bangladesh as separate polygons. We have the shapefile of Dhaka but not of Gazipur, so well create a shapefile of Gazipur before we start merging these two shapefile. Lets load the required packages now: require(sf) require(tidyverse) Then, we load the `` shapefile. As we have seen in previous chapters dive.sites = st_read(&quot;e:/GIS/Projects/MASTER/tansea/tza_dive_sites_200k.shp&quot;, quiet = TRUE) dive.sites Simple feature collection with 113 features and 4 fields geometry type: POINT dimension: XY bbox: xmin: 38.98799 ymin: -10.32008 xmax: 40.39575 ymax: -4.88812 geographic CRS: WGS 84 First 10 features: ID SITEID CLASS NAME 1 1 W1 TYPE1 BAWE NORTH 2 2 W2 TYPE1 BAWE SOUTH 3 3 W3 TYPE1 MPAPE REEF 4 4 W4 TYPE1 MUROGO NORTH 5 5 W5 TYPE1 MUROGO SOUTH 6 6 W6 TYPE4 IN THE WRECK 7 7 W7 TYPE1 FUNGU REEF 8 8 W8 TYPE1 PANGE SANDBANK 9 9 W9 TYPE1 NTANGE WEST 10 10 W10 TYPE1 NYANGE EAST geometry 1 POINT (39.13748 -6.148545) 2 POINT (39.13027 -6.161018) 3 POINT (39.09486 -6.189248) 4 POINT (39.12962 -6.184647) 5 POINT (39.12897 -6.19843) 6 POINT (39.14602 -6.181361) 7 POINT (39.15258 -6.181359) 8 POINT (39.1611 -6.182669) 9 POINT (39.11455 -6.239784) 10 POINT (39.14341 -6.24634) dugong.sites = st_read(&quot;e:/GIS/Projects/MASTER/tansea/tza_dugong_sightings_200k.shp&quot;, quiet = TRUE) dugong.sites Simple feature collection with 6 features and 2 fields geometry type: POINT dimension: XY bbox: xmin: 39.31837 ymin: -10.29022 xmax: 40.45371 ymax: -7.917048 geographic CRS: WGS 84 Name Descr geometry 1 Untitled Placemark &lt;NA&gt; POINT (39.31837 -8.346401) 2 Untitled Placemark &lt;NA&gt; POINT (39.35014 -8.325881) 3 Untitled Placemark &lt;NA&gt; POINT (39.39996 -8.230001) 4 Untitled Placemark &lt;NA&gt; POINT (39.38986 -8.304997) 5 Untitled Placemark &lt;NA&gt; POINT (39.46473 -7.917048) 6 Untitled Placemark &lt;NA&gt; POINT (40.45371 -10.29022) dive.sites = dive.sites %&gt;% select(name = NAME)%&gt;% mutate(type = &quot;Dive sites&quot;) %&gt;% sample_n(6) dive.sites Simple feature collection with 6 features and 2 fields geometry type: POINT dimension: XY bbox: xmin: 39.12018 ymin: -10.30071 xmax: 40.3632 ymax: -5.342382 geographic CRS: WGS 84 name type geometry 1 &lt;NA&gt; Dive sites POINT (39.84212 -9.989132) 2 TURTLE GARDEN Dive sites POINT (39.53094 -6.138454) 3 &lt;NA&gt; Dive sites POINT (39.12018 -5.342382) 4 JACKFISH SPOT Dive sites POINT (39.41381 -5.795065) 5 &lt;NA&gt; Dive sites POINT (40.3632 -10.30071) 6 FUNGU REEF Dive sites POINT (39.15258 -6.181359) dugong.sites = dugong.sites %&gt;% select(name = Name) %&gt;% mutate(type = &quot;Dugong sites&quot;) dugong.sites Simple feature collection with 6 features and 2 fields geometry type: POINT dimension: XY bbox: xmin: 39.31837 ymin: -10.29022 xmax: 40.45371 ymax: -7.917048 geographic CRS: WGS 84 name geometry 1 Untitled Placemark POINT (39.31837 -8.346401) 2 Untitled Placemark POINT (39.35014 -8.325881) 3 Untitled Placemark POINT (39.39996 -8.230001) 4 Untitled Placemark POINT (39.38986 -8.304997) 5 Untitled Placemark POINT (39.46473 -7.917048) 6 Untitled Placemark POINT (40.45371 -10.29022) type 1 Dugong sites 2 Dugong sites 3 Dugong sites 4 Dugong sites 5 Dugong sites 6 Dugong sites dive.dugong.sites = dive.sites %&gt;% rbind(dugong.sites) ggplot()+ geom_sf(data = dive.dugong.sites, aes(color = type)) 11.2 Clipping in R Here, well learn how to clip spatial points to a shapefile. For example, we have many data points over many places of Bangladesh and we want to clip these data points to the shapele of Dhaka, named dhaka.shp. In this example, we have a CSV le, arbitrary_indicator.csv, which has two columns, lon and lat, containing the longitude and latitude of these points, respectively. To achieve our objective, we can take the following steps. First, read the CSV le and then use coordinates() to turn it into a spatial object that R can recognize. In doing so, indicate the longitude and latitude elds followed by ~: st_crop() 11.3 DIfferences in R Now, what if we only want the points that are outside Dhaka? Thats what the st_difference() function under the sf package in R helps us to do. With st_difference(), we provide points first, followed by dhaka. This is very straightforward to implement: st_difference() 11.4 Area calculation in R We can very easily calculate the area of all of the features of a polygon using st_area(). We use this to calculate the area, in square kilometers, of difference districts in Bangladesh. First, we transform the area using st_tranform() to Universal Transverse Mercator (UTM) with units in meters; this will make it easy to have the area in square kilometers. Well use the reprojected map of Bangladesh with UTM in st_area() and use an additional argument, byid = TRUE, so that it computes the area for all unique features (in this case, districts). Now, as st_area() returns in square units of whatever unit in which the map was fetched, well get square meters returned by st_area(). So, divide this by 1,000 square to get square kilometers: st_area() 11.5 Buffer Now well learn how to create buffer around a vector file. Using this, we can create a buffer around a point, line, or polygon feature. Suppose we have a shapefile of railways in Dhaka named railway_dhaka.shp. Now, if we want a buffer around this railway, we can use the buer feature of the mmqgis plugin. The steps are for doing so are described here: sf::st_buffer() 11.6 Intersection Intersection gives us back the intersection of two vector les. So, if we point vector data and line vector data, the intersection will give all of the point data that intersects with the line data. We will now use food court locations, saved as food_dhaka.shp, and see whether there are any food courts within a 0.5 radius of the railway line in Dhaka (contained in rail_buffer.shp); we interpret this by seeing all of the points where food_dhaka.shp intersects with rail_buffer.shp. We can do so by performing the following steps 11.7 Statistical summary of vector layers Suppose we have a shapele that has a numeric eld and we want to look at a summary of its statistics, including the count, mean, variance, and quantile. This can be very easily done using the Statistics panel of QGIS. Well now show the step-by-step process of doing this: 11.8 Summary We learned how to use different tools in R and QGIS for working with vector data and modifying or creating new data by doing different operations such as merging, clipping, intersecting, creating buffers, and more. We also learned how to use different spatial queries in R. In doing so, we have seen how important it is for two different layers to have the same projection system. We havent touched upon many other tasks, such as using the rgeos package for calculating distance between geometries using st_distance() or using st_union() for taking the union of two shapefiles. With the tools you are equipped with now, you can easily understand other functionalities not covered here and should now be able to perform most of these data manipulation tasks. "],["working-with-raster-data-in-r.html", "Chapter 12 Working with raster data in R 12.1 NC data 12.2 Changing the projection system of a raster file 12.3 Summary 12.4 Questions", " Chapter 12 Working with raster data in R The two common raster data format widely used in science are .nc and .tiff. R has some dedicated packages for working with these type of raster data. We can use raster (Hijmans 2020), ncdf4 (Pierce 2019), and more to analyze raster data in R. 12.1 NC data We begin with nc data format. We can use the nc_open() function of ncdf4 package to R to read the metadata of raster data in NC format. Lets read the metadata of ocean current stored in nc format: wind = ncdf4::nc_open(&quot;data/nc/wio_geostrophic_uv_july_2015.nc&quot;) wind File data/nc/wio_geostrophic_uv_july_2015.nc (NC_FORMAT_CLASSIC): 3 variables (excluding dimension variables): int crs[] comment: This is a container variable that describes the grid_mapping used by the data in this file. This variable does not contain any data; only information about the geographic coordinate system. grid_mapping_name: latitude_longitude inverse_flattening: 298.257 semi_major_axis: 6378136.3 _CoordinateTransformType: Projection _CoordinateAxisTypes: GeoX GeoY int v[lon,lat,time] _CoordinateAxes: lon lat time lat lon _FillValue: -2147483647 coordinates: lon lat grid_mapping: crs long_name: Absolute geostrophic velocity; meridional component scale_factor: 1e-04 standard_name: surface_northward_geostrophic_sea_water_velocity units: m/s int u[lon,lat,time] _CoordinateAxes: lon lat time lat lon _FillValue: -2147483647 coordinates: lon lat grid_mapping: crs long_name: Absolute geostrophic velocity; zonal component scale_factor: 1e-04 standard_name: surface_eastward_geostrophic_sea_water_velocity units: m/s 3 dimensions: time Size:31 axis: T long_name: Time standard_name: time units: days since 1950-01-01 00:00:00 _CoordinateAxisType: Time lat Size:420 axis: Y bounds: lat_bnds long_name: Latitude standard_name: latitude units: degrees_north _CoordinateAxisType: Lat lon Size:401 axis: X bounds: lon_bnds long_name: Longitude standard_name: longitude units: degrees_east _CoordinateAxisType: Lon 13 global attributes: title: NRT merged all satellites Global Ocean Gridded Absolute Dynamic Topography L4 product institution: CNES, CLS references: http://www.aviso.altimetry.fr source: Altimetry measurements Conventions: CF-1.0 history: Data extracted from dataset http://opendap.aviso.altimetry.fr/thredds/dodsC/dataset-duacs-nrt-over30d-global-allsat-madt-uv time_min: 23922 time_max: 23952 julian_day_unit: days since 1950-01-01 00:00:00 latitude_min: -74.875 latitude_max: 29.875 longitude_min: 19.875 longitude_max: 119.875 This will give us the metadata of this Absolute geostrophic velocity image, where we can see that it has zonal velocity (U) and meridional velocity (V) at each longitude and latitude within the region for 30 days in July 2015, UTM projection, and WGS84 as datum. The metadata is complex, let us make it easy using a tidy format table with the tidync package tidync::tidync(&quot;data/nc/wio_geostrophic_uv_july_2015.nc&quot;) Data Source (1): wio_geostrophic_uv_july_2015.nc ... Grids (5) &lt;dimension family&gt; : &lt;associated variables&gt; [1] D2,D1,D0 : v, u **ACTIVE GRID** ( 5221020 values per variable) [2] D0 : time [3] D1 : lat [4] D2 : lon [5] S : crs Dimensions 3 (all active): dim name length min max start count &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; 1 D0 time 31 23922 2.40e4 1 31 2 D1 lat 420 -74.9 2.99e1 1 420 3 D2 lon 401 19.9 1.20e2 1 401 # ... with 4 more variables: dmin &lt;dbl&gt;, dmax &lt;dbl&gt;, # unlim &lt;lgl&gt;, coord_dim &lt;lgl&gt; Often the tidync gives data in Julian, that is another challenges for people unfamiliar with this date format. Fortunate, R has a a nc2time function from oceanmap package (Bauer 2020), which can turn this complex of julian day into gregorian calender with a single line of code as shown below; oceanmap::nc2time(wind) [1] &quot;2015-07-01&quot; &quot;2015-07-02&quot; &quot;2015-07-03&quot; [4] &quot;2015-07-04&quot; &quot;2015-07-05&quot; &quot;2015-07-06&quot; [7] &quot;2015-07-07&quot; &quot;2015-07-08&quot; &quot;2015-07-09&quot; [10] &quot;2015-07-10&quot; &quot;2015-07-11&quot; &quot;2015-07-12&quot; [13] &quot;2015-07-13&quot; &quot;2015-07-14&quot; &quot;2015-07-15&quot; [16] &quot;2015-07-16&quot; &quot;2015-07-17&quot; &quot;2015-07-18&quot; [19] &quot;2015-07-19&quot; &quot;2015-07-20&quot; &quot;2015-07-21&quot; [22] &quot;2015-07-22&quot; &quot;2015-07-23&quot; &quot;2015-07-24&quot; [25] &quot;2015-07-25&quot; &quot;2015-07-26&quot; &quot;2015-07-27&quot; [28] &quot;2015-07-28&quot; &quot;2015-07-29&quot; &quot;2015-07-30&quot; [31] &quot;2015-07-31&quot; 12.2 Changing the projection system of a raster file We can change the projection system of a raster file using the epsg code to wgs84 as follows raster::raster(&quot;data/nc/wio_geostrophic_uv_july_2015.nc&quot;, band = 30) %&gt;% raster::projectRaster(crs = 4326) class : RasterLayer dimensions : 430, 411, 176730 (nrow, ncol, ncell) resolution : 0.25, 0.25 (x, y) extent : 18.5, 121.25, -76.25, 31.25 (xmin, xmax, ymin, ymax) crs : +proj=longlat +datum=WGS84 +no_defs source : memory names : Absolute.geostrophic.velocity..meridional.component values : -1.6078, 1.3798 (min, max) We can reproject from longitude and latitude to UTM by using the projectRaster() function in the following way raster::raster(&quot;data/nc/wio_geostrophic_uv_july_2015.nc&quot;, band = 30) %&gt;% raster::projectRaster(crs = 32737) class : RasterLayer dimensions : 598, 672, 401856 (nrow, ncol, ncell) resolution : 28500, 30700 (x, y) extent : -1826307, 17325693, 115204.9, 18473805 (xmin, xmax, ymin, ymax) crs : +proj=utm +zone=37 +south +datum=WGS84 +units=m +no_defs source : memory names : Absolute.geostrophic.velocity..meridional.component values : -1.575943, 1.339576 (min, max) 12.2.1 Reading Tiff format rain = raster::raster(&quot;data/raster/wc2.1_10m_prec/wc2.1_10m_prec_01.tif&quot;) rain class : RasterLayer dimensions : 1080, 2160, 2332800 (nrow, ncol, ncell) resolution : 0.1666667, 0.1666667 (x, y) extent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax) crs : +proj=longlat +datum=WGS84 +no_defs source : E:/bookdown/geoMarine/data/raster/wc2.1_10m_prec/wc2.1_10m_prec_01.tif names : wc2.1_10m_prec_01 values : 0, 908 (min, max) 12.2.1.1 Manipulating raster jan = raster::raster(&quot;data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_01.tif&quot;) jan %&gt;% raster::plot() jul = raster::raster(&quot;data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_07.tif&quot;) jul %&gt;% raster::plot() dec = raster::raster(&quot;data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_12.tif&quot;) dec %&gt;% raster::plot() temp.diff = jan-jul temp.diff %&gt;% raster::plot() temp.diff = dec-jul temp.diff %&gt;% raster::plot() 12.2.2 Reclassifying rasters dec.group = dec %&gt;% raster::reclassify(rcl = seq(-46,35,10)) dec.group %&gt;% raster::plot() 12.2.3 Mapping raster 12.2.3.1 Land surface temperature jan.temperature = stars::read_stars(&quot;data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_01.tif&quot;) ggplot() + stars::geom_stars(data = jan.temperature)+ coord_cartesian(expand = FALSE)+ scale_fill_gradientn(colours = oce::oce.colors9A(120), na.value = &quot;white&quot;) 12.2.3.2 Land surface wind jan.wind = stars::read_stars(&quot;data/raster/wc2.1_10m_wind/wc2.1_10m_wind_01.tif&quot;) ggplot() + stars::geom_stars(data = jan.wind)+ coord_cartesian(expand = FALSE)+ scale_fill_gradientn(colours = oce::oce.colors9A(120), na.value = &quot;white&quot;) 12.2.3.3 Land surface precipitation jan.rain = stars::read_stars(&quot;data/raster/wc2.1_10m_prec/wc2.1_10m_prec_01.tif&quot;) ggplot() + stars::geom_stars(data = jan.rain)+ coord_cartesian(expand = FALSE)+ scale_fill_gradientn(colours = oce::oce.colors9A(120), na.value = &quot;white&quot;) 12.2.3.4 Land surface evaporation jan.vapr = stars::read_stars(&quot;data/raster/wc2.1_10m_vapr/wc2.1_10m_vapr_01.tif&quot;) ggplot() + stars::geom_stars(data = jan.vapr)+ coord_cartesian(expand = FALSE)+ scale_fill_gradientn(colours = oce::oce.colors9A(120), na.value = &quot;white&quot;) 12.2.3.5 Solar radiation jan.srad = stars::read_stars(&quot;data/raster/wc2.1_10m_srad/wc2.1_10m_srad_01.tif&quot;) ggplot() + stars::geom_stars(data = jan.srad)+ coord_cartesian(expand = FALSE)+ scale_fill_gradientn(colours = oce::oce.colors9A(120), na.value = &quot;white&quot;) 12.3 Summary In this chapter, we learned how to load, manipulate, and analyze raster data. The things we have learned here will work as the cornerstone for doing complicated raster analysis tasks, such as applying machine learning algorithms, as well be discussing in later chapters. We learned how to read and load raster data using the raster() function from the raster package in R. For multispectral images, such as Landsat images, we have learned how 12.4 Questions Now that youve completed this chapter, you should be ready to answer the following questions: How do you load raster data in R? How do you change projections of rasters in R? How do you visualize raster in R? How do you reclassify rasters in R ? How do you clip rasters by vector data? How do you sample raster data using points? References "],["maps.html", "Chapter 13 Maps 13.1 Geographical data in a tidy format 13.2 Introduction 13.3 Static Maps 13.4 Basemap 13.5 Creating contour map 13.6 Inset maps 13.7 Animated maps 13.8 Interactive maps", " Chapter 13 Maps Along with all the geographical data making its way into the public domain, a variety of tools to map that data have also been developed. For a long time R base provides tools to map geographical datadata with latitude and longitude coordinates attached to it. However, mapping in the early days of R was not easy, it was not elegant either. Recently, several packages have been developed for mapping geographical data that align with the ggplot framework. So they allow us to map spatial data in similar ways as other plots with a grammar of graphic principle. For example Edzer Pebesma (2018) developed an awesome sf package for mapping vector data in R. The power of this function lies in the fact that it structure the spatial data in tidy format, allowing for easy manipulation with the tidyverse function and also for plotting with the ggplot2 flavor. The sf package allows you to read in and work with geographical data in a tidy format. 13.1 Geographical data in a tidy format The sf package allows you to create a simple featurea data frame with additional columns that hold spatial component called the geometry. This column contains the geometrical nature of the data needed to draw the data. Often the sf object has two classesthe simple feature and the data.frame classes. The data.frame holds attribute information of the dataset and the geometry contains the geographical coordinates. For example, the simple feature displayed below is a dataset of sampling stations, where each row gives the data for each station. The data.frame here holds the first four columns the id, type, depth and sst, whereas the geometry column include the geometry type, for this case the point and the embedded latitude and longitude geographical coordinates. There different ways to create simple feature in R using the sf package. We will create a few of them later that we will use for mapping examples. We need to load the packages we are going to use in this chapter. require(sf) require(tidyverse) FALSE Simple feature collection with 11 features and 4 fields FALSE geometry type: POINT FALSE dimension: XY FALSE bbox: xmin: 39.50958 ymin: -8.425115 xmax: 42.00623 ymax: -6.414011 FALSE geographic CRS: WGS 84 FALSE First 10 features: FALSE id type depth sst FALSE 1 294 marker 29 27.87999 FALSE 2 300 marker -604 27.97999 FALSE 3 306 marker -569 27.97999 FALSE 4 312 marker -485 28.03999 FALSE 5 318 marker -325 28.03999 FALSE 6 326 marker -461 28.03999 FALSE 7 414 marker -505 28.02999 FALSE 8 428 marker -132 28.23999 FALSE 9 434 marker -976 28.16999 FALSE 10 456 marker -3311 28.33999 FALSE geometry FALSE 1 POINT (39.50958 -6.438159) FALSE 2 POINT (39.6318 -6.621774) FALSE 3 POINT (39.65447 -6.746649) FALSE 4 POINT (39.62563 -6.805321) FALSE 5 POINT (39.58374 -6.833973) FALSE 6 POINT (39.66476 -6.837384) FALSE 7 POINT (39.95728 -7.843535) FALSE 8 POINT (39.67712 -8.136846) FALSE 9 POINT (39.74853 -8.425115) FALSE 10 POINT (42.00623 -7.025368) 13.1.1 Create simple feature from data.frame If you have a regular dataframe, you can convert it into a simple feature object with tools in the sf package. For instance, in the working directory we have a dataset of eleven stations named points.csv. We can simply import this dataset into R session with the read_csv() function. If we print the file, it give about the variables and rows presented in the datasete. There six variablesid, type, depth and sst along with the latitude and longitude coordinates. These stations contains measured variable of sea surface temperature and their maximum depth. stations = read_csv(&quot;./data/shp/points.csv&quot;) stations # A tibble: 11 x 6 lon lat id type depth sst &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 39.5 -6.44 294 marker 29 27.9 2 39.6 -6.62 300 marker -604 28.0 3 39.7 -6.75 306 marker -569 28.0 4 39.6 -6.81 312 marker -485 28.0 5 39.6 -6.83 318 marker -325 28.0 6 39.7 -6.84 326 marker -461 28.0 7 40.0 -7.84 414 marker -505 28.0 8 39.7 -8.14 428 marker -132 28.2 9 39.7 -8.43 434 marker -976 28.2 10 42.0 -7.03 456 marker -3311 28.3 11 41.8 -6.41 462 marker -3248 28.6 Athough this dataset contains geographical coordinates in it (latitude and longitude), its just a regular data frame. We can use the geographical coordinates in the dataframe to convert it to simple feature with the st_as_sf() function, and specify the columns with the geographical information using the coords parameter. simple_feature = stations %&gt;% sf::st_as_sf(coords = c(&quot;lon&quot;, &quot;lat&quot;)) Once we have the simple feature object, we can set the geographica coordinate system to World Geodetic System of 1984 (WGS84). I prefer using its code, which is easy to punch in instead of the whole text. If we print out the simple feature we just created, it gives the extra information at the top of the print-out, which include the number of features and columls(fields), the geometry type as point, the geographical extent (the bounding box) of and the projection both in epsg code and string. simple_feature = simple_feature %&gt;% sf::st_set_crs(4326) ggplot()+ geom_sf(data = simple_feature, aes(col = sst, size = wior::inverse_hyperbolic((sst))))+ scale_colour_gradientn(colors = oce::oce.colors9A(120)) 13.1.2 Importing shapefile When you create maps, you will often want to import shapefilea widely used format for storing geographical data in GIS. sf package offers tools to read and load shapefiles into R. Lets import africas country boundary shapefile from the working directory. We use the st_read() function from sf package to read the shapefile boundary layer. Like the simple features we created, the shapefile also display extra information at the top confirming that its no longer a shapefile but rather a simple feature. ## read africa = sf::st_read(&quot;./data/shp/africa.shp&quot;, quiet = TRUE) africa Simple feature collection with 59 features and 7 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -25.35875 ymin: -34.83983 xmax: 57.80085 ymax: 37.34962 geographic CRS: WGS 84 First 10 features: COUNT CNTRY_NAME FIPS_CNTRY 1 34 Angola AO 2 114 Burundi BY 3 77 Benin BN 4 301 Burkina Faso UV 5 25 Botswana BC 6 51 Central African Republic CT 7 51 Cameroon CM 8 186 Ivory Coast IV 9 46 Congo CF 10 15 Cape Verde CV LAND_AREA_ REGIONA EMPTY EMPTY2 1 124670 &lt;NA&gt; 0 0 2 2783 &lt;NA&gt; 0 0 3 11262 &lt;NA&gt; 0 0 4 27400 &lt;NA&gt; 0 0 5 58173 &lt;NA&gt; 0 0 6 62298 &lt;NA&gt; 0 0 7 47544 &lt;NA&gt; 0 0 8 32246 &lt;NA&gt; 0 0 9 34200 &lt;NA&gt; 0 0 10 403 &lt;NA&gt; 0 0 geometry 1 MULTIPOLYGON (((12.84118 -6... 2 MULTIPOLYGON (((29.05021 -2... 3 MULTIPOLYGON (((3.849006 10... 4 MULTIPOLYGON (((-5.272945 1... 5 MULTIPOLYGON (((23.14635 -1... 6 MULTIPOLYGON (((22.03557 4.... 7 MULTIPOLYGON (((9.640797 3.... 8 MULTIPOLYGON (((-6.091862 4... 9 MULTIPOLYGON (((16.45276 2.... 10 MULTIPOLYGON (((-24.64849 1... Since the layer is for the whole Africa, to reduce the processing time, we must reduce the geographical extent to the area of interest. We use the st_crop() function to chop the area that we want to map and discard the rest. kimbiji = africa %&gt;% sf::st_crop(xmin = 38.0, xmax = 40.5, ymin = -8, ymax = -5.5) 13.1.3 Reading other format Sometimes you have geographical data that are neither in tabular form or shapefile. In that situation, you ought to use the st_layers() function to identify, first the driver used to create the file and, second, the layer name you want to extract. Once you have identified the layer of interest, you can use the st_read() function to import the layer from the file. For example, we have the track file that was recorded with a GPS device. Lets explore the layer it contains with the `st_layers() function. tracks = sf::st_layers(&quot;data/tracks/Track-181204-075451.gpx&quot;) tracks Driver: GPX Available layers: layer_name geometry_type features fields 1 waypoints Point 1 19 2 routes Line String 0 12 3 tracks Multi Line String 1 12 4 route_points Point 0 21 5 track_points Point 1467 24 Once we print, it shows that is a GPX format with five layers name. We are only interested with the tracks and track_points layers. We can extract them with the st_read() function, by specifying the dsn and the layer. This can be written as; ## obtain track points track.points = sf::st_read(dsn =&quot;data/tracks/Track-181204-075451.gpx&quot; ,layer = &quot;track_points&quot;) Reading layer `track_points&#39; from data source `E:\\bookdown\\geoMarine\\data\\tracks\\Track-181204-075451.gpx&#39; using driver `GPX&#39; Simple feature collection with 1467 features and 24 fields geometry type: POINT dimension: XY bbox: xmin: 39.68927 ymin: -8.033337 xmax: 39.75059 ymax: -7.977127 geographic CRS: WGS 84 ## drop other variable that are not needed track.points = track.points %&gt;% select(elevation = ele, time, speed) ## display track.points Simple feature collection with 1467 features and 3 fields geometry type: POINT dimension: XY bbox: xmin: 39.68927 ymin: -8.033337 xmax: 39.75059 ymax: -7.977127 geographic CRS: WGS 84 First 10 features: elevation time speed 1 -3.7 2018-12-04 07:54:55 1.471591 2 -5.0 2018-12-04 07:54:58 1.479312 3 -5.6 2018-12-04 07:55:00 1.358867 4 -5.8 2018-12-04 07:55:02 1.530269 5 -5.9 2018-12-04 07:55:04 1.424751 6 -6.1 2018-12-04 07:55:05 1.381000 7 -6.2 2018-12-04 07:55:07 1.437619 8 -6.3 2018-12-04 07:55:09 0.994958 9 -6.3 2018-12-04 07:55:11 1.032018 10 -6.4 2018-12-04 07:55:13 1.369676 geometry 1 POINT (39.75051 -7.977127) 2 POINT (39.75052 -7.977157) 3 POINT (39.75052 -7.977185) 4 POINT (39.75052 -7.977215) 5 POINT (39.75052 -7.977243) 6 POINT (39.75052 -7.977262) 7 POINT (39.75052 -7.97729) 8 POINT (39.75051 -7.97731) 9 POINT (39.75051 -7.97733) 10 POINT (39.7505 -7.977353) track = sf::st_read(dsn =&quot;data/tracks/Track-181204-075451.gpx&quot; ,layer = &quot;tracks&quot;) Reading layer `tracks&#39; from data source `E:\\bookdown\\geoMarine\\data\\tracks\\Track-181204-075451.gpx&#39; using driver `GPX&#39; Simple feature collection with 1 feature and 12 fields geometry type: MULTILINESTRING dimension: XY bbox: xmin: 39.68927 ymin: -8.033337 xmax: 39.75059 ymax: -7.977127 geographic CRS: WGS 84 track Simple feature collection with 1 feature and 12 fields geometry type: MULTILINESTRING dimension: XY bbox: xmin: 39.68927 ymin: -8.033337 xmax: 39.75059 ymax: -7.977127 geographic CRS: WGS 84 name cmt 1 Tracking android:60fd0ef637a6eb1b &lt;NA&gt; desc src 1 Tracking recently started 12/4/18 07:54 &lt;NA&gt; link1_href link1_text link1_type link2_href 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; link2_text link2_type number type 1 &lt;NA&gt; &lt;NA&gt; 10193 &lt;NA&gt; geometry 1 MULTILINESTRING ((39.75051 ... 13.2 Introduction A satisfying and important aspect of geographic research is communicating the results in spatial context. With recent advance in technology from satellite, internet and mobile location services, the amount of geographical data has increased significantly. Plenty of data are generated daily with latitude and longitude coordinates attached to it both from satellite observation and social media. To be able to build up a good mental model of the spatial data, you need to invest considerable effort in making your maps as self-explanatory as possible. In this chapter, youll learn some of the tools that ggplot2 provides to make elegant maps and graphics (Wickham 2016). Maps are great way to understand patterns from data over space. They are scaled down versions of the physical world, and theyre everywhere. R has several systems for making graphs, but ggplot2 is one of the most elegant and most versatile. ggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. The chapter also introduce you to some extended functionalities from sf (Pebesma 2018), cowplot (Wilke 2018), ggsn (???), ggsci (???), metR (Campitelli 2019), ggrepel (Slowikowski 2020), gganimate (Pedersen and Robinson 2017) and egg (Auguie 2018) packages. Therefore, this chapter focuses on the tools you need to create good graphics. Rather than loading those extensions here, well refer to their functions explicitly, using the :: notation. This will help make it clear which functions are built into ggplot2, and which come from other packages. Ensure you have these packages in your machine, otherwise install them with install.packages() if you dont already have them. 13.3 Static Maps Static maps are the most common type of visual output from spatial objects. Fixed images for printed outputs, common formats for static maps include .png and .pdf, for raster and vector outputs, respectively. Initially static maps were the only type of map that R could produce. Things have advanced greatly since sp was released (see Bivand, Pebesma, and Gomez-Rubio (2013)). Many new techniques for map making have been developed since then. However, a decade later static plotting was still the emphasis of geographic data visualisation in R (Cheshire and Lovelace 2015). Despite the innovation of interactive mapping in R, static maps are still the foundation of mapping in R. The base plot() function is often the fastest way to create static maps from vector and raster spatial objects. Sometimes simplicity and speed are priorities, especially during the development phase of a project, and this is where plot() excels. The base R approach is also extensible, with plot() offering dozens of arguments. Another low-level approach is the grid package, which provides functions for low-level control of graphical outputs. This section, however, focus on how to make static maps with ggplot2, emphasizing the important aesthetic and layout options. 13.3.1 The bathmetry data ggplot2 works with data that are tidydata frame arranged in such way that observations are in rows and variables are in columns and each value must have its own cell. But, the bathmetry data is from ETOPO1 and came in .asc format. First read the file with the raster::raster() function. ## read the ascii file tz.bath = raster::raster(&quot;e:/GIS/ROADMAP/Etopo 1/Tanzania_etopo1/tanz1_-3432.asc&quot;) tz.bath %&gt;% class() [1] &quot;RasterLayer&quot; attr(,&quot;package&quot;) [1] &quot;raster&quot; We notice that the file is raster and ggplot2 requires data.frame. To continue, we need to convert the data and tidy in format that is **plot2readable. Specifically, we need to convert raster file into data frame with raster::as.data.frame(xy = TRUE) and specify the xy = TRUE argument. We then rename the x to lon, y to lat and convert bathmetry values from the double presion to integer and select values within the geographical extend of interest and depth between 0 and 1200 meter deep. ## convert raster to data frame tz.bath.df = tz.bath %&gt;% raster::as.data.frame(xy = TRUE) %&gt;% dplyr::as_tibble() ## rename the variable tz.bath.df = tz.bath.df %&gt;% dplyr::rename(lon = x, lat = y, depth = 3)%&gt;% dplyr::mutate(depth = as.integer(depth)) ## chop the area of interest off.kimbiji = tz.bath.df %&gt;% dplyr::filter(lon &gt; 38.5 &amp; lon &lt; 40 &amp; lat &gt; -7.2 &amp; lat &lt; - 6 &amp; depth &gt; -1200&amp; depth &lt; 0 ) The bathmetry file now contain three variables, the lon, lat and depth as seen in table 13.1 off.kimbiji %&gt;% dplyr::sample_n(10) %&gt;% knitr::kable(col.names = c(&quot;Longitude&quot;, &quot;Latitude&quot;, &quot;Depth (meters)&quot;), digits = 3, caption = &quot;Ten randomly selected points of bathmetry values off Kimbiji, Tanzania&quot;, align = &quot;c&quot;)%&gt;% kableExtra::column_spec(column = 2:3, width = &quot;3cm&quot;)%&gt;% kableExtra::add_header_above(c(&quot;Coordinate (Degree)&quot; = 2,&quot;&quot;)) Table 13.1: Ten randomly selected points of bathmetry values off Kimbiji, Tanzania Coordinate (Degree) Longitude Latitude Depth (meters) 39.917 -6.767 -198 39.417 -6.733 -266 39.583 -6.067 -413 39.617 -6.717 -560 39.833 -6.817 -96 39.967 -7.033 -513 39.017 -6.267 -3 39.967 -6.883 -112 39.817 -6.733 -232 39.383 -6.683 -294 13.4 Basemap We also need basemapcountry boundary layer. We use the st_read() function from sf package to read the shapefile boundary layer. Since the layer is for the whole Africa, to reduce the processing time for ploting the map of africa, we use the st_crop() function to chop the area of interest. africa = sf::st_read(&quot;./data/shp/africa.shp&quot;, quiet = TRUE) kimbiji = africa %&gt;% sf::st_crop(xmin = 38.0, xmax = 40.5, ymin = -8, ymax = -5.5) 13.5 Creating contour map Once we have the data ready, we can tools in ggplot2 and add-on packages to create the bathmetry map offKimbiji located between longitude 38.5°E and 40.1°E and latitude 7.2°S and `r metR::LatLabel(-6.0). The code block below was used to create figure 13.1 Figure 13.1: Map of Off-Kimbiji showing contour lines. The grey lines are contour at 50 m interval and the black line are contoured at 200 m intervals There are fourteen lined of codes in the chunk to make figure 13.1. Thats a lot! Dont get intimidated, I will explain in detail how each line of code work together to make this figure. As before, you start plotting ggplot2 with the ggplot() function as the first line. Surprisingly, the ggplot() is empty without any argument specified. When mapping with geom_sf() function in ggplot2 package, you are advised to leave the ggplot() function empty. This will allow the geom_sf() to label the axes with the appropriate geographical labelling for longitude and latitude. The second line of gode add a simple feature with a geom_sf() function from sf package. Note however, I specified the geom_sf() to fill the boundary layer with grey of 90 shade and the stroke with black colour. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;) note that ggplot2 plot the map with default aesthetic settings. The plot background is filled with gray color and without stroke but the grids are white colored. The third line add the contour lines spaced at 50 meter intervals. Instead of using geom_contour() from ggplot2, the geom_contour2() from metR package was used. They both serve the same task. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;) Like the third line, the fourth line add contour lines, but instead of spacing them into meters, these are spaced at 200 meters interval and are black in color. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200) The fifth line add the label on contour spaced at 200 meter interval with geom_text_contour() function from metR package. Here is where you will find the useful of package like metR that extends the ggplot2, for which the current version (2.3.1.1) is unable. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200)+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200, rotate = FALSE) The sixth line zoom the map to only the geographical extent we are interested with using the coord_sf() function from sf package. We could also use the coord_cartesin() to limit the area. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200)+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200, rotate = FALSE)+ coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3)) We got a glimpse of the map now, let us use theme to make some changes. The background was set to white with panel.background = element_rect(fill = \"white\"), and removed grids with panel.grid = element_line(colour = NA) and change the font size of the axis label to 11 points with axis.text = element_text(size = 11). The theme_bw() just set the border of the plot to black with solid line. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200)+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200, rotate = FALSE)+ coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))+ theme_bw()+ theme(panel.background = element_rect(fill = &quot;white&quot;), panel.grid = element_line(colour = NA), axis.text = element_text(size = 11)) The good thing to start making maps is with an understanding of the map elements. A static map can be composed of many different map elements. These include main map body, legend, title, scale indicator, orientation indicator, inset map and source or ancillary information. By increasing the font size of axis textual label to 11, the axes are cluttered. adding the scale can improve the labelling. scale_x_continuous(breaks = seq(39.2, 39.8, .2)) in line 9 force ggplot2 to label the xaxis four letter that are spaced with 0.2 latitude and scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4)) in line 10 label four digits of longitude. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200)+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200, rotate = FALSE)+ coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))+ theme_bw()+ theme(panel.background = element_rect(fill = &quot;white&quot;), panel.grid = element_line(colour = NA), axis.text = element_text(size = 11))+ scale_x_continuous(breaks = seq(39.2, 39.8, .2))+ scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4)) Because the axes are abbreviated with longitude and latitude symbol, line 11 in the code remove the axes title label. Line 12 to 14 add textual label on the map with the annotate() function. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200)+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200, rotate = FALSE)+ coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))+ theme_bw()+ theme(panel.background = element_rect(fill = &quot;white&quot;), panel.grid = element_line(colour = NA), axis.text = element_text(size = 11))+ scale_x_continuous(breaks = seq(39.2, 39.8, .2))+ scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4))+ labs(x = NULL, y = NULL)+ annotate(geom = &quot;text&quot;, x = 39.28, y = -6.48, label = &quot;Zanzibar \\nChannel&quot;)+ annotate(geom = &quot;text&quot;, x = 39.5, y = -6.37, label = &quot;Unguja \\nIsland&quot;)+ annotate(geom = &quot;text&quot;, x = 39.3, y = -6.91, label = &quot;Dar es Salaam&quot;) Close look of figure 13.1, the north arrrow and the scale bar are missing. The last two lines of our code inset the scalebar and north arrow on map using the ggsn::scalebar() from ggsn package and ggspatial::annotation_north_arrow() functions from ggspatial package. In a nutshell, making this map using ggplot2 and ancillary extensions used fiften line codes and hundred of arguments. This are very common task of making maps with the combination of tools from different packages. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200)+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200, rotate = FALSE)+ coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))+ theme_bw()+ theme(panel.background = element_rect(fill = &quot;white&quot;), panel.grid = element_line(colour = NA), axis.text = element_text(size = 11))+ scale_x_continuous(breaks = seq(39.2, 39.8, .2))+ scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4))+ labs(x = NULL, y = NULL)+ annotate(geom = &quot;text&quot;, x = 39.28, y = -6.48, label = &quot;Zanzibar \\nChannel&quot;)+ annotate(geom = &quot;text&quot;, x = 39.5, y = -6.37, label = &quot;Unguja \\nIsland&quot;)+ annotate(geom = &quot;text&quot;, x = 39.3, y = -6.91, label = &quot;Dar es Salaam&quot;)+ ggsn::scalebar(location = &quot;bottomleft&quot;, x.min = 39.2, x.max = 39.8, y.min = -6.98, y.max = -6.35, dist = 10, dist_unit = &quot;km&quot;, transform = TRUE, model = &quot;WGS84&quot;, st.dist = 0.03, st.size = 4, height = 0.03)+ # ggspatial::annotation_scale(height = unit(.35, &quot;cm&quot;), pad_x = unit(.5, &quot;cm&quot;), # tick_height = unit(3, &quot;cm&quot;), pad_y = unit(.5, &quot;cm&quot;), text_cex = .85)+ ggspatial::annotation_north_arrow(location = &quot;tr&quot;, width = unit(.75, &quot;cm&quot;), height = unit(1, &quot;cm&quot;)) metR package has geom_contour_fill() function that draw filled contour lines and geom_contour_tanaka(), which illunate contours with varying brithtness to create an illusion of relief. The code chunk to create highlighted filled contour using metR function can be written as; ggplot()+ metR::geom_contour_fill(data = off.kimbiji, aes(x = lon, y = lat, z=depth), na.fill = TRUE, show.legend = FALSE)+ metR::geom_contour_tanaka(data = off.kimbiji, aes(x = lon, y = lat, z=depth))+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), rotate = TRUE, check_overlap = TRUE, size = 3.0)+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))+ theme_bw()+ theme(panel.background = element_rect(fill = &quot;white&quot;), panel.grid = element_line(colour = NA), axis.text = element_text(size = 11))+ scale_x_continuous(breaks = seq(39.2, 39.8, .2))+ scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4))+ scale_fill_gradientn(colours = oce::oce.colorsGebco(120))+ labs(x = NULL, y = NULL)+ annotate(geom = &quot;text&quot;, x = 39.28, y = -6.48, label = &quot;Zanzibar \\nChannel&quot;)+ annotate(geom = &quot;text&quot;, x = 39.5, y = -6.37, label = &quot;Unguja \\nIsland&quot;)+ annotate(geom = &quot;text&quot;, x = 39.3, y = -6.91, label = &quot;Dar es Salaam&quot;)+ ggsn::scalebar(location = &quot;bottomleft&quot;, x.min = 39.2, x.max = 39.8, y.min = -6.98, y.max = -6.35, dist = 10, dist_unit = &quot;km&quot;, transform = TRUE, model = &quot;WGS84&quot;, st.dist = 0.03, st.size = 4, height = 0.03)+ # ggspatial::annotation_scale(location = &quot;bl&quot;)+ ggspatial::annotation_north_arrow(location = &quot;tr&quot;, width = unit(.75, &quot;cm&quot;), height = unit(1.0, &quot;cm&quot;)) 13.6 Inset maps An inset map is a smaller map rendered within or next to the main map. It could serve many different purposes, including showing the relative position of the study area in regional area. In figure ?? is the map showing the contour interval off-kimbiji, Tanzania. The inset map show the area of Kimbiji in the Western Indian Ocean Region. The chunk below was used to create figure 13.2. In a nutshell, we assign the study area map as main.map and the regional map as inset.map and then we used function from the cowplot package to combine the two maps. main.map = ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200)+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200, rotate = FALSE)+ coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))+ theme_bw()+ theme(panel.background = element_rect(fill = &quot;white&quot;), panel.grid = element_line(colour = NA), axis.text = element_text(size = 11))+ scale_x_continuous(breaks = seq(39.2, 39.8, .2))+ scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4))+ labs(x = NULL, y = NULL)+ annotate(geom = &quot;text&quot;, x = 39.28, y = -6.48, label = &quot;Zanzibar \\nChannel&quot;)+ annotate(geom = &quot;text&quot;, x = 39.5, y = -6.37, label = &quot;Unguja \\nIsland&quot;)+ annotate(geom = &quot;text&quot;, x = 39.3, y = -6.91, label = &quot;Dar es Salaam&quot;)+ ggspatial::annotation_scale(location = &quot;bl&quot;)+ ggspatial::annotation_north_arrow(location = &quot;tr&quot;) world = spData::world aoi = data.frame(lon = c(38.5, 40, 40, 38.5, 38.5), lat = c(-8, -8, -6, -6, -8)) inset.map = ggplot()+ geom_sf(data = world, fill = &quot;grey90&quot;, col = 1)+ coord_sf(xlim = c(37, 45), ylim = c(-12,-1))+ geom_path(data = aoi, aes(x = lon, y = lat), size = 1.2)+ theme_bw()+ theme(plot.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), panel.grid = element_line(colour = &quot;white&quot;)) + labs(x = NULL, y = NULL) cowplot::ggdraw()+ cowplot::draw_plot(plot = main.map, x = 0, y = 0, width = 1, height = 1, scale = 1)+ cowplot::draw_plot(plot = inset.map, x = .558, y = .05, width = .3,height = .3) Figure 13.2: The main map with the inset map showing the positon of the study areas in the region main.map = ggplot()+ metR::geom_contour_fill(data = off.kimbiji, aes(x = lon, y = lat, z=depth), na.fill = TRUE, show.legend = FALSE)+ metR::geom_contour_tanaka(data = off.kimbiji, aes(x = lon, y = lat, z=depth))+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), rotate = TRUE, check_overlap = TRUE, size = 3.4)+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))+ theme_bw()+ theme(panel.background = element_rect(fill = &quot;white&quot;), panel.grid = element_line(colour = NA), axis.text = element_text(size = 11))+ scale_x_continuous(breaks = seq(39.2, 39.8, .2))+ scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4))+ scale_fill_gradientn(colours = oce::oce.colorsGebco(120))+ labs(x = NULL, y = NULL)+ annotate(geom = &quot;text&quot;, x = 39.28, y = -6.48, label = &quot;Zanzibar \\nChannel&quot;)+ annotate(geom = &quot;text&quot;, x = 39.5, y = -6.37, label = &quot;Unguja \\nIsland&quot;)+ annotate(geom = &quot;text&quot;, x = 39.3, y = -6.91, label = &quot;Dar es Salaam&quot;)+ ggspatial::annotation_scale(location = &quot;bl&quot;)+ ggspatial::annotation_north_arrow(location = &quot;tr&quot;, width = unit(.75, &quot;cm&quot;)) world = spData::world aoi = data.frame(lon = c(38.5, 40, 40, 38.5, 38.5), lat = c(-8, -8, -6, -6, -8)) inset.map = ggplot()+ geom_sf(data = world, fill = &quot;grey90&quot;, col = 1)+ coord_sf(xlim = c(37, 45), ylim = c(-12,-1))+ geom_path(data = aoi, aes(x = lon, y = lat), size = 1.2)+ theme_bw()+ theme(plot.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), panel.grid = element_line(colour = &quot;white&quot;)) + labs(x = NULL, y = NULL) cowplot::ggdraw()+ cowplot::draw_plot(plot = main.map, x = 0, y = 0, width = 1, height = 1, scale = 1)+ cowplot::draw_plot(plot = inset.map, x = .558, y = .05, width = .3,height = .3) Figure 13.3: The main map with the inset map showing the positon of the study areas in the region 13.6.1 Choropleth maps Chloropleth maps use color or shading on predefined areas to map values of a numeric or categorical variable in that area. For example we are interested to map the different coral reefs in Jibondo Island, Mafia, Tanzania. First we import the data into R using the st_read() function from sf package (Pebesma 2018) jibondo.reefs = sf::st_read(dsn = &quot;./data/shp/jibondo_reefs.shp&quot;,quiet = TRUE) The jibondo.reefs file is simple feature (equivalent to shapefile) with sixteeen polygons in four groupscoastal-shallow areas, islands, reef flat and submerged reefs. We use the variable type to map the different coastal features in Jibondo. The code used to make figure 13.4 is written as: require(RColorBrewer) ggplot()+ geom_sf(data = jibondo.reefs, aes(fill = type)) + coord_sf(xlim = c(39.57, 39.88), ylim = c(-8.15,-7.88)) + geom_sf_text(data = jibondo.reefs, aes(label = mwamba), check_overlap = TRUE) + theme_bw() %+% theme(axis.text = element_text(size = 11), legend.position = c(.8,.18)) + scale_fill_brewer(palette = &quot;Accent&quot;) + metR::scale_x_longitude(ticks = 0.1) + metR::scale_y_latitude(ticks = 0.08) + guides(fill = guide_legend(title = &quot;Reef Type&quot;, title.position = &quot;top&quot;, keywidth = 1.1, ncol = 1)) Figure 13.4: Reefs and non-reeef features in Jibondo Island, Mafia The variable used to make Figure 13.4 is a categorical, but we can also map continuous variables. For this case, we want to map the catch per unit effort (CPUE) of octopus at each reef to identify octopus catches at different reefs as seen in figure require(RColorBrewer) ggplot()+ geom_sf(data = jibondo.reefs, aes(fill = cpue %&gt;%round(2) %&gt;% as.factor()))+ coord_sf(xlim = c(39.57, 39.88), ylim = c(-8.15,-7.88))+ geom_sf_text(data = jibondo.reefs, aes(label = mwamba), check_overlap = TRUE) + theme_bw() %+% theme(axis.text = element_text(size = 11), legend.position = c(.9,.25)) + # scale_fill_brewer(palette = &quot;Accent&quot;) + ggsci::scale_fill_d3()+ metR::scale_x_longitude(ticks = 0.1) + metR::scale_y_latitude(ticks = 0.08)+ guides(fill = guide_legend(title.position = &quot;top&quot;, keywidth = 1.1, ncol = 1, title = &quot;CPUE&quot;)) Figure 13.5: Catches of Octopus at Jibondo Island Finally lets map the the spatial patterns of sea surface temperature anomaly. We plot the departure of sea surface temperature from zonal average mean. Lets import the dataset from the workspace. For more information on how to compute the zonal departure see chapter temperature.anomaly = read_csv(&quot;./data/shp/sst_anomaly.csv&quot;) Coordinate (Degree) Latitude Longitude Depth Anomaly 85.5 104.5 0 -0.02 80.5 136.5 0 -0.06 -65.5 64.5 0 -0.43 -35.5 54.5 0 0.60 25.5 -49.5 0 0.81 -10.5 10.5 0 -2.02 -10.5 -20.5 0 -1.00 -50.5 -172.5 0 3.94 -35.5 -84.5 0 -0.75 10.5 -51.5 0 -0.29 ggplot() + metR::geom_contour_fill(data = temperature.anomaly, aes(x = lon, y = lat, z = anomaly), na.fill = T) + geom_sf(data = spData::world, col = NA, fill = &quot;grey40&quot;)+ coord_sf(xlim = c(-180,178), ylim = c(-90,85), clip = &quot;on&quot;, expand = FALSE)+ scale_fill_gradientn(colours = oce::oce.colors9A(120), breaks = seq(-6,6,2))+ theme_bw() + theme(legend.position = &quot;right&quot;, panel.background = element_blank(), axis.text = element_text(size = 11, colour = &quot;black&quot;), legend.text = element_text(size = 10), legend.title = element_text(size = 11))+ guides(fill = guide_colorbar(title = expression(Temperature~anomaly~(degree*C)), title.position = &quot;right&quot;, title.hjust = 0.5, title.theme = element_text(angle = 90), label.theme = element_text(size = 10), direction = &quot;vertical&quot;, reverse = FALSE, raster = FALSE, barwidth = unit(.4, &quot;cm&quot;), barheight = unit(6.5, &quot;cm&quot;)))+ metR::scale_y_latitude(ticks = 30) + metR::scale_x_longitude(ticks = 45)+ labs(x = NULL, y = NULL) Figure 13.6: Departure of the sea surfae temprature at each location from the zonally averaged field 13.7 Animated maps raster.files = list.files(&quot;data/raster/wc2.1_10m_tavg/&quot;, pattern = &quot;.tif&quot;, full.names = TRUE) raster.files = raster.files[-c(2:3)] months = c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;) climatology.temperature = list() for (i in 1:length(raster.files)){ climatology.temperature[[i]] = raster.files[i] %&gt;% raster::raster() %&gt;% # wior::raster_tb() %&gt;% raster::as.data.frame(xy = TRUE) %&gt;% rename(lon = 1, lat = 2, z = 3) %&gt;% mutate(month = months[i]) } climatology.temperature = climatology.temperature %&gt;% bind_rows() require(gganimate) temperature.animate = ggplot()+ geom_raster(data = climatology.temperature , aes(x = lon, y = lat, fill = z))+ scale_fill_gradientn(colours = oce::oce.colors9A(120))+ theme_void()+ annotate(geom = &quot;text&quot;, x = -150, y = 0, label = month, check_overlap = TRUE, size=10, fontface=&quot;bold&quot;) + transition_states(states = month, transition_length = 1, state_length = 20) # save as gif mapGIF &lt;- animate(temperature.animate) 13.8 Interactive maps require(leaflet) ## read the file jan.raster = raster.files[1] %&gt;% raster::raster()%&gt;% raster::projectRaster(crs = 4326) ## create color pallet pal = colorNumeric(c(&quot;#7f007f&quot;, &quot;#0000ff&quot;, &quot;#007fff&quot;, &quot;#00ffff&quot;, &quot;#00bf00&quot;, &quot;#7fdf00&quot;, &quot;#ffff00&quot;, &quot;#ff7f00&quot;, &quot;#ff3f00&quot;, &quot;#ff0000&quot;, &quot;#bf0000&quot;), raster::values(jan.raster), na.color = &quot;transparent&quot;) ## interactive map of temperature leaflet() %&gt;% addTiles() %&gt;% addRasterImage(x = jan.raster, colors = pal, opacity = .8) %&gt;% addLegend(pal = pal, values = raster::values(jan.raster), title = &quot;Temperature&quot;) Figure 13.7: Interactive map showin spatial variation of global land surface temperature for January ## read the file aug.raster = raster.files[8] %&gt;% raster::raster() %&gt;% raster::projectRaster(crs = 4326) aug.raster[aug.raster&lt; -15] = NA ## create color pallet pal = colorNumeric(c(&quot;#7f007f&quot;, &quot;#0000ff&quot;, &quot;#007fff&quot;, &quot;#00ffff&quot;, &quot;#00bf00&quot;, &quot;#7fdf00&quot;, &quot;#ffff00&quot;, &quot;#ff7f00&quot;, &quot;#ff3f00&quot;, &quot;#ff0000&quot;, &quot;#bf0000&quot;), raster::values(aug.raster), na.color = &quot;transparent&quot;) ## interactive map of temperature leaflet() %&gt;% addTiles() %&gt;% addRasterImage(x = aug.raster, colors = pal, opacity = .6) %&gt;% addLegend(pal = pal, values = raster::values(aug.raster), title = &quot;Temperature&quot;) Figure 13.8: Interactive map showing a spatial variation of global land surface temperature for August References "],["c18.html", "Chapter 14 Additional Resources 14.1 Chapter Overview 14.2 Data Science Courses 14.3 Workshop Materials 14.4 Data Visualization 14.5 Books Related to Data Science in Education 14.6 Articles Related to Data Science in Education 14.7 Equity Resources 14.8 Programming with R 14.9 Statistics 14.10 R packages and Statistical Software Development 14.11 A Career in Data Science 14.12 Places to Share Your Work 14.13 Cheat Sheets", " Chapter 14 Additional Resources 14.1 Chapter Overview In this chapter, we provide links and references to additional, recommended resources relevant to data science in education. 14.2 Data Science Courses Anderson, D. J. (2019). University of Oregon Data Science Specialization for the College of Education. https://github.com/uo-datasci-specialization A series of courses that emphasize the use of R on data science in education (graduate-level). Landers, R. N. (2019). Data science for social scientists. http://datascience.tntlab.org/ A data science course for social scientists. R Studio. (2019). Data Science in a Box. https://datasciencebox.org/hello/ A complete course, including a curriculum and teaching materials, for data science. 14.3 Workshop Materials Staudt Willet, B., Greenhalgh, S., &amp; Rosenberg, J. M. (2019, October). Workshop on using R at the Association for Educational Communications and Technology. https://github.com/bretsw/aect19-workshop Contains slides and code for a workshop carried out at an educational research conference, focused on how R can be used to analyze Internet (and social media) data. Anderson, D. J., and Rosenberg, J. M. (2019, April). Transparent and reproducible research with R. Workshop carried out at the Annual Meeting of the American Educational Research Association, Toronto, Canada. https://github.com/ResearchTransparency/rr_aera19 Slides and code for another workshop carried out at an educational research conference, focused on reproducible research and R Markdown. 14.4 Data Visualization Tufte, E. (2006). Beautiful evidence. Cheshire, CT: Graphics Press LLC. https://www.edwardtufte.com/tufte/books_be A classic text on data visualization. Healy, K. (2018). Data visualization: A practical introduction. Princeton, NJ: Princeton University Press. http://socviz.co/ A programming- (and R-) based introduction to data visualization. Chang, W. (2013). R graphics cookbook. Sebastopol, CA: OReilly. https://r-graphics.org/ This book is a great reference and how-to for executing many visualization techniques using {ggplot2}. Wilke, C. (2019). Fundamentals of data visualization. OReilly. https://serialmentor.com/dataviz/ A fantastic (though more conceptual than practical, i.e., there is no R code or other software implementation ror creating the plots) introduction to data visualization. 14.5 Books Related to Data Science in Education Geller, W., Cratty, D., &amp; Knowles, J. (2020). Education data done right: Lessons from the trenches of applied data science. Leanpub. https://leanpub.com/eddatadoneright This book explores best practices in education data work. It includes chapters on data governance, working with IT, and managing data requests. This book will help you apply your data science skills effectively in an education system. Krumm, A., Means, B., &amp; Bienkowski, M. (2018). Learning analytics goes to school: A collaborative approach to improving education. Routledge. https://www.routledge.com/Learning-Analytics-Goes-to-School-A-Collaborative-Approach-to-Improving/Krumm-Means-Bienkowski/p/book/9781315650722 This book emphasizes data-driven improvement using new sources of data and learning analytics and data mining techniques. 14.6 Articles Related to Data Science in Education Dutt, A., Ismail, M. A., &amp; Herawan, T. (2017). A systematic review on educational data mining. IEEE Access, 5, 15991-16005. https://ieeexplore.ieee.org/abstract/document/7820050 A comprehensive review of past research on educational data mining, with an emphasis on methods used in past research. Lee, V. R., &amp; Wilkerson, M. (2018). Data use by middle and secondary students in the digital age: A status report and future prospects. Commissioned Paper for the National Academies of Sciences, Engineering, and Medicine, Board on Science Education, Committee on Science Investigations and Engineering Design for Grades 6-12. Washington, D.C. https://digitalcommons.usu.edu/itls_facpub/634/ A comprehensive and incisive review of both recent and foundational research on what is known about how learners at the K-12 level analyze data. Lehrer, R. &amp; Schauble, L. (2015). Developing scientific thinking. In L. S. Liben &amp; U. Müller (Eds.), Cognitive processes. Handbook of child psychology and developmental science (Vol. 2, 7th ed., pp. 671-174). Hoboken, NJ: Wiley. https://www.wiley.com/en-us/Handbook+of+Child+Psychology+and+Developmental+Science%2C+7th+Edition-p-9781118136850 Describes the data modeling approach which has been used to support learners at the K-12 level to develop data analysis-related capabilities. Rosenberg, J. M., Edwards, A., &amp; Chen, B. (2020). Getting messy with data: Tools and strategies to help students analyze and interpret complex data sources. The Science Teacher, 87(5). https://search.proquest.com/openview/efbd11290f17b5dd9ff27c9c491ca25b/1?pq-origsite=gscholar&amp;cbl=40590 An overview of digital tools (including R) and strategies for teaching data analysis to K-12 students (particularly in science education settings). Rosenberg, J. M., Lawson, M. A., Anderson, D. J., Jones, R. S., &amp; Rutherford, T. (2020). Making data science count in and for education. In E. Romero-Hall (Ed.), Research Methods in Learning Design &amp; Technology. Routledge. https://edarxiv.org/hc2dw/ Defines data science in education (as the use of data science methods) and data science for education (as a context for teaching and learning). Schneider, B., Reilly, J., &amp; Radu, I. (2020). Lowering barriers for accessing sensor data in education: Lessons learned from teaching multimodal learning analytics to educators. Journal for STEM Education Research, 1-34. https://link.springer.com/article/10.1007/s41979-020-00027-x A study of the effects of a course designed to teach graduate students in educational graduate programs to analyze data using learning analytics techniques. Wise, A. F. (2020). Educating data scientists and data literate citizens for a new generation of data. Journal of the Learning Sciences, 29(1), 165-181. https://doi.org/10.1080/10508406.2019.1705678 A description of some of the opportunities and challenges of learning to analyze data in light of new data sources and data analysis (and data science) techniques. Wilkerson, M. H., &amp; Polman, J. L. (2020). Situating data science: Exploring how relationships to data shape learning. Journal of the Learning Sciences, 29(1), 1-10. https://doi.org/10.1080/10508406.2019.1705664 An introduction to a special issue of the Journal of the Learning Sciences on data science education. 14.7 Equity Resources ONeil, Cathy. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. First edition. New York: Crown, 2016. We All Count: https://weallcount.com/ Data for Black Lives: http://d4bl.org/ 14.8 Programming with R Wickham, H. &amp; Grolemund, G. (2017). R for data science. OReilly. You have data but have no idea on how to make sense off it?. If this statement resonates to you, then look no further. Introducing R for data analysis. At its core, R is a statistical programming language. It helps to derive useful information from the data deluge. This book assumes your a novice at data analytics and will subtly introduce you to the nuances of R, RStudio, and the tidyverse (which is a collection of R packages designed to ensure your learning curve is minimal). Teetor, P. (2011). R cookbook. Sebastopol, CA: OReilly. This book provides over 200 practical solutions for analyzing data using R. Bryan, J. &amp; Hestor, J. Happy git and github for the useR. Retrieved from https://happygitwithr.com A fantastic and accessible introduction to using git and GitHub. 14.9 Statistics 14.9.1 Introductory Statistics Open Intro. (2019). Textbooks. https://www.openintro.org/ Three open-source textbooks for statistics, one for high school students. Navarro, D. (2019). Learning Statistics With R. https://learningstatisticswithr.com/ An introductory textbook with a focus on applications to psychological research. Field, A., Miles, J., &amp; Field, Z. (2012). Discovering statistics using R. Sage publications. Emphasizes many of the most common statistical tests, especially those used in psychology and educational psychology. Covers the foundations thoroughly and in an entertaining way. Ismay, C., &amp; Kim, A. Y. (2019). ModernDive: Statistical inference via data science. CRC Press. https://moderndive.com/ An introductory statistics textbook with an emphasis on developing an intuition for the processes underlying modeling data (and hypothesis testing). James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2015). An introduction to statistical learning with applications in R. Springer. This is an introductory (and R-based) version of a classic book on machine learning by (???). Peng, R. D. (2019). R programming for data science. Leanpub. https://leanpub.com/rprogramming Emphasizes R as a programming language and writing R functions and packages. Peng, R. D., &amp; Matsui, E. (2018). The art of data science. Leanpub. https://leanpub.com/artofdatascience This book is a wonderful teaching tool and reference for R users. It describes underlying concepts of R as a programming language and provides practical guides for commonly-used functions. 14.9.2 Advanced Statistics Gelman, A., &amp; Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press. A fantastic introduction not only to regression (and multi-level/hierarchical linear models, as well as Bayesian methods), but also to statistical analysis in general. Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction. Springer Science &amp; Business Media. A classic text on machine learning. West, B. T., Welch, K. B., &amp; Galecki, A. T. (2014). Linear mixed models: a practical guide using statistical software. Chapman and Hall/CRC. A solid introduction to multi-level/hierarchical linear models, including code in R (with an emphasis on the lme4 R package). McElreath, R. (2018). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC. A new classic, accessible introduction to Bayesian methods. We note that this book has been translated into tidyverse code by (???). 14.10 R packages and Statistical Software Development Peng, R. D. (2019). Mastering software development in R. Leanpub. https://leanpub.com/msdr Developing packages in R, including a description of an example package for data visualization. Wickham, H. (2015). R packages: Organize, test, document, and share your code. OReilly. http://r-pkgs.had.co.nz/ A comprehensive introduction to (and walkthrough for) creating your own R packages. 14.11 A Career in Data Science Robinson, E., &amp; Nolis, J. (2020). Building a career in data science. Manning. https://www.manning.com/books/build-a-career-in-data-science?a_aid=buildcareer&amp;a_bid=76784b6a Advice on the technical and practical requirements to work in a data science role. 14.12 Places to Share Your Work Twitter: twitter.com Especially through the hashtags we mentioned below. LinkedIn: linkedin.com Can be a place not only to share career updates, but also data science-related works-in-progress. 14.13 Cheat Sheets R Studio Cheat Sheets (https://rstudio.com/resources/cheatsheets/) See especially the {dplyr}, {tidyr}, {purrr}, {ggplot2}, and other cheat sheets. "],["c20.html", "Chapter 15 Appendices 15.1 Practical A: Getting started 15.2 Practical B: Working with climate data 15.3 Practical C: Handling Projections and Coordinate Systems 15.4 Practical D: Finding, loading, and cleaning Spatial data 15.5 Practical E: Making maps with ggplot2 15.6 Practical F: The bathmetry data 15.7 Practical G: Drawing maps programmatically with R 15.8 Colophon", " Chapter 15 Appendices This chapter includes four practicals: Practical A: Getting started (associated with Chapter 6) Practical B: Working with climate data (associated with Chapter 11 and Chapter 12) Practical C: Finding, loading, and cleaning Spatial data (associated with Chapter 12) Practical D: Making maps with ggplot2 15.1 Practical A: Getting started Before we start, make sure that you have opened RStudio and ArcGIS on your computer. 15.1.1 Install necessary packages The most important packages We will be using for the GIS work are sf, raster, lubridate, oce, ocedata, leaflet, tmap, and tidyverse. To install these and others that we will be using, enter at the R console prompt (or copy-and-paste): install.packages( c(&quot;sf&quot;, &quot;raster&quot;, &quot;leaflet&quot;, &quot;tidyverse&quot;, &quot;kableExtra&quot;, &quot;leaflet&quot;, &quot;oce&quot;, &quot;pander&quot;, &quot;knitr&quot;, &quot;kableExtra&quot;, &quot;tidycensus&quot;, &quot;ocedata&quot;, &quot;forcats&quot;, &quot;mapedit&quot;, &quot;mapview&quot;) ) This should only done once on any user R installation. 15.1.2 Create An Rstudio Project Create a new RStudio project in a new folder on your desktop named r_gis (File &gt; New Project). Then choose New Project Then Define the directory name and browse to specify the location you want folder of the project to live in your machine. 15.1.3 Create an R Markdown file Steps to create a new R Markdown file are as File &gt; New File &gt; R Markdown...and assign the name as `GIS in R and specify the author name with your name and leave other option as default. We will come to the other parameters later. Delete most of the content in the rmarkdwon file you just created except the YAML header and the first chunk of setup and save it in your project working directory as r_gis.Rmd. 15.1.4 Create New folders We can organize our project by having a folder structure that allows us to organize our data, the scripts and output during the process. We can simply create the folder programmatically using a dir.create function, which simply create a folder in your working directory. dir.create(&quot;data&quot;) dir.create(&quot;output&quot;) dir.create(&quot;script&quot;) Create a new chunk and load the packages we will use using the require() function. We will load some packages we are going to use during this session as the chunk below highlight. require(tidyverse) require(leaflet) require(sf) require(tmap) 15.2 Practical B: Working with climate data This practical is composed of three parts. To start with were going to load some global raster data into R. In the second part we extract data points (cities and towns) from this data and generate some descriptive statistics and histograms. In the final section we explore interpolation using point data. 15.2.1 Part 1: rasters So far weve only really considered vector data. Within this practical we will explore some raster data sources and processing techniques. If you recall rasters are grids of cell with individual values. There are many, many possible sources to obtain raster data from as it is the data type used for the majority (basically all) of remote sensing data. 15.2.1.1 WorldClim data We will deal with historical climate data accessed from the worldClim Data website at this link: https://worldclim.org/data/worldclim21.html. This is version 2.1 climate data for 1970-2000 released in January 2020. There are monthly climate data for minimum, mean, and maximum temperature, precipitation, solar radiation, wind speed, water vapor pressure, and for total precipitation. There are also 19 bioclimatic variables. The data is available at the four spatial resolutions, between 30 seconds (~1 km2) to 10 minutes (~340 km2). Each download is a zip file containing 12 GeoTiff (.tif) files, one for each month of the year (January is 1; December is 12). Download the data from: https://worldclim.org/data/worldclim21.html Select any variable you want at the 5 minute second resolution3. Unzip and move the data to your project folder. Now load the data. We could do this individually. require(tidyverse) require(sf) jan = raster::raster(&quot;data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_01.tif&quot;) Then have a guick look jan %&gt;% raster::plot() jan.tb = jan %&gt;% raster::as.data.frame(xy = TRUE) %&gt;% as_tibble() %&gt;% rename(lon = 1, lat = 2, temperature = 3) jan.tb %&gt;% ggplot() + metR::geom_contour_fill(aes(x = lon, y = lat, z = temperature)) + scale_fill_gradientn(colors = oce::oce.colors9A(120), breaks = seq(-40,40,10), label = seq(-40,40,10), guide = guide_colorbar(title = expression(Average~temperature~(degree*C)), title.theme = element_text(angle = 90), title.hjust = 0.5, title.position = &quot;right&quot;,barheight = unit(8, &quot;cm&quot;))) 1. A better and more efficient way is to firstly list all the files stored within our directory files = list.files(path = &quot;data/raster/wc2.1_10m_tavg/&quot;, pattern = &quot;.tif&quot;, full.names = TRUE) We notice that since we opened the file in ArcGIS and allowed the file to create an overview, it simply created other two temporaly files. We need to remove them from our list files.clean = files[-c(2,3)] files.clean [1] &quot;data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_01.tif&quot; [2] &quot;data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_02.tif&quot; [3] &quot;data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_03.tif&quot; [4] &quot;data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_04.tif&quot; [5] &quot;data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_05.tif&quot; [6] &quot;data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_06.tif&quot; [7] &quot;data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_07.tif&quot; [8] &quot;data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_08.tif&quot; [9] &quot;data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_09.tif&quot; [10] &quot;data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_10.tif&quot; [11] &quot;data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_11.tif&quot; [12] &quot;data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_12.tif&quot; Once we have the files in the the list, Then load all of the data straight into a raster stack. A raster stack is a collection of raster layers with the same spatial extent and resolution. temperature.stacked = files.clean %&gt;% raster::stack() temperature.stacked class : RasterStack dimensions : 1080, 2160, 2332800, 12 (nrow, ncol, ncell, nlayers) resolution : 0.1666667, 0.1666667 (x, y) extent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax) crs : +proj=longlat +datum=WGS84 +no_defs names : wc2.1_10m_tavg_01, wc2.1_10m_tavg_02, wc2.1_10m_tavg_03, wc2.1_10m_tavg_04, wc2.1_10m_tavg_05, wc2.1_10m_tavg_06, wc2.1_10m_tavg_07, wc2.1_10m_tavg_08, wc2.1_10m_tavg_09, wc2.1_10m_tavg_10, wc2.1_10m_tavg_11, wc2.1_10m_tavg_12 min values : -45.88400, -44.80000, -57.92575, -64.19250, -64.81150, -64.35825, -68.46075, -66.52250, -64.56325, -55.90000, -43.43475, -45.32700 max values : 34.00950, 32.82425, 32.90950, 34.19375, 36.25325, 38.35550, 39.54950, 38.43275, 35.79000, 32.65125, 32.78800, 32.82525 In the raster stack youll notice that under dimensions there are 12 layers (nlayers). The stack has loaded the 12 months of average temperature data for us in order. To access single layers within the stack: temperature.stacked[[1]] class : RasterLayer dimensions : 1080, 2160, 2332800 (nrow, ncol, ncell) resolution : 0.1666667, 0.1666667 (x, y) extent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax) crs : +proj=longlat +datum=WGS84 +no_defs source : E:/bookdown/geoMarine/data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_01.tif names : wc2.1_10m_tavg_01 values : -45.884, 34.0095 (min, max) We can also rename our layers within the stack: month = seq(lubridate::dmy(010120), lubridate::dmy(311220), by = &quot;month&quot;) %&gt;% lubridate::month(abbr = TRUE, label = TRUE) names(temperature.stacked) = month Now to get data for just January use our new layer name temperature.stacked$Jan class : RasterLayer dimensions : 1080, 2160, 2332800 (nrow, ncol, ncell) resolution : 0.1666667, 0.1666667 (x, y) extent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax) crs : +proj=longlat +datum=WGS84 +no_defs source : E:/bookdown/geoMarine/data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_01.tif names : Jan values : -45.884, 34.0095 (min, max) temperature.stacked[[1]] class : RasterLayer dimensions : 1080, 2160, 2332800 (nrow, ncol, ncell) resolution : 0.1666667, 0.1666667 (x, y) extent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax) crs : +proj=longlat +datum=WGS84 +no_defs source : E:/bookdown/geoMarine/data/raster/wc2.1_10m_tavg/wc2.1_10m_tavg_01.tif names : Jan values : -45.884, 34.0095 (min, max) 15.2.1.2 Extract data from a raster Using a raster stack we can extract data with a single command!! For example lets make a dataframe of some sample sites  Australian cities/towns. ## create geographical location of the places in the world locations = data.frame(name = c(&quot;Australia&quot;,&quot;Africa&quot;, &quot;South America&quot;, &quot;North America&quot;, &quot;Europe&quot;, &quot;China&quot;, &quot;Russia&quot;), lon = c(129.567317, 26.916542, -58.63894, -102.5173197,9.4532431,89.285466, 91.926973), lat = c(-23.803969, -2.168066, -18.84251, 47.14847, 49.107177, 38.103829, 63.275941)) ## Extract the data from the Rasterstack for all points and convert them to data frmae temperature.locations = temperature.stacked %&gt;% raster::extract(locations %&gt;% select(lon,lat)) %&gt;% as.data.frame() %&gt;% as_tibble() monthly.temperature.locations = locations %&gt;% bind_cols(temperature.locations) monthly.temperature.locations %&gt;% mutate(across(is.numeric, round, 2)) %&gt;% knitr::kable() %&gt;% kableExtra::column_spec(column = 1:15,width = &quot;4cm&quot;) %&gt;% kableExtra::add_header_above(c(&quot;&quot;, &quot;Location&quot; = 2, &quot;Month&quot; = 12)) Location Month name lon lat Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec Australia 129.57 -23.80 32.02 30.66 28.60 24.83 19.49 15.24 14.55 16.64 21.23 25.78 28.26 29.91 Africa 26.92 -2.17 23.85 23.92 24.02 24.07 23.88 23.22 23.06 23.56 23.78 23.70 23.51 23.59 South America -58.64 -18.84 28.73 28.15 27.97 26.66 23.94 22.91 22.92 25.01 26.85 28.57 28.65 28.74 North America -102.52 47.15 -11.15 -7.06 -1.36 6.05 12.99 18.02 21.13 20.85 14.45 7.32 -2.17 -8.57 Europe 9.45 49.11 0.38 1.18 4.73 7.93 12.74 15.58 17.69 17.40 13.64 8.96 4.03 1.46 China 89.29 38.10 -15.34 -12.36 -7.24 -0.74 4.57 8.56 11.16 10.62 5.62 -2.10 -9.24 -13.86 Russia 91.93 63.28 -29.19 -26.59 -16.59 -7.17 1.26 10.52 15.27 11.37 4.40 -6.89 -19.32 -26.05 world_countries = st_read(&quot;data/shp/world_countries.shp&quot;, quiet = TRUE) world_countries = world_countries %&gt;% mutate(area = st_area(geometry) %&gt;% as.numeric, area_km = area/1000000) We then dissolve the countries for each continent to remain with continent alone; continent = st_read(&quot;data/shp/Continents.shp&quot;, quiet = TRUE) %&gt;% st_set_crs(4326) %&gt;% janitor::clean_names() continent = continent %&gt;% arrange(desc(area)) %&gt;% slice(1:7) ggplot() + ggspatial::layer_spatial(data = continent) continent.names = continent %&gt;% pull(placename) %&gt;% as.character() sample.points = list() for (i in 2:length(continent.names)){ sample.points[[i]] = continent %&gt;% filter(placename == continent.names[i]) %&gt;% st_sample(size = 30, quiet = TRUE) %&gt;% st_coordinates() %&gt;% as_tibble() %&gt;% mutate(continent = continent.names[i]) %&gt;% rename(lon = 1, lat = 2) # print(i) } sample.points = sample.points %&gt;% bind_rows() sample.points %&gt;% ggplot(aes(x = lon, y = lat, color = continent))+ ggspatial::layer_spatial(data = continent)+ geom_point() You notice that there are not points in the Antarctica, we never processed them because of high computation time it take to process because of the shape of this continent. 15.2.1.3 Extract Raster Pixels Values Using Vector Polygons Often we want to extract values from a raster layer for particular locations - for example, plot locations that we are sampling on the ground. We can extract all pixel values within 20m of our x,y point of interest. These can then be summarized into some value of interest (e.g. mean, maximum, total). Once we have the points, we can use them to extract the monthly temperature for each continent. To do this in R, we use the extract() function. The extract() function requires: The raster that we wish to extract values from, The vector layer containing the polygons that we wish to use as a boundary or boundaries, we can tell it to store the output values in a data frame using df = TRUE. (This is optional, the default is to return a list, NOT a data frame.) . We will begin by extracting all canopy height pixel values located within our aoi_boundary_HARV polygon which surrounds the tower located at the NEON Harvard Forest field site. temperature.extract = temperature.stacked %&gt;% raster::extract( sample.points %&gt;% select(-continent)) %&gt;% as.data.frame() %&gt;% as_tibble() temperature.extract.wide = sample.points %&gt;% bind_cols(temperature.extract %&gt;% mutate(across(is.numeric, round, 2))) %&gt;% relocate(continent, .before = lon) 15.2.2 Part 2 descriptive statistics Descriptive (or summary) statistics provide a summary of our data, often forming the base of quantitiatve analysis leading to inferential statistics which we use to make infereces about our data (e.g. judegements of the probability that the observed difference between two datasets is not by chance) 15.2.2.1 Data preparation Before we explore the data, we need first to tidy our dataset into a format that is easy for program to analyse and plotting. We need to change the data from the wide form to long form that is tidy. temperature.extract.long = temperature.extract.wide %&gt;% pivot_longer(cols = 4:15, names_to = &quot;month&quot;, values_to = &quot;temperature&quot;) 15.2.2.2 Histogram A histogram lets us see the frequency of distribution of our data. Make a histogram of Perths temperature temperature.extract.long %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% ggplot(aes(x = temperature)) + geom_histogram() Remember what were looking at here. The x axis is the temperature and the y is the frequency of occurrence. temperature.extract.long %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% ggplot(aes(x = temperature)) + geom_histogram()+ facet_wrap(~month)+ geom_vline(xintercept = 0, linetype = 3, color = &quot;red&quot;) 15.2.2.3 Boxplot temperature.extract.long %&gt;% filter(continent == &quot;Europe&quot;) %&gt;% ggplot() + geom_boxplot(aes(x = month,y = temperature, fill = temperature)) + scale_x_discrete(limits =month) temperature.extract.long %&gt;% filter(continent == &quot;Europe&quot; &amp; month %in% c(&quot;Jan&quot;, &quot;Jul&quot;)) %&gt;% ggplot()+ geom_histogram(aes(x = temperature, fill = month, color = month), alpha = .6, position = &quot;identity&quot;, bins = 12) 15.2.3 Using more data In the previous section, we simply extracted thirty points for each continent. Sometimes, you may be required to have more samples and you may be interested to extract at the continental level tanzania = world_countries %&gt;% filter(name_long == &quot;Tanzania&quot;) ggplot() + geom_sf(data = tanzania) #### Crop We often work with spatial layers that have different spatial extents. The spatial extent of a shapefile or R spatial object represents the geographic edge or location that is the furthest north, south east and west. Thus is represents the overall geographic coverage of the spatial object. We can use the crop() function to crop a raster to the extent of another spatial object. To do this, we need to specify the raster to be cropped and the spatial object that will be used to crop the raster. R will use the extent of the spatial object as the cropping boundary. temperature.tz.crop = temperature.stacked %&gt;% raster::crop(tanzania) temperature.tz.crop[[1]] %&gt;% raster::plot() 15.2.3.1 Mask raster::crop() always output the extracted areas as rectangular, however, when you need a raster that has the same geometry of the shapefile, then you need to use raster::mask() function. Unfortunately, the mask trail with NA values that is assigned to the cells outside the geographical extent of the shapefile. To resolve this, you run raster::mask followed with raster::crop to remove NA. temperature.tz.mask = temperature.stacked %&gt;% raster::mask(tanzania) %&gt;% raster::crop(tanzania) temperature.tz.mask[[1]] %&gt;% raster::plot() 15.2.4 Wind wind.jan = stars::read_stars(&quot;data/raster/wc2.1_10m_wind/wc2.1_10m_wind_01.tif&quot;) ggplot() + stars::geom_stars(data = wind.jan)+ scale_fill_gradientn(colors = oce::oce.colorsPalette(120), trans = scales::log10_trans(),na.value = &quot;white&quot;, # breaks = seq(-40,40,10), label = seq(-40,40,10), guide = guide_colorbar(title = expression(Wind~Speed~(ms^(-1))), title.theme = element_text(angle = 90), title.hjust = 0.5, title.position = &quot;right&quot;,barheight = unit(8, &quot;cm&quot;)))+ coord_cartesian(expand = FALSE)+ theme(panel.background = element_blank()) 15.2.5 Precipitation rain.jan = stars::read_stars(&quot;data/raster/wc2.1_10m_prec/wc2.1_10m_prec_01.tif&quot;) # rain.jan = rain.jan %&gt;% wior::inverse_hyperbolic() ggplot() + stars::geom_stars(data = rain.jan)+ scale_fill_gradientn(colors = oce::oce.colorsPalette(120), trans = scales::log10_trans(), na.value = &quot;white&quot;, # breaks = seq(-40,40,10), label = seq(-40,40,10), guide = guide_colorbar(title = expression(Precipitation(ml~year^{-1})), title.theme = element_text(angle = 90), title.hjust = 0.5, title.position = &quot;right&quot;,barheight = unit(8, &quot;cm&quot;)))+ coord_cartesian(expand = FALSE)+ theme(panel.background = element_blank()) 15.2.6 Advanced analysis Are you already competent with raster analysis and R, then have a go at completing this task in the practical session. Within the practical weve loaded one and created one raster layer. Undertake some comparative analysis to determine spatial (and temporal if appropraite) differences between the rasters here and any others you may wish to create (e.g. from other interpolation methods). Try to identify where the varaitions are and explain why they are occuring. You could assume that one raster is the gold standard meaning its beleived to be fully correct and compare others to it. Note: take stars, cubelyr and dbplyr package to appendix 15.2.7 stars package Package stars provides infrastructure for data cubes, array data with labeled dimensions, with emphasis on arrays where some of the dimensions relate to time and/or space. Spatial data cubes are arrays with one or more spatial dimensions. Raster data cubes have at least two spatial dimensions that form the raster tesselation. Vector data cubes have at least one spatial dimension that may for instance reflect a polygon tesselation, or a set of point locations. Conversions between the two (rasterization, polygonization) are provided. Vector data are represented by simple feature geometries (packages sf). Tidyverse methods are provided. jan = files.clean[1] %&gt;% stars::read_stars() ggplot()+ stars::geom_stars(data = jan) + scale_fill_gradientn(colors = oce::oce.colors9A(120), na.value = &quot;white&quot;, breaks = seq(-40,40,10), label = seq(-40,40,10), guide = guide_colorbar(title = expression(Average~temperature~(degree*C)), title.theme = element_text(angle = 90), title.hjust = 0.5, title.position = &quot;right&quot;,barheight = unit(8, &quot;cm&quot;)))+ coord_cartesian(expand = FALSE)+ theme(panel.background = element_blank()) 15.3 Practical C: Handling Projections and Coordinate Systems Even experienced GIS-experts can stumble over projections and World Coordinate Systems (WGS). A projection is simply a mathematical function for transforming the X and Y coordinates of a spatial object from spatial reference framework to a different spatial reference framework. In short, any point on the earth can have its location specified by the degrees north or south of the equator (latitude) and west or east of the Greenwich meridian (longitude). These spherical coordinates can be transformed planar Cartesian coordinates using projection transformation equations. For example, the symbol shows up in google maps at latitude = 6.2599°S and longitude = 39.2346°W Simple feature collection with 1 feature and 0 fields geometry type: POINT dimension: XY bbox: xmin: 39.2346 ymin: -6.2599 xmax: 39.2346 ymax: -6.2599 geographic CRS: WGS 84 geometry 1 POINT (39.2346 -6.2599) For the same location on the earths surface, the IMS WGS84 coordinates are (6.2599 , 39.2346 ) degree, and the UTM Zone 37 S coordinates are (525950.6 , 9308060 ) m. Simple feature collection with 1 feature and 0 fields geometry type: POINT dimension: XY bbox: xmin: 525950.6 ymin: 9308060 xmax: 525950.6 ymax: 9308060 projected CRS: WGS 84 / UTM zone 37S geometry 1 POINT (525950.6 9308060) BONUS: Why would you use a Cartesian Projected Coordinate reference System versus WGS84? 15.3.1 Assigning Coordinate reference system If you have a sf spatial data frame consisting of vector data or a raster data set (covered in Chapter @(raster)) that is not tagged with its CRS, there are simple commands to do so: st_crs() for sf data frames and crs() for rasters. The function can be used to show the current CRS or to (re)define the CRS. The CRS can be specified in one of two ways, using EPSG codes, which uses numerical codes for different CRSs, or a prj4 string, which verbosely lists all the parameters for a given CRS. Using EPSG codes is more convenient than using prj4 strings. If you obtain a spatial data set, one of the things you need to be absolutely certain of is its CRS. Most data sets are provided with either files (e.g., .prj files for shape files) or internal metadata (e.g., embedded in a GeoTIFF), or at least a description of their CRS. If you do not know the CRS of your data, you can make an educated guesses. In any case if you have a data set that has no CRS defined, although it may not be absolutely necessary, you should define its CRS in order to follow best practices. Lets create a data frame with coordinates of some countries: require(sf) require(tidyverse) taasisi = data.frame(name = c(&quot;IMS&quot;, &quot;SUZA&quot;, &quot;TAFIRI&quot;), lat = c(-6.2599, -6.1647,-6.6689), lon = c(39.2346, 39.1918, 39.2148)) taasisi name lat lon 1 IMS -6.2599 39.2346 2 SUZA -6.1647 39.1918 3 TAFIRI -6.6689 39.2148 Then create a simple feature from the geographical coordinates taasisi.sf = taasisi %&gt;% sf::st_as_sf(coords = c(&quot;lon&quot;, &quot;lat&quot;)) taasisi.sf Simple feature collection with 3 features and 1 field geometry type: POINT dimension: XY bbox: xmin: 39.1918 ymin: -6.6689 xmax: 39.2346 ymax: -6.1647 CRS: NA name geometry 1 IMS POINT (39.2346 -6.2599) 2 SUZA POINT (39.1918 -6.1647) 3 TAFIRI POINT (39.2148 -6.6689) Looking at the printed results, we notice that the CRS is NA, implying that it has not been defined. We can confirm this by using the st_crs function taasisi %&gt;% sf::st_crs() Coordinate Reference System: NA Because we know that the geographical location were provided in decimal degrees, then we are sure that the appropriate reference system is WGS84 (EPSG = 4326), we can now set the CRS of the spatial object taasisi.sf.wgs84 = taasisi.sf %&gt;% sf::st_set_crs(value = 4326) taasisi.sf.wgs84 Simple feature collection with 3 features and 1 field geometry type: POINT dimension: XY bbox: xmin: 39.1918 ymin: -6.6689 xmax: 39.2346 ymax: -6.1647 geographic CRS: WGS 84 name geometry 1 IMS POINT (39.2346 -6.2599) 2 SUZA POINT (39.1918 -6.1647) 3 TAFIRI POINT (39.2148 -6.6689) Note that the coordinate values remain the same when setting the CRS in the spatial object, only the metadata which changes. taasisi.sf.wgs84 %&gt;% sf::st_crs() Coordinate Reference System: User input: EPSG:4326 wkt: GEOGCRS[&quot;WGS 84&quot;, DATUM[&quot;World Geodetic System 1984&quot;, ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, LENGTHUNIT[&quot;metre&quot;,1]]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], CS[ellipsoidal,2], AXIS[&quot;geodetic latitude (Lat)&quot;,north, ORDER[1], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], AXIS[&quot;geodetic longitude (Lon)&quot;,east, ORDER[2], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], USAGE[ SCOPE[&quot;unknown&quot;], AREA[&quot;World&quot;], BBOX[-90,-180,90,180]], ID[&quot;EPSG&quot;,4326]] 15.3.2 Transforming coordinate System The last section showed you how to define or modify the coordinate system definition. This section shows you how to transform the coordinate values associated with the spatial object to a different coordinate system. This process calculates new coordinate pair values for the points or vertices defining the spatial object. sf package has a st_transform function that is dedicated to transform between reference system. For instance, we might want to change the WGS to UTM. That requires a basic understanding of the UTM zone. Anyway, for now we should not bother with that but bear in your mind that in most cases you will delve into EPSG = 4326 for WGS84 and EPSG = 32737 for UTM Zone 37S. I recommend you to take note of this two numbers as you will most frequent use them especially for scientists working along the coastal waters of East African region in the Indian Ocean. We might want to change the WGS84 to UTM of the three taasisi we just created above. we simply pipe the process as the chunk below highlight. taasisi.sf.utm = taasisi.sf.wgs84 %&gt;% sf::st_transform(crs = 32737) taasisi.sf.utm %&gt;% sf::st_crs() Coordinate Reference System: User input: EPSG:32737 wkt: PROJCRS[&quot;WGS 84 / UTM zone 37S&quot;, BASEGEOGCRS[&quot;WGS 84&quot;, DATUM[&quot;World Geodetic System 1984&quot;, ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, LENGTHUNIT[&quot;metre&quot;,1]]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ID[&quot;EPSG&quot;,4326]], CONVERSION[&quot;UTM zone 37S&quot;, METHOD[&quot;Transverse Mercator&quot;, ID[&quot;EPSG&quot;,9807]], PARAMETER[&quot;Latitude of natural origin&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8801]], PARAMETER[&quot;Longitude of natural origin&quot;,39, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8802]], PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996, SCALEUNIT[&quot;unity&quot;,1], ID[&quot;EPSG&quot;,8805]], PARAMETER[&quot;False easting&quot;,500000, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8806]], PARAMETER[&quot;False northing&quot;,10000000, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8807]]], CS[Cartesian,2], AXIS[&quot;(E)&quot;,east, ORDER[1], LENGTHUNIT[&quot;metre&quot;,1]], AXIS[&quot;(N)&quot;,north, ORDER[2], LENGTHUNIT[&quot;metre&quot;,1]], USAGE[ SCOPE[&quot;unknown&quot;], AREA[&quot;World - S hemisphere - 36Â°E to 42Â°E - by country&quot;], BBOX[-80,36,0,42]], ID[&quot;EPSG&quot;,32737]] 15.4 Practical D: Finding, loading, and cleaning Spatial data One of the benefits of R is its large ecosystem of packages developed and maintained by individuals and teams, and available to the general public via CRAN. The downside of this, however, is that it is sometimes difficult for even experienced coders to determine which package is the best place to start when learning a new skill. Map data (aka geospatial data or Geographic Information System data or GIS data) is no exception to this rule, and is, in fact, particularly intimidating to the uninitiated. This is for two reasons: developers have created (1) different ways of storing and representing GIS data and (2) multiple, similar packages for each type of GIS data representation. In the tutorials in this series, we will introduce GIS data visualization in R using the simple features standard, which has become increasingly popular in recent years and which has the smallest learning curve for those who are already comfortable with data frames and the principles of the tidyverse. The simple features approach works well for many common map-making applications, including drawing regions (e.g., states) or points (e.g., cities) and coloring them to create an analytical insight (e.g., shading by population density). In order to manipulate and visualize data, we will rely primarily on three geospatial packages: sf, ggplot2, and leaflet. You may find that other packages assist greatly with collecting data (e.g., tidycensus for U.S. Census data) or improving the aesthitics of your map (e.g., ggspatial to add a map scale). 15.4.1 Thinking about map data the way R does Many GIS programs (ArcGIS, QGIS, etc.) make it extraordinarily easy for users to create maps by loading a file with a geographic identifier and data to plot. They are designed to simply drag and drop a layers of shapefiles onto a map and automatically draw the appropriate shapes. Then, you can drag and drop a field called population density or GDP per capita onto the map, and the shapes automatically color appropriately. These drag and drop GIS programs do a lot of work behind the scenes to translate your geographic identifier into a geometric shape and your fields into colorful metrics. In R, we have to do this work ourselves. R has no innate knowledge of what we want to graph; we have to provide every detail. This means we need to pass R the information it needs in order to, say, draw the shape of Africa or draw a line representing major roads. R needs to be told the 4 coordinates defining a rectangle; R needs to be told the hundreds of points defining a rectangle-ish shape like Africa. If we want to fill Africa with a color, we need to explicitly tell R how to do so. The manual nature of GIS in R can cause some headaches, as we need to hunt down all of the information in order to provide it to R to graph. Once you have the desired information, however, you will find that the manual nature of Rs graphing allows significantly more flexibility than drag and drop programs allow. We are not constrained by the type of information pre-loaded into the program, by the number of shapes we can draw at once, by the color palletes provided, or by any other factor. We have complete flexibility. If you want to draw state borders (polygons), county borders (more polygons), major highways (lines), and highway rest stops (points), add each of them as an individual layer to the same plot, and color them as you please. There are no constraints when visualizing geospatial data in R. This post will focus on how to find, import, and clean geospatial data. The actual graphing will come in Part 2 (static maps with ggplot2) and Part 3 (interactive maps with leaflet). 15.4.2 A brief introduction to simple features data in R Out in the wild, map data most frequntly comes as either geoJSON files (.geojson) or Shapefiles (.shp). These files will, at the very minimum, contain information about the geometry of each object to be drawn, such as instructions to draw a point in a certain location or to draw a polygon with certain dimensions. The raw file may, however, also contain any amount of additional information, such as a name for the object, or summary statistics (GDP per capita, total population, etc.). Regardless of whether the data is geoJSON or a Shapefile, and regardless of how much additional data the file has, you can use one convenient function from the sf package to import the raw data into R as a simple features object. The function is st_read() and you parse the argument like the path of the shapefile in your working directory. require(tidyverse) require(sf) require(rnaturalearth) africa = st_read(&quot;data/shp/africa.shp&quot;, quiet = TRUE) Lets us explore a simple feature that we just imported africa Simple feature collection with 59 features and 7 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -25.35875 ymin: -34.83983 xmax: 57.80085 ymax: 37.34962 geographic CRS: WGS 84 First 10 features: COUNT CNTRY_NAME FIPS_CNTRY 1 34 Angola AO 2 114 Burundi BY 3 77 Benin BN 4 301 Burkina Faso UV 5 25 Botswana BC 6 51 Central African Republic CT 7 51 Cameroon CM 8 186 Ivory Coast IV 9 46 Congo CF 10 15 Cape Verde CV LAND_AREA_ REGIONA EMPTY EMPTY2 1 124670 &lt;NA&gt; 0 0 2 2783 &lt;NA&gt; 0 0 3 11262 &lt;NA&gt; 0 0 4 27400 &lt;NA&gt; 0 0 5 58173 &lt;NA&gt; 0 0 6 62298 &lt;NA&gt; 0 0 7 47544 &lt;NA&gt; 0 0 8 32246 &lt;NA&gt; 0 0 9 34200 &lt;NA&gt; 0 0 10 403 &lt;NA&gt; 0 0 geometry 1 MULTIPOLYGON (((12.84118 -6... 2 MULTIPOLYGON (((29.05021 -2... 3 MULTIPOLYGON (((3.849006 10... 4 MULTIPOLYGON (((-5.272945 1... 5 MULTIPOLYGON (((23.14635 -1... 6 MULTIPOLYGON (((22.03557 4.... 7 MULTIPOLYGON (((9.640797 3.... 8 MULTIPOLYGON (((-6.091862 4... 9 MULTIPOLYGON (((16.45276 2.... 10 MULTIPOLYGON (((-24.64849 1... The object provide summary, which are; We have 59 features and four fields. Each row (feature) represent individual country in Africa and Each column is a field with (potentially) useful information about the feature. Note that the geometry column is not considered a field We are told this is a collection of multipolygons, as opposed to points, lines, etc. We are told the bounding box for our data (the most western/eastern longitudes and northern/southern latitudes) We are told the Coordinate Reference System (CRS), which in this case is WGS 84. CRSs are cartographers ways of telling each other what system they used for describing points on the earth. Cartographers need to pick an equation for an ellipsoid to approximate earths shape since its slightly pear-shaped. Cartographers also need to determine a set of reference markersknown as a datumto use to set coordinates, as earths tectonic plates shift ever so slightly over time. Togehether, the ellipsoid and datum become a CRS. And the last information is a column called geometry. This column contains everything that R will need to draw boundary of each country in Africa. 15.4.3 Finding data Simple features data in R will always look similar to the example above. You will have some metadata describing the type of geometry, the CRS, and so on; a geometry column; and optionally some fields of additional data. The trouble comes in trying to find the data you needboth the geometry and the proper additional fieldsand getting them together into the same object in R. Fortunate, the package rnaturalearth, which is a well-supported part of the rOpenSci project, provides easy access to global data. We can import directly as a simple features object. Heres a quick look at how to import all the countries in Asia. ne.africa = rnaturalearth::ne_countries(continent = &quot;Africa&quot;, returnclass = &quot;sf&quot;) ne.africa Simple feature collection with 51 features and 63 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -17.62504 ymin: -34.81917 xmax: 51.13387 ymax: 37.34999 CRS: +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 First 10 features: scalerank featurecla labelrank 1 1 Admin-0 country 3 11 1 Admin-0 country 6 13 1 Admin-0 country 5 14 1 Admin-0 country 3 25 1 Admin-0 country 4 26 1 Admin-0 country 4 31 1 Admin-0 country 3 32 1 Admin-0 country 3 33 1 Admin-0 country 2 34 1 Admin-0 country 4 sovereignt sov_a3 adm0_dif 1 Angola AGO 0 11 Burundi BDI 0 13 Benin BEN 0 14 Burkina Faso BFA 0 25 Botswana BWA 0 26 Central African Republic CAF 0 31 Ivory Coast CIV 0 32 Cameroon CMR 0 33 Democratic Republic of the Congo COD 0 34 Republic of Congo COG 0 level type 1 2 Sovereign country 11 2 Sovereign country 13 2 Sovereign country 14 2 Sovereign country 25 2 Sovereign country 26 2 Sovereign country 31 2 Sovereign country 32 2 Sovereign country 33 2 Sovereign country 34 2 Sovereign country admin adm0_a3 geou_dif 1 Angola AGO 0 11 Burundi BDI 0 13 Benin BEN 0 14 Burkina Faso BFA 0 25 Botswana BWA 0 26 Central African Republic CAF 0 31 Ivory Coast CIV 0 32 Cameroon CMR 0 33 Democratic Republic of the Congo COD 0 34 Republic of Congo COG 0 geounit gu_a3 su_dif 1 Angola AGO 0 11 Burundi BDI 0 13 Benin BEN 0 14 Burkina Faso BFA 0 25 Botswana BWA 0 26 Central African Republic CAF 0 31 Ivory Coast CIV 0 32 Cameroon CMR 0 33 Democratic Republic of the Congo COD 0 34 Republic of Congo COG 0 subunit su_a3 brk_diff 1 Angola AGO 0 11 Burundi BDI 0 13 Benin BEN 0 14 Burkina Faso BFA 0 25 Botswana BWA 0 26 Central African Republic CAF 0 31 Ivory Coast CIV 0 32 Cameroon CMR 0 33 Democratic Republic of the Congo COD 0 34 Republic of Congo COG 0 name 1 Angola 11 Burundi 13 Benin 14 Burkina Faso 25 Botswana 26 Central African Rep. 31 Côte d&#39;Ivoire 32 Cameroon 33 Dem. Rep. Congo 34 Congo name_long brk_a3 1 Angola AGO 11 Burundi BDI 13 Benin BEN 14 Burkina Faso BFA 25 Botswana BWA 26 Central African Republic CAF 31 Côte d&#39;Ivoire CIV 32 Cameroon CMR 33 Democratic Republic of the Congo COD 34 Republic of Congo COG brk_name brk_group 1 Angola &lt;NA&gt; 11 Burundi &lt;NA&gt; 13 Benin &lt;NA&gt; 14 Burkina Faso &lt;NA&gt; 25 Botswana &lt;NA&gt; 26 Central African Rep. &lt;NA&gt; 31 Côte d&#39;Ivoire &lt;NA&gt; 32 Cameroon &lt;NA&gt; 33 Democratic Republic of the Congo &lt;NA&gt; 34 Republic of Congo &lt;NA&gt; abbrev postal formal_en 1 Ang. AO People&#39;s Republic of Angola 11 Bur. BI Republic of Burundi 13 Benin BJ Republic of Benin 14 B.F. BF Burkina Faso 25 Bwa. BW Republic of Botswana 26 C.A.R. CF Central African Republic 31 I.C. CI Republic of Ivory Coast 32 Cam. CM Republic of Cameroon 33 D.R.C. DRC Democratic Republic of the Congo 34 Rep. Congo CG Republic of Congo formal_fr note_adm0 note_brk 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 11 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 13 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 14 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 25 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 26 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 31 Republic of Cote D&#39;Ivoire &lt;NA&gt; &lt;NA&gt; 32 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 33 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 34 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; name_sort name_alt mapcolor7 1 Angola &lt;NA&gt; 3 11 Burundi &lt;NA&gt; 2 13 Benin &lt;NA&gt; 1 14 Burkina Faso &lt;NA&gt; 2 25 Botswana &lt;NA&gt; 6 26 Central African Republic &lt;NA&gt; 5 31 Côte d&#39;Ivoire &lt;NA&gt; 4 32 Cameroon &lt;NA&gt; 1 33 Congo, Dem. Rep. &lt;NA&gt; 4 34 Congo, Rep. &lt;NA&gt; 2 mapcolor8 mapcolor9 mapcolor13 pop_est gdp_md_est 1 2 6 1 12799293 110300 11 2 5 8 8988091 3102 13 2 2 12 8791832 12830 14 1 5 11 15746232 17820 25 5 7 3 1990876 27060 26 6 6 9 4511488 3198 31 6 3 3 20617068 33850 32 4 1 3 18879301 42750 33 4 4 7 68692542 20640 34 1 3 10 4012809 15350 pop_year lastcensus gdp_year 1 NA 1970 NA 11 NA 2008 NA 13 NA 2002 NA 14 NA 2006 NA 25 NA 2011 NA 26 NA 2003 NA 31 NA 1998 NA 32 NA 2005 NA 33 NA 1984 NA 34 NA 2007 NA economy income_grp 1 7. Least developed region 3. Upper middle income 11 7. Least developed region 5. Low income 13 7. Least developed region 5. Low income 14 7. Least developed region 5. Low income 25 6. Developing region 3. Upper middle income 26 7. Least developed region 5. Low income 31 6. Developing region 4. Lower middle income 32 6. Developing region 4. Lower middle income 33 7. Least developed region 5. Low income 34 6. Developing region 4. Lower middle income wikipedia fips_10 iso_a2 iso_a3 iso_n3 un_a3 wb_a2 1 NA &lt;NA&gt; AO AGO 024 024 AO 11 NA &lt;NA&gt; BI BDI 108 108 BI 13 NA &lt;NA&gt; BJ BEN 204 204 BJ 14 NA &lt;NA&gt; BF BFA 854 854 BF 25 NA &lt;NA&gt; BW BWA 072 072 BW 26 NA &lt;NA&gt; CF CAF 140 140 CF 31 NA &lt;NA&gt; CI CIV 384 384 CI 32 NA &lt;NA&gt; CM CMR 120 120 CM 33 NA &lt;NA&gt; CD COD 180 180 ZR 34 NA &lt;NA&gt; CG COG 178 178 CG wb_a3 woe_id adm0_a3_is adm0_a3_us adm0_a3_un 1 AGO NA AGO AGO NA 11 BDI NA BDI BDI NA 13 BEN NA BEN BEN NA 14 BFA NA BFA BFA NA 25 BWA NA BWA BWA NA 26 CAF NA CAF CAF NA 31 CIV NA CIV CIV NA 32 CMR NA CMR CMR NA 33 ZAR NA COD COD NA 34 COG NA COG COG NA adm0_a3_wb continent region_un subregion 1 NA Africa Africa Middle Africa 11 NA Africa Africa Eastern Africa 13 NA Africa Africa Western Africa 14 NA Africa Africa Western Africa 25 NA Africa Africa Southern Africa 26 NA Africa Africa Middle Africa 31 NA Africa Africa Western Africa 32 NA Africa Africa Middle Africa 33 NA Africa Africa Middle Africa 34 NA Africa Africa Middle Africa region_wb name_len long_len abbrev_len 1 Sub-Saharan Africa 6 6 4 11 Sub-Saharan Africa 7 7 4 13 Sub-Saharan Africa 5 5 5 14 Sub-Saharan Africa 12 12 4 25 Sub-Saharan Africa 8 8 4 26 Sub-Saharan Africa 20 24 6 31 Sub-Saharan Africa 13 13 4 32 Sub-Saharan Africa 8 8 4 33 Sub-Saharan Africa 15 32 6 34 Sub-Saharan Africa 5 17 10 tiny homepart geometry 1 NA 1 MULTIPOLYGON (((16.32653 -5... 11 NA 1 MULTIPOLYGON (((29.34 -4.49... 13 NA 1 MULTIPOLYGON (((2.691702 6.... 14 NA 1 MULTIPOLYGON (((-2.827496 9... 25 NA 1 MULTIPOLYGON (((25.64916 -1... 26 NA 1 MULTIPOLYGON (((15.27946 7.... 31 NA 1 MULTIPOLYGON (((-2.856125 4... 32 NA 1 MULTIPOLYGON (((13.07582 2.... 33 NA 1 MULTIPOLYGON (((30.83386 3.... 34 NA 1 MULTIPOLYGON (((12.99552 -4... Figure 15.1: Basim map of countries in Africa. Source:rnaturalearth R package 15.4.4 Finding non-geospatial data Chances are that you are coming to a geospatial mapping project with a particular dataset in mind. Perhaps you want to explore the GDP in Africa. To do this, we will need to find the Global Domestic Product data for each country. The World Bank offers that data in their website as table. Unfortunately, the website of World Bank is not always intuitive to navigate, as the data live in many different tables from different goverment surveys. Fortunately, R has a wbstats package that make life easy to query the World Bank database on the indicator of interest. my.indicators = c(life_exp = &quot;SP.DYN.LE00.IN&quot;, gdp_capita =&quot;NY.GDP.PCAP.CD&quot;, pop = &quot;SP.POP.TOTL&quot;) # # check the code for the indicator # wbstats::wb_cachelist ## data wb.data = wbstats::wb_data(indicator = my.indicators, start_date = 2016) ## source source = wbstats::wb_cachelist$indicators source %&gt;% filter(indicator_id == &quot;NY.GDP.PCAP.CD&quot;) %&gt;% pull(source_org) 15.4.5 Combining spatial data with non-spatial data Now we have our spatial Africa countries data and the nonspatial GDP data in tabular form. How in the world do we plot this? Four simple steps to prepare your data for graphing. Import all data (already completed above) Clean your geospatial data frame Combine non-spatial data into a single, clean data frame Merge your two data frames together africa %&gt;% glimpse() Rows: 59 Columns: 8 $ COUNT &lt;dbl&gt; 34, 114, 77, 301, 25, 51, 51, ... $ CNTRY_NAME &lt;fct&gt; Angola, Burundi, Benin, Burkin... $ FIPS_CNTRY &lt;fct&gt; AO, BY, BN, UV, BC, CT, CM, IV... $ LAND_AREA_ &lt;dbl&gt; 124670, 2783, 11262, 27400, 58... $ REGIONA &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA... $ EMPTY &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... $ EMPTY2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... $ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((1... wb.data %&gt;% glimpse() Rows: 217 Columns: 7 $ iso2c &lt;chr&gt; &quot;AW&quot;, &quot;AF&quot;, &quot;AO&quot;, &quot;AL&quot;, &quot;AD&quot;, ... $ iso3c &lt;chr&gt; &quot;ABW&quot;, &quot;AFG&quot;, &quot;AGO&quot;, &quot;ALB&quot;, &quot;A... $ country &lt;chr&gt; &quot;Aruba&quot;, &quot;Afghanistan&quot;, &quot;Angol... $ date &lt;dbl&gt; 2016, 2016, 2016, 2016, 2016, ... $ gdp_capita &lt;dbl&gt; 28281.3505, 509.2187, 3506.072... $ life_exp &lt;dbl&gt; 75.86800, 63.76300, 59.92500, ... $ pop &lt;dbl&gt; 104872, 35383128, 28842484, 28... We notice that, the spatial africa simple feature and nonspatial GDP data have one common variablecountry namethought labeled different. We can use this columns as key to combine the two dataset. Before we do that, we have to clean and rename those columns to a common wordcountry africa = africa %&gt;% select(country = CNTRY_NAME, country_code = FIPS_CNTRY) wb.data = wb.data %&gt;% rename(country_code = iso2c) A dplyr package has several functions for combining spatial and non-spatial data. Since we need to retain matching dataset from the shapefile, we use left_join() function and combine using the column country, which is found in both dataset. The left_join function will keep all rows from the spatial dataset and fill in NA for missing non-spatial rows. africa.gdp = africa %&gt;% left_join(wb.data, by = &quot;country&quot;) africa.gdp Simple feature collection with 59 features and 8 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -25.35875 ymin: -34.83983 xmax: 57.80085 ymax: 37.34962 geographic CRS: WGS 84 First 10 features: country country_code.x 1 Angola AO 2 Burundi BY 3 Benin BN 4 Burkina Faso UV 5 Botswana BC 6 Central African Republic CT 7 Cameroon CM 8 Ivory Coast IV 9 Congo CF 10 Cape Verde CV country_code.y iso3c date gdp_capita life_exp 1 AO AGO 2016 3506.0729 59.925 2 BI BDI 2016 282.1931 60.528 3 BJ BEN 2016 1087.2878 60.885 4 BF BFA 2016 688.2497 60.354 5 BW BWA 2016 7243.8703 68.178 6 CF CAF 2016 402.1913 51.593 7 CM CMR 2016 1364.3301 58.063 8 &lt;NA&gt; &lt;NA&gt; NA NA NA 9 &lt;NA&gt; &lt;NA&gt; NA NA NA 10 &lt;NA&gt; &lt;NA&gt; NA NA NA pop geometry 1 28842484 MULTIPOLYGON (((12.84118 -6... 2 10487998 MULTIPOLYGON (((29.05021 -2... 3 10872067 MULTIPOLYGON (((3.849006 10... 4 18646378 MULTIPOLYGON (((-5.272945 1... 5 2159944 MULTIPOLYGON (((23.14635 -1... 6 4537687 MULTIPOLYGON (((22.03557 4.... 7 23926539 MULTIPOLYGON (((9.640797 3.... 8 NA MULTIPOLYGON (((-6.091862 4... 9 NA MULTIPOLYGON (((16.45276 2.... 10 NA MULTIPOLYGON (((-24.64849 1... Once we have combined the non-spatial data into the spatial, we are now ready to map the distribution of GDP across Africa. We use the ggplot package to map this distribution as shown in the chunk below and the figure thereafter ggplot(data = africa.gdp, aes(fill = gdp_capita)) + geom_sf()+ scale_fill_viridis_c()+ labs(fill = NULL, # title = &quot;Gross Domestic Product for 2016&quot;, caption = &quot;Source World Bank national accounts data, and OECD National Accounts data files.&quot;) Figure 15.2: The Africas countries Gross Domestic Product for 2016 15.4.6 Conclusion To conclude, we have now seen how to find geospatial data and import it into R as a simple features object. We have seen how to find World Bank data or other non-spatial data and import it into R. We have seen how to tidy both geospatial and non-spatial data and join them as a single dataset in preparation for visualization. Finally, we have had a brief preview of how to plot using ggplot2. Visualization was not a focus of this post, but how can we write a whole post on maps without showing you a single map! 15.5 Practical E: Making maps with ggplot2 To draw static maps in R, we will use ggplot2, which is not only the standard package for drawing traditional bar plots, line plots, historgrams, and other standard visualizations of discrete or continuous data, but is also the standard package for drawing maps. A few other packages are excellent alternatives, including sf and maps. We prefer ggplot2 because it has a consistent grammar of graphics between its various applications and offers a robust set of geospatial graphing functions. Lets take a moment to refresh ourselves on ggplot2s functionality. To make a plot, you need three steps: (1) initate the plot, (2) add as many data layers as you want, and (3) adjust plot aesthetics, including scales, titles, and footnotes. To (1) initiate the plot, we first call ggplot(), and to (2) add data layers, we next call geom_sf() once for each layer. In ggplot2, functions inherit from functions called higher up. Thus, if we call ggplot(data = neighborhoods), we do not need to subsequently specify geom_sf(data = neighborhoods), as the data argument is inherited in geom_sf(). The same goes for aesthetics. If we specify ggplot(data = neighborhoods, aes(fill = FILL_GROUP)), the subsequent geom_sf() call will already know how we want to color our plot. The behavior of inheriting aesthetics is convenient for drawing graphs that only use one dataset, which is most common in non-geospatial visualiation. A line plot might call both geom_line() and geom_point() and want both the lines and points to inherit the same dataset. Since maps frequently have many layers from different sources, we will elect to specify our data and aesthetics within the geom_sf() calls and not in the ggplot() calls. Finally, to (3) adjust overall aesthictics, we will use a range of functions, such as the theme_* family of functions, the scale_fill_* family of functions, and the coord_sf() function. Before we get there, we need to load the package we are going to use and also prepare the data. Load package require(sf) require(tidyverse) 15.5.1 Import and clean the datasets The code below are used to load and clean the dataset we will be using in this assignment. The first dataset is the shapefile of Africas countries. This dataset is a polygon and will form a basemap for layering on additional features while making a map. The second dataset is also the mangrove, which is also a polygon containing mangrove forest locations along the coastal areas of Tanzania mainland and Zanzibar. The third datasetstations.names contains points representing locations of sampling in the Rufiji delta. #read Africa continental shapefile that we will use throught the munuscript as basemap dataset for maps africa = read_sf(&quot;E:/Data Manipulation/nyamisi/regional/africa.shp&quot;) # clean the file tz = africa %&gt;% select(-c(FIPS_CNTRY, REGIONA, EMPTY, EMPTY2)) %&gt;% filter(CNTRY_NAME == &quot;Tanzania&quot;) mangrove = st_read(&quot;e:/GIS/Projects/MASTER/tansea/tza_mangroves_50k.shp&quot;, quiet = TRUE) coral = st_read(&quot;e:/GIS/Projects/MASTER/tansea/tza_coral_reefs_50k.shp&quot;, quiet = TRUE) stations = st_read(&quot;e:/Data Manipulation/maps/files/mindu_stations.shp&quot;, quiet = TRUE) station.names = stations %&gt;% st_coordinates() %&gt;% as_tibble() %&gt;% rename(lon = 1, lat = 2) %&gt;% bind_cols(st_drop_geometry(stations)) names.df = st_read(&quot;e:/Data Manipulation/maps/files/mindu_stations_names.shp&quot;, quiet = TRUE) %&gt;% wior::point_tb() And then make a simple plot with just our two data layers. ggplot() + geom_sf(data = tz, col = NA, fill = &quot;#DCDDDE&quot;, size = .2)+ geom_sf(data = mangrove, fill = &quot;#9D9D9E&quot;, col = NA)+ coord_sf(xlim = c(39.22,39.62), ylim = c(-8.25,-7.74), crs = 4326)+ theme_bw()%+% theme(panel.background = element_rect(fill = &quot;#FFFFFF&quot;), axis.text = element_text(colour = &quot;black&quot;, size = 10)) + scale_x_continuous(breaks = seq(39.27, 39.57, length.out = 4), position = &quot;top&quot;) + scale_y_continuous(breaks = seq(-8.2,-7.75, length.out = 6)) 15.5.2 Add highlights and annotations I want to highlight and annotate my map. Since the highlight rule need to distinguish land from mangroves and water, we need to add and specify color for each particular component. First we use fill argument to define the land color to #DCDDDE and mangrove #9D9D9E. We also add the label text using ggrepel::geom_text_repel() using the aes(label = names) to specify the text and other parameters to adjust how it appears on the plot. map = ggplot() + geom_sf(data = tz, col = NA, fill = &quot;#DCDDDE&quot;, size = .2)+ geom_sf(data = mangrove, fill = &quot;#9D9D9E&quot;, col = NA)+ geom_point(data = names.df, aes(x = lon, y = lat), size = 3)+ ggrepel::geom_text_repel(data = names.df, aes(x = lon, y = lat, label = names), point.padding = .25, size = 3.5, nudge_x = 0.025)+ coord_sf(xlim = c(39.22,39.62), ylim = c(-8.25,-7.74), crs = 4326)+ theme_bw()%+% theme(panel.background = element_rect(fill = &quot;#FFFFFF&quot;), axis.title = element_blank(), axis.text = element_text(colour = &quot;black&quot;, size = 10)) + scale_x_continuous(breaks = seq(39.27, 39.57, length.out = 4), position = &quot;top&quot;) + scale_y_continuous(breaks = seq(-8.2,-7.75, length.out = 6)) + ggspatial::annotation_north_arrow(location = &quot;tr&quot;, width = unit(.5, &quot;cm&quot;), height = unit(.7, &quot;cm&quot;))+ ggspatial::annotation_scale(location = &quot;br&quot;) map 15.5.3 inset map An inset map is a smaller map featured on the same page as the main map. Traditionally, inset maps are shown at a larger scale (smaller area) than the main map. Often, an inset map is used as a locator map that shows the area of the main map in a broader, more familiar geographical frame of reference. require(tmap) data(&quot;World&quot;) africa = World %&gt;% st_transform(4326) %&gt;% filter(continent == &quot;Africa&quot;) extent = data.frame(lon = c(39.0, 39.8, 39.8, 39.0, 39.0), lat = c(-8.2,-8.2,-7.5,-7.5,-8.2)) inset = africa %&gt;% ggplot() + geom_sf()+ geom_path(data = extent, aes(x = lon, y = lat), col = &quot;black&quot;, size = 1.05, lineend = &quot;square&quot;, linejoin = &quot;mitre&quot;)+ coord_sf(xlim = c(30.1,50), ylim = c(-24.0,5))+ theme_bw()+ theme(panel.grid = element_blank(), axis.text = element_blank(), axis.title = element_blank(), axis.ticks = element_blank(), plot.background = element_blank(), panel.background = element_rect(fill = &quot;white&quot;)) inset then we combine the main map and an inset using functions from cowplot package as the chunk below highlight cowplot::ggdraw() + cowplot::draw_plot(plot = map,x = 0, y = 0, width = 1, height = 1) + cowplot::draw_plot(plot = inset, x = 0.62, y = .075, width = .32, height = .32) 15.5.4 Final beautification The options for beautifying your map are endless. Some people like to add a scale and north arrow using the ggspatial package. I prefer to leave it off but to add axis labels, a title and subtitle, a source note, and to make a few additional adjustments using the theme() function. The aesthetic choice is yours. ggplot() + geom_sf(data = tz, col = NA, fill = &quot;#E8D6B1&quot;, size = .2)+ geom_sf(data = mangrove, fill = &quot;darkgreen&quot;, col = NA)+ geom_point(data = names.df, aes(x = lon, y = lat), size = 3)+ ggrepel::geom_text_repel(data = names.df, aes(x = lon, y = lat, label = names), point.padding = .25, size = 3.5, nudge_x = 0.025)+ coord_sf(xlim = c(39.22,39.62), ylim = c(-8.25,-7.74), crs = 4326)+ theme_bw()%+% theme(panel.background = element_rect(fill = &quot;#98F5FF&quot;), axis.text = element_text(colour = &quot;black&quot;, size = 10)) + scale_x_continuous(breaks = seq(39.27, 39.57, length.out = 4), position = &quot;top&quot;) + scale_y_continuous(breaks = seq(-8.2,-7.75, length.out = 6)) + labs(x = &quot;&quot;, y = &quot;&quot;)+ ggspatial::annotation_north_arrow(location = &quot;tr&quot;, width = unit(.5, &quot;cm&quot;), height = unit(.7, &quot;cm&quot;))+ ggspatial::annotation_scale(location = &quot;br&quot;) 15.6 Practical F: The bathmetry data ggplot2 works with data that are tidydata frame arranged in such way that observations are in rows and variables are in columns and each value must have its own cell. But, the bathmetry data is from ETOPO1 and came in .asc format. First read the file with the raster::raster() function. Load package require(sf) require(tidyverse) ## read the ascii file tz.bath = raster::raster(&quot;e:/GIS/ROADMAP/Etopo 1/Tanzania_etopo1/tanz1_-3432.asc&quot;) ## convert raster to data frame tz.bath.df = tz.bath %&gt;% raster::as.data.frame(xy = TRUE) %&gt;% dplyr::as_tibble() ## rename the variable tz.bath.df = tz.bath.df %&gt;% dplyr::rename(lon = x, lat = y, depth = 3)%&gt;% dplyr::mutate(depth = as.integer(depth)) ## chop the area of interest study.area.bath= tz.bath.df %&gt;% dplyr::filter(lon &gt; 38.5 &amp; lon &lt; 41 &amp; lat &gt; -9 &amp; lat &lt; - 6 &amp; depth &lt; 0 ) ## Importing shapefile africa = sf::st_read(&quot;e:/bookdown/spatil_r/data/africa.shp&quot;, quiet = TRUE) Since the layer is for the whole Africa, to reduce the processing time, we must reduce the geographical extent to the area of interest. We use the st_crop() function to chop the area that we want to map and discard the rest. study.area = africa %&gt;% sf::st_crop(xmin = 38.0, xmax = 40.5, ymin = -9, ymax = -5.5) stations = tibble(sites = c(&quot;Mbegani&quot;,&quot;Kunduchi&quot;, &quot;Pombwe&quot;), lon = c(38.970029, 39.223579, 39.356356), lat = c(-6.471443, -6.668289,-8.271291)) main.map = ggplot() + geom_sf(data = study.area, col = &quot;#ED6300&quot;, fill = &quot;#FBE1CB&quot;,size = .5) + metR::geom_contour2(data = study.area.bath, aes(x = lon, y = lat, z=depth*-1), binwidth = 200)+ metR::geom_text_contour(data = study.area.bath, aes(x = lon, y = lat, z=depth*-1), binwidth = 200, rotate = TRUE, check_overlap = TRUE, parse = FALSE, size = 3.5, stroke = .35, col = &quot;#ED6300&quot;)+ metR::geom_contour2(data = study.area.bath %&gt;% filter(depth &gt; -30), aes(x = lon, y = lat, z=depth*-1), binwidth = 20, col = &quot;black&quot;)+ metR::geom_text_contour(data = study.area.bath%&gt;% filter(depth &gt; -30), aes(x = lon, y = lat, z=depth*-1), binwidth = 20, rotate = TRUE, check_overlap = TRUE, parse = FALSE, size = 3.5, stroke = .35, col = &quot;#ED6300&quot;)+ geom_point(data = stations, aes(x = lon, y = lat), size = 5) + ggrepel::geom_text_repel(data = stations, aes(x = lon, y = lat, label = sites), size = 4, point.padding = TRUE, segment.colour = NA,nudge_x = -0.095, nudge_y = .0075,) + coord_sf(xlim = c(38.7, 40.0), ylim = c(-8.3, -6.3))+ theme_bw() %+% theme(panel.background = element_rect(fill = NA), panel.grid.major = element_line(linetype = 3, colour = &quot;grey60&quot;), axis.text = element_text(colour = 1, size = 12), axis.title = element_blank(), legend.background = element_blank(), legend.key = element_blank(), legend.position = c(.08,.32))+ scale_x_continuous(breaks = seq(38.8,40.5, 0.3))+ scale_y_continuous(breaks = seq(-8.5,-6, 0.4))+ ggspatial::annotation_scale(location = &quot;bl&quot;, text_cex = .95)+ ggspatial::annotation_north_arrow(location = &quot;tr&quot;, height = unit(2, &quot;lines&quot;), width = unit(1, &quot;lines&quot;)) world = spData::world aoi = data.frame(lon = c(38.5, 40, 40, 38.5, 38.5), lat = c(-8.5, -8.5, -6, -6, -8.5)) inset.map = ggplot()+ geom_sf(data = spData::world, col = &quot;#ED6300&quot;, fill = &quot;#FBE1CB&quot;,size = .5)+ geom_sf(data = spData::world %&gt;% filter(name_long==&quot;Tanzania&quot;), col = &quot;#ED6300&quot;, fill = &quot;#F2B88C&quot;,size = .5)+ coord_sf(xlim = c(36, 42), ylim = c(-13,-1))+ geom_path(data = aoi, aes(x = lon, y = lat), size = 1.2)+ theme_bw()+ theme(plot.background = element_blank(), panel.background = element_rect(fill =&quot;white&quot;), axis.text = element_blank(), axis.ticks = element_blank(), panel.grid = element_line(colour = &quot;white&quot;)) + labs(x = NULL, y = NULL) cowplot::ggdraw()+ cowplot::draw_plot(plot = main.map, x = 0, y = 0, width = 1, height = 1, scale = 1) + cowplot::draw_plot(plot = inset.map, x = 0.2, y = 0.1, width = 0.3,height = 0.3) # cowplot::ggsave2(filename = &quot;e:/Data Manipulation/maps/map1.png&quot;, dpi = 1200) 15.6.1 Zoom in on a region of interest By default, ggplot2 will zoom out so that all of the mapping objects are in the image. Suppose, however, that we are interested in a smaller region of the map. We can use ggplot2::coord_sf() to specify the coordinates to display. By default, geom_sf() calls coord_sf() in the background, but by explicitly calling it ourselves, we can override the default parameters. Below, we will specify our latitude and longitude, and set expand = FALSE. By default, expand is true, which puts a small buffer around the coordinates we specify. Its an aesthetic choice. While were here, lets make a brief comment on CRSs in ggplot2 mapping. If you recall from Part 1 of this series, the CRS is the ellipsoid and datum used to reference points on the globe. ggplot2 will take the first CRS provided (in this case, in our africa dataset) and ensure that all subsequent layers use the same CRS. It automatically converts any mismatched CRSs to the first one provided. Using coord_sf(), we have options to change the CRS and the datum. Changing the datum wont affect plotting, but will affect where graticules (latitude/longitude lines) are drawn if you choose to include them. By default, ggplot2 draws graticules using WGS 84 (EPSG: 4326), which happens to be the CRS of our two datasets. ggplot() + geom_sf(data = study.area, col = &quot;#ED6300&quot;, fill = &quot;#FBE1CB&quot;,size = .5) + metR::geom_contour2(data = study.area.bath, aes(x = lon, y = lat, z=depth*-1), binwidth = 200)+ metR::geom_text_contour(data = study.area.bath, aes(x = lon, y = lat, z=depth*-1), binwidth = 200, rotate = TRUE, check_overlap = TRUE, parse = FALSE, size = 3.5, stroke = .35, col = &quot;black&quot;, label.placement = metR::label_placement_flattest(ref_angle = 0))+ geom_point(data = stations %&gt;% filter(sites != &quot;Pombwe&quot;), aes(x = lon, y = lat), size = 5) + ggrepel::geom_text_repel(data = stations %&gt;% filter(sites != &quot;Pombwe&quot;), aes(x = lon, y = lat, label = sites), size = 4, point.padding = TRUE, segment.colour = NA,nudge_x = -0.095, nudge_y = .0075,) + coord_sf(xlim = c(38.8, 40.0), ylim = c(-7, -6.3), expand = TRUE)+ theme_bw() %+% theme(panel.background = element_rect(fill = NA), panel.grid.major = element_line(linetype = 3, colour = &quot;grey60&quot;), axis.text = element_text(colour = 1, size = 12), axis.title = element_blank(), legend.background = element_blank(), legend.key = element_blank(), legend.position = c(.08,.32))+ scale_x_continuous(breaks = seq(38.8,40.5, 0.3))+ scale_y_continuous(breaks = seq(-8.5,-6, 0.4))+ ggspatial::annotation_scale(location = &quot;bl&quot;, text_cex = .95)+ ggspatial::annotation_north_arrow(location = &quot;tr&quot;, height = unit(2, &quot;lines&quot;), width = unit(1, &quot;lines&quot;))+ annotate(geom = &quot;text&quot;, x = 39.15, y = -6.4, label = &quot;Zanzibar\\nChannel&quot;, size = 5, col = &quot;#ED6300&quot;) 15.7 Practical G: Drawing maps programmatically with R In this part, we will cover the fundamentals of mapping using ggplot2 associated to sf, and presents the basics elements and parameters we can play with to prepare a map. Maps are used in a variety of fields to express data in an appealing and interpretive way. Data can be expressed into simplified patterns, and this data interpretation is generally lost if the data is only seen through a spread sheet. Maps can add vital context by incorporating many variables into an easy to read and applicable context. Maps are also very important in the information world because they can quickly allow the public to gain better insight so that they can stay informed. Its critical to have maps be effective, which means creating maps that can be easily understood by a given audience. For instance, maps that need to be understood by children would be very different from maps intended to be shown to geographers. Knowing what elements are required to enhance your data is key into making effective maps. Basic elements of a map that should be considered are polygon, points, lines, and text. Polygons, on a map, are closed shapes such as country borders. Lines are considered to be linear shapes that are not filled with any aspect, such as highways, streams, or roads. Finally, points are used to specify specific positions, such as city or landmark locations. With that in mind, one need to think about what elements are required in the map to really make an impact, and convey the information for the intended audience. Layout and formatting are the second critical aspect to enhance data visually. The arrangement of these map elements and how they will be drawn can be adjusted to make a maximum impact. 15.7.1 A solution using R and its ecosystem of packages Current solutions for creating maps usually involves GIS software, such as ArcGIS, QGIS, eSpatial, etc., which allow to visually prepare a map, in the same approach as one would prepare a poster or a document layout. On the other hand, R, a free and open-source software development environment (IDE) that is used for computing statistical data and graphic in a programmable language, has developed advanced spatial capabilities over the years, and can be used to draw maps programmatically. R is a powerful and flexible tool. R can be used from calculating data sets to creating graphs and maps with the same data set. R is also free, which makes it easily accessible to anyone. Some other advantages of using R is that it has an interactive language, data structures, graphics availability, a developed community, and the advantage of adding more functionalities through an entire ecosystem of packages. R is a scriptable language that allows the user to write out a code in which it will execute the commands specified. Using R to create maps brings these benefits to mapping. Elements of a map can be added or removed with ease. R code can be tweaked to make major enhancements with a stroke of a key. It is also easy to reproduce the same maps for different data sets. It is important to be able to script the elements of a map, so that it can be re-used and interpreted by any user. In essence, comparing typical GIS software and R for drawing maps is similar to comparing word processing software (e.g. Microsoft Office or LibreOffice) and a programmatic typesetting system such as LaTeX, in that typical GIS software implement a WYSIWIG approach (What You See Is What You Get), while R implements a WYSIWYM approach (What You See Is What You Mean). The package ggplot2 implements the grammar of graphics in R, as a way to create code that make sense to the user: The grammar of graphics is a term used to breaks up graphs into semantic components, such as geometries and layers. Practically speaking, it allows (and forces!) the user to focus on graph elements at a higher level of abstraction, and how the data must be structured to achieve the expected outcome. While ggplot2 is becoming the de facto standard for R graphs, it does not handle spatial data specifically. The current state-of-the-art of spatial objects in R relies on Spatial classes defined in the package sp, but the new package sf has recently implemented the simple feature standard, and is steadily taking over sp. Recently, the package ggplot2 has allowed the use of simple features from the package sf as layers in a graph4. The combination of ggplot2 and sf therefore enables to programmatically create maps, using the grammar of graphics, just as informative or visually appealing as traditional GIS software. 15.7.2 Getting started Many R packages are available from CRAN, the Comprehensive R Archive Network, which is the primary repository of R packages. The full list of packages necessary for this series of tutorials can be installed with: install.packages(c(&quot;cowplot&quot;, &quot;ggrepel&quot;, &quot;tidyverse&quot;, &quot;patchwork&quot; &quot;ggspatial&quot;, &quot;libwgeom&quot;, &quot;sf&quot;, &quot;rnaturalearth&quot;, &quot;rnaturalearthdata&quot;) We start by loading the basic packages necessary for all maps, i.e. ggplot2 and sf. We also suggest to use the classic dark-on-light theme for ggplot2 (theme_bw), which is appropriate for maps: require(tidyverse) require(sf) theme_set(theme_bw()) The package rnaturalearth provides a map of countries of the entire world. Use ne_countries to pull country data and choose the scale (rnaturalearthhires is necessary for scale = \"large\"). The function can return sp classes (default) or directly sf classes, as defined in the argument returnclass: library(rnaturalearth) library(rnaturalearthdata) world &lt;- rnaturalearth::ne_countries(scale = &quot;medium&quot;, returnclass = &quot;sf&quot;) class(world) [1] &quot;sf&quot; &quot;data.frame&quot; 15.7.3 Data and basic plot (ggplot and geom_sf) First, let us start with creating a base map of the world using ggplot2. This base map will then be extended with different map elements, as well as zoomed in to an area of interest. We can check that the world map was properly retrieved and converted into an sf object, and plot it with ggplot2: ggplot(data = world) + geom_sf() This call nicely introduces the structure of a ggplot call: The first part ggplot(data = world) initiates the ggplot graph, and indicates that the main data is stored in the world object. The line ends up with a + sign, which indicates that the call is not complete yet, and each subsequent line correspond to another layer or scale. In this case, we use the geom_sf function, which simply adds a geometry stored in a sf object. By default, all geometry functions use the main data defined in ggplot(), but we will see later how to provide additional data. Note that layers are added one at a time in a ggplot call, so the order of each layer is very important. All data will have to be in an sf format to be used by ggplot2; data in other formats (e.g. classes from sp) will be manually converted to sf classes if necessary. 15.7.4 Title, subtitle, and axis labels (ggtitle, xlab, ylab) A title and a subtitle can be added to the map using the function ggtitle, passing any valid character string (e.g. with quotation marks) as arguments. Axis names are absent by default on a map, but can be changed to something more suitable (e.g. Longitude and Latitude), depending on the map: ggplot(data = world) + geom_sf() + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) + ggtitle(&quot;World map&quot;, subtitle = paste0(&quot;(&quot;, length(unique(world$NAME)), &quot; countries)&quot;)) 15.7.5 Map color (geom_sf) In many ways, sf geometries are no different than regular geometries, and can be displayed with the same level of control on their attributes. Here is an example with the polygons of the countries filled with a green color (argument fill), using black for the outline of the countries (argument color): ggplot(data = world) + geom_sf(color = &quot;black&quot;, fill = &quot;lightgreen&quot;) The package ggplot2 allows the use of more complex color schemes, such as a gradient on one variable of the data. Here is another example that shows the population of each country. In this example, we use the viridis colorblind-friendly palette for the color gradient (with option = \"plasma\" for the plasma variant), using the square root of the population (which is stored in the variable pop_est of the world object): ggplot(data = world) + geom_sf(aes(fill = pop_est)) + scale_fill_viridis_c(option = &quot;plasma&quot;, trans = &quot;sqrt&quot;) 15.7.6 Projection and extent (coord_sf) The function coord_sf allows to deal with the coordinate system, which includes both projection and extent of the map. By default, the map will use the coordinate system of the first layer that defines one (i.e. scanned in the order provided), or if none, fall back on WGS84 (latitude/longitude, the reference system used in GPS). Using the argument crs, it is possible to override this setting, and project on the fly to any projection. This can be achieved using any valid PROJ4 string (here, the European-centric ETRS89 Lambert Azimuthal Equal-Area projection): ggplot(data = world) + geom_sf() + coord_sf(crs = &quot;+proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +units=m +no_defs &quot;) Spatial Reference System Identifier (SRID) or an European Petroleum Survey Group (EPSG) code are available for the projection of interest, they can be used directly instead of the full PROJ4 string. The two following calls are equivalent for the ETRS89 Lambert Azimuthal Equal-Area projection, which is EPSG code 3035: ggplot(data = world) + geom_sf() + coord_sf(crs = &quot;+init=epsg:3035&quot;) ggplot(data = world) + geom_sf() + coord_sf(crs = st_crs(3035)) The extent of the map can also be set in coord_sf, in practice allowing to zoom in the area of interest, provided by limits on the x-axis (xlim), and on the y-axis (ylim). Note that the limits are automatically expanded by a fraction to ensure that data and axes dont overlap; it can also be turned off to exactly match the limits provided with expand = FALSE: ggplot(data = world) + geom_sf() + coord_sf(xlim = c(25, 60), ylim = c(-35, 8), expand = FALSE) 15.7.7 Scale bar and North arrow (package ggspatial) Several packages are available to create a scale bar on a map (e.g. prettymapr, vcd, ggsn, or legendMap). We introduce here the package ggspatial, which provides easy-to-use functions scale_bar that allows to add simultaneously the north symbol and a scale bar into the ggplot map. Five arguments need to be set manually: lon, lat, distance_lon, distance_lat, and distance_legend. The location of the scale bar has to be specified in longitude/latitude in the lon and lat arguments. The shaded distance inside the scale bar is controlled by the distance_lon argument. while its width is determined by distance_lat. Additionally, it is possible to change the font size for the legend of the scale bar (argument legend_size, which defaults to 3). The North arrow behind the N north symbol can also be adjusted for its length (arrow_length), its distance to the scale (arrow_distance), or the size the N north symbol itself (arrow_north_size, which defaults to 6). Note that all distances (distance_lon, distance_lat, distance_legend, arrow_length, arrow_distance) are set to \"km\" by default in distance_unit; they can also be set to nautical miles with nm, or miles with mi. require(ggspatial) ggplot(data = world) + geom_sf() + coord_sf(xlim = c(25, 60), ylim = c(-35, 8), expand = FALSE)+ ggspatial::annotation_scale(location = &quot;bl&quot;, width_hint = 0.5) + ggspatial::annotation_north_arrow(location = &quot;br&quot;, which_north = &quot;true&quot;, # pad_x = unit(0.75, &quot;in&quot;), pad_y = unit(0.5, &quot;in&quot;), style = ggspatial::north_arrow_fancy_orienteering) Scale on map varies by more than 10%, scale bar may be inaccurate Note the warning of the inaccurate scale bar: since the map use unprojected data in longitude/latitude (WGS84) on an equidistant cylindrical projection (all meridians being parallel), length in (kilo)meters on the map directly depends mathematically on the degree of latitude. Plots of small regions or projected data will often allow for more accurate scale bars. 15.7.8 Country names and other names (geom_text and annotate) The world data set already contains country names and the coordinates of the centroid of each country (among more information). We can use this information to plot country names, using world as a regular data.frame in ggplot2. The function geom_text can be used to add a layer of text to a map using geographic coordinates. The function requires the data needed to enter the country names, which is the same data as the world map. Again, we have a very flexible control to adjust the text at will on many aspects: The size (argument size); The alignment, which is centered by default on the coordinates provided. The text can be adjusted horizontally or vertically using the arguments hjust and vjust, which can either be a number between 0 (right/bottom) and 1 (top/left) or a character (left, middle, right, bottom, center, top). The text can also be offset horizontally or vertically with the argument nudge_x and nudge_y; The font of the text, for instance its color (argument color) or the type of font (fontface); The overlap of labels, using the argument check_overlap, which removes overlapping text. Alternatively, when there is a lot of overlapping labels, the package ggrepel provides a geom_text_repel function that moves label around so that they do not overlap. For the text labels, we are defining the centroid of the counties with st_centroid, from the package sf. Then we combined the coordinates with the centroid, in the geometry of the spatial data frame. The package sf is necessary for the command st_centroid. Additionally, the annotate function can be used to add a single character string at a specific location, as demonstrated here to add the Gulf of Mexico: require(sf) country.points = world %&gt;% wior::polygon_tb() %&gt;% select(lon,lat,country = sovereignt, continent) # world_points&lt;- st_centroid(world) # world_points &lt;- cbind(world, st_coordinates(st_centroid(world$geometry))) ggplot(data = world) + geom_sf() + geom_text(data = country.points, aes(x = lon, y = lat, label = country), check_overlap = TRUE)+ annotate(geom = &quot;text&quot;, x = 43, y = -27, label = &quot;INDIAN OCEAN&quot;, color = &quot;grey80&quot;) + coord_sf(xlim = c(25, 60), ylim = c(-35, 8), expand = FALSE)+ ggspatial::annotation_scale(location = &quot;bl&quot;, width_hint = 0.5) + ggspatial::annotation_north_arrow(location = &quot;br&quot;, which_north = &quot;true&quot;, # pad_x = unit(0.75, &quot;in&quot;), pad_y = unit(0.5, &quot;in&quot;), style = ggspatial::north_arrow_fancy_orienteering) 15.8 Colophon This book was written using bookdown (Xie 2016) using RStudio (RStudio Team 2015). The book website (https://lugoga.github.io/geomarine/) is hosted with Github. This version of the book was built with: R version 3.6.3 (2020-02-29) Platform: x86_64-w64-mingw32/x64 (64-bit) Running under: Windows 10 x64 (build 18363) Matrix products: default Random number generation: RNG: Mersenne-Twister Normal: Inversion Sample: Rounding locale: [1] LC_COLLATE=English_United States.1252 [2] LC_CTYPE=English_United States.1252 [3] LC_MONETARY=English_United States.1252 [4] LC_NUMERIC=C [5] LC_TIME=English_United States.1252 attached base packages: [1] stats graphics grDevices utils datasets [6] methods base other attached packages: [1] ggspatial_1.1.4 rnaturalearthdata_0.1.0 [3] rnaturalearth_0.1.0 tmap_3.2 [5] RColorBrewer_1.1-2 metR_0.9.0 [7] htmltools_0.5.0 kableExtra_1.3.1 [9] haven_2.3.1 readxl_1.3.1 [11] lubridate_1.7.9.2 magrittr_2.0.1 [13] leaflet_2.0.3 forcats_0.5.0 [15] stringr_1.4.0 dplyr_1.0.2 [17] purrr_0.3.4 readr_1.4.0 [19] tidyr_1.1.2 tibble_3.0.4 [21] ggplot2_3.3.2 tidyverse_1.3.0 [23] sf_0.9-6 knitr_1.30 loaded via a namespace (and not attached): [1] backports_1.2.1 spam_2.6-0 [3] servr_0.21 lwgeom_0.2-5 [5] plyr_1.8.6 sp_1.4-4 [7] splines_3.6.3 operator.tools_1.6.3 [9] crosstalk_1.1.0.1 digest_0.6.27 [11] ncmeta_0.3.0 leaflet.providers_1.9.0 [13] fansi_0.4.1 checkmate_2.0.0 [15] memoise_1.1.0 modelr_0.1.8 [17] extrafont_0.17 formula.tools_1.7.1 [19] extrafontdb_1.0 jpeg_0.1-8.1 [21] colorspace_2.0-0 tidync_0.2.4 [23] rvest_0.3.6 ggrepel_0.9.0 [25] leafem_0.1.3 xfun_0.19 [27] rgdal_1.5-18 callr_3.5.1 [29] crayon_1.3.4 jsonlite_1.7.2 [31] glue_1.4.2 stars_0.4-3 [33] gtable_0.3.0 webshot_0.5.2 [35] Rttf2pt1_1.3.8 maps_3.3.0 [37] abind_1.4-5 scales_1.1.1 [39] DBI_1.1.0 Rcpp_1.0.5 [41] isoband_0.2.3 plotrix_3.7-8 [43] viridisLite_0.3.0 spData_0.3.8 [45] units_0.6-7 foreign_0.8-75 [47] dotCall64_1.0-0 Formula_1.2-4 [49] DT_0.16 htmlwidgets_1.5.3 [51] httr_1.4.2 oceanmap_0.1.1 [53] ellipsis_0.3.1 XML_3.99-0.3 [55] pkgconfig_2.0.3 farver_2.0.3 [57] dbplyr_2.0.0 utf8_1.1.4 [59] janitor_2.0.1 later_1.1.0.1 [61] tmaptools_3.1 tidyselect_1.1.0 [63] labeling_0.4.2 rlang_0.4.9 [65] munsell_0.5.0 cellranger_1.1.0 [67] tools_3.6.3 cli_2.2.0 [69] generics_0.1.0 ggmap_3.0.0 [71] broom_0.7.3 ggridges_0.5.2 [73] evaluate_0.14 yaml_2.2.1 [75] oce_1.2-0 processx_3.4.5 [77] leafsync_0.1.0 fs_1.5.0 [79] RNetCDF_2.4-2 RgoogleMaps_1.4.5.3 [81] ncdf4_1.17 egg_0.4.5 [83] nlme_3.1-151 mime_0.9 [85] wior_0.0.0.1 xml2_1.3.2 [87] compiler_3.6.3 rstudioapi_0.13 [89] curl_4.3 png_0.1-7 [91] e1071_1.7-4 testthat_3.0.1 [93] mapdata_2.3.0 reprex_0.3.0 [95] stringi_1.5.3 ps_1.5.0 [97] highr_0.8 fields_11.6 [99] rgeos_0.5-5 lattice_0.20-41 [101] Matrix_1.2-18 classInt_0.4-3 [103] ggsci_2.9 vctrs_0.3.6 [105] pillar_1.4.7 lifecycle_0.2.0 [107] gsw_1.0-5 data.table_1.13.4 [109] cowplot_1.1.0 bitops_1.0-6 [111] maptools_1.0-2 raster_3.4-5 [113] httpuv_1.5.4 R6_2.5.0 [115] promises_1.1.1 bookdown_0.21 [117] KernSmooth_2.23-16 gridExtra_2.3 [119] spDataLarge_0.4.1 codetools_0.2-16 [121] dichromat_2.0-0 assertthat_0.2.1 [123] rjson_0.2.20 withr_2.3.0 [125] ggsn_0.5.0 mgcv_1.8-33 [127] parallel_3.6.3 hms_0.5.3 [129] grid_3.6.3 class_7.3-15 [131] rmarkdown_2.6 snakecase_0.11.0 [133] base64enc_0.1-3 References "],["references.html", "References", " References Allaire, JJ, Jeffrey Horner, Yihui Xie, Vicent Marti, and Natacha Porte. 2019. Markdown: Render Markdown with the c Library Sundown. https://github.com/rstudio/markdown. Appelhans, Tim, Florian Detsch, Christoph Reudenbach, and Stefan Woellauer. 2020. Mapview: Interactive Viewing of Spatial Data in R. https://CRAN.R-project.org/package=mapview. Appelhans, Tim, Kenton Russell, and Lorenzo Busetto. 2020. Mapedit: Interactive Editing of Spatial Data in R. https://CRAN.R-project.org/package=mapedit. Auguie, Baptiste. 2018. Egg: Extensions for Ggplot2: Custom Geom, Plot Alignment, Symmetrised Scale, and Fixed Panel Size. https://CRAN.R-project.org/package=egg. Bache, Stefan Milton, and Hadley Wickham. 2020. Magrittr: A Forward-Pipe Operator for R. https://CRAN.R-project.org/package=magrittr. Barr, Dale, and Lisa DeBruine. 2019. Webex: Create Interactive Web Exercises in R Markdown. https://github.com/psyteachr/webex. Bauer, Robert K. 2020. Oceanmap: A Plotting Toolbox for 2D Oceanographic Data. https://CRAN.R-project.org/package=oceanmap. Bivand, Roger, Jakub Nowosad, and Robin Lovelace. 2020. SpData: Datasets for Spatial Analysis. https://nowosad.github.io/spData/. Bivand, Roger S., Edzer Pebesma, and Virgilio Gomez-Rubio. 2013. Applied Spatial Data Analysis with R, Second Edition. Springer, NY. https://asdar-book.org/. Bolstad, Paul. 2016. GIS Fundamentals: A First Text on Geographic Information Systems. Eider (PressMinnesota). Campitelli, Elio. 2019. MetR: Tools for Easier Analysis of Meteorological Fields. https://CRAN.R-project.org/package=metR. . 2020. MetR: Tools for Easier Analysis of Meteorological Fields. https://github.com/eliocamp/metR. Chang, Winston, Joe Cheng, JJ Allaire, Yihui Xie, and Jonathan McPherson. 2020. Shiny: Web Application Framework for R. https://CRAN.R-project.org/package=shiny. Cheng, Joe, Bhaskar Karambelkar, and Yihui Xie. 2019. Leaflet: Create Interactive Web Maps with the Javascript Leaflet Library. https://CRAN.R-project.org/package=leaflet. Dunnington, Dewey. 2020. Ggspatial: Spatial Data Framework for Ggplot2. https://CRAN.R-project.org/package=ggspatial. Hijmans, Robert J. 2020. Raster: Geographic Data Analysis and Modeling. https://CRAN.R-project.org/package=raster. Iannone, Richard, JJ Allaire, and Barbara Borges. 2020. Flexdashboard: R Markdown Format for Flexible Dashboards. https://CRAN.R-project.org/package=flexdashboard. Kelley, Dan, and Clark Richards. 2020. Oce: Analysis of Oceanographic Data. https://dankelley.github.io/oce. Longley, J. W. 1967. An Appraisal of Least-Squares Programs from the Point of View of the User. Journal of the American Statistical Association, 81941. Lovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with R. CRC Press. Müller, Kirill, and Hadley Wickham. 2020. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble. Pebesma, Edzer. 2018. Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal 10 (1): 43946. https://doi.org/10.32614/RJ-2018-009. Pedersen, Thomas Lin. 2020. Patchwork: The Composer of Plots. https://CRAN.R-project.org/package=patchwork. Pedersen, Thomas Lin, and David Robinson. 2017. Gganimate: A Grammar of Animated Graphics. http://github.com/thomasp85/gganimate. Piburn, Jesse. 2020. Wbstats: Programmatic Access to Data and Statistics from the World Bank Api. https://github.com/nset-ornl/wbstats. Pierce, David. 2019. Ncdf4: Interface to Unidata netCDF (Version 4 or Earlier) Format Data Files. https://CRAN.R-project.org/package=ncdf4. Ram, Karthik, Carl Boettiger, and Andrew Dyck. 2016. Rfisheries: Programmatic Interface to the Openfisheries.org Api. https://www.github.com/ropensci/rfisheries. R Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. RStudio Team. 2015. RStudio: Integrated Development Environment for R. Boston, MA: RStudio, Inc. http://www.rstudio.com/. Semba, Masumbuko, and Nyamisi Peter. 2020. Wior: Easy Tidy and Process Oceanographic Data. Slowikowski, Kamil. 2020. Ggrepel: Automatically Position Non-Overlapping Text Labels with Ggplot2. https://CRAN.R-project.org/package=ggrepel. South, Andy. 2017. Rnaturalearth: World Map Data from Natural Earth. https://CRAN.R-project.org/package=rnaturalearth. Tennekes, Martijn. 2018. tmap: Thematic Maps in R. Journal of Statistical Software 84 (6): 139. https://doi.org/10.18637/jss.v084.i06. Vanderkam, Dan, JJ Allaire, Jonathan Owen, Daniel Gromer, and Benoit Thieurmel. 2018. Dygraphs: Interface to Dygraphs Interactive Time Series Charting Library. https://CRAN.R-project.org/package=dygraphs. Wickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org. . 2019. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr. . 2020. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy DAgostino McGowan, Romain François, Garrett Grolemund, et al. 2019. Welcome to the tidyverse. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, and Jennifer Bryan. 2019. Readxl: Read Excel Files. https://CRAN.R-project.org/package=readxl. Wickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey Dunnington. 2020. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2. Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2020. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr. Wickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" OReilly Media, Inc.\". Wickham, Hadley, and Jim Hester. 2020. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr. Wilke, Claus O. 2018. Cowplot: Streamlined Plot Theme and Plot Annotations for Ggplot2. https://CRAN.R-project.org/package=cowplot. Xie, Yihui. 2016. Bookdown: Authoring Books and Technical Documents with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://github.com/rstudio/bookdown. . 2020. Bookdown: Authoring Books and Technical Documents with R Markdown. https://github.com/rstudio/bookdown. Xie, Yihui, Joe Cheng, and Xianying Tan. 2020. DT: A Wrapper of the Javascript Library Datatables. https://CRAN.R-project.org/package=DT. Xie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook. Zhu, Hao. 2020. KableExtra: Construct Complex Table with Kable and Pipe Syntax. https://CRAN.R-project.org/package=kableExtra. "]]
